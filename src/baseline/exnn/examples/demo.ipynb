{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T06:41:26.567543Z",
     "start_time": "2020-07-21T06:36:09.338163Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial training.\n",
      "Training epoch: 1, train loss: 0.15248, val loss: 0.15325\n",
      "Training epoch: 2, train loss: 0.13367, val loss: 0.13557\n",
      "Training epoch: 3, train loss: 0.11582, val loss: 0.11764\n",
      "Training epoch: 4, train loss: 0.09918, val loss: 0.10050\n",
      "Training epoch: 5, train loss: 0.08543, val loss: 0.08640\n",
      "Training epoch: 6, train loss: 0.07818, val loss: 0.07827\n",
      "Training epoch: 7, train loss: 0.06875, val loss: 0.06865\n",
      "Training epoch: 8, train loss: 0.06563, val loss: 0.06523\n",
      "Training epoch: 9, train loss: 0.05757, val loss: 0.05775\n",
      "Training epoch: 10, train loss: 0.05770, val loss: 0.05791\n",
      "Training epoch: 11, train loss: 0.04955, val loss: 0.05008\n",
      "Training epoch: 12, train loss: 0.04903, val loss: 0.04945\n",
      "Training epoch: 13, train loss: 0.04844, val loss: 0.04874\n",
      "Training epoch: 14, train loss: 0.04466, val loss: 0.04488\n",
      "Training epoch: 15, train loss: 0.04277, val loss: 0.04296\n",
      "Training epoch: 16, train loss: 0.03933, val loss: 0.03972\n",
      "Training epoch: 17, train loss: 0.04030, val loss: 0.04039\n",
      "Training epoch: 18, train loss: 0.03780, val loss: 0.03791\n",
      "Training epoch: 19, train loss: 0.03727, val loss: 0.03747\n",
      "Training epoch: 20, train loss: 0.03549, val loss: 0.03573\n",
      "Training epoch: 21, train loss: 0.03513, val loss: 0.03545\n",
      "Training epoch: 22, train loss: 0.03354, val loss: 0.03372\n",
      "Training epoch: 23, train loss: 0.03204, val loss: 0.03219\n",
      "Training epoch: 24, train loss: 0.03263, val loss: 0.03277\n",
      "Training epoch: 25, train loss: 0.03101, val loss: 0.03101\n",
      "Training epoch: 26, train loss: 0.03086, val loss: 0.03092\n",
      "Training epoch: 27, train loss: 0.03000, val loss: 0.03007\n",
      "Training epoch: 28, train loss: 0.02992, val loss: 0.03000\n",
      "Training epoch: 29, train loss: 0.02911, val loss: 0.02925\n",
      "Training epoch: 30, train loss: 0.02797, val loss: 0.02806\n",
      "Training epoch: 31, train loss: 0.02812, val loss: 0.02838\n",
      "Training epoch: 32, train loss: 0.02673, val loss: 0.02695\n",
      "Training epoch: 33, train loss: 0.02774, val loss: 0.02781\n",
      "Training epoch: 34, train loss: 0.02628, val loss: 0.02649\n",
      "Training epoch: 35, train loss: 0.02623, val loss: 0.02635\n",
      "Training epoch: 36, train loss: 0.02549, val loss: 0.02578\n",
      "Training epoch: 37, train loss: 0.02553, val loss: 0.02563\n",
      "Training epoch: 38, train loss: 0.02468, val loss: 0.02498\n",
      "Training epoch: 39, train loss: 0.02540, val loss: 0.02566\n",
      "Training epoch: 40, train loss: 0.02479, val loss: 0.02508\n",
      "Training epoch: 41, train loss: 0.02460, val loss: 0.02492\n",
      "Training epoch: 42, train loss: 0.02424, val loss: 0.02450\n",
      "Training epoch: 43, train loss: 0.02567, val loss: 0.02605\n",
      "Training epoch: 44, train loss: 0.02370, val loss: 0.02399\n",
      "Training epoch: 45, train loss: 0.02404, val loss: 0.02429\n",
      "Training epoch: 46, train loss: 0.02341, val loss: 0.02370\n",
      "Training epoch: 47, train loss: 0.02293, val loss: 0.02330\n",
      "Training epoch: 48, train loss: 0.02338, val loss: 0.02367\n",
      "Training epoch: 49, train loss: 0.02346, val loss: 0.02388\n",
      "Training epoch: 50, train loss: 0.02285, val loss: 0.02318\n",
      "Training epoch: 51, train loss: 0.02293, val loss: 0.02328\n",
      "Training epoch: 52, train loss: 0.02311, val loss: 0.02353\n",
      "Training epoch: 53, train loss: 0.02336, val loss: 0.02372\n",
      "Training epoch: 54, train loss: 0.02251, val loss: 0.02283\n",
      "Training epoch: 55, train loss: 0.02229, val loss: 0.02269\n",
      "Training epoch: 56, train loss: 0.02248, val loss: 0.02274\n",
      "Training epoch: 57, train loss: 0.02236, val loss: 0.02270\n",
      "Training epoch: 58, train loss: 0.02234, val loss: 0.02260\n",
      "Training epoch: 59, train loss: 0.02227, val loss: 0.02268\n",
      "Training epoch: 60, train loss: 0.02227, val loss: 0.02253\n",
      "Training epoch: 61, train loss: 0.02176, val loss: 0.02218\n",
      "Training epoch: 62, train loss: 0.02179, val loss: 0.02214\n",
      "Training epoch: 63, train loss: 0.02194, val loss: 0.02234\n",
      "Training epoch: 64, train loss: 0.02170, val loss: 0.02212\n",
      "Training epoch: 65, train loss: 0.02156, val loss: 0.02192\n",
      "Training epoch: 66, train loss: 0.02155, val loss: 0.02194\n",
      "Training epoch: 67, train loss: 0.02151, val loss: 0.02190\n",
      "Training epoch: 68, train loss: 0.02176, val loss: 0.02225\n",
      "Training epoch: 69, train loss: 0.02162, val loss: 0.02209\n",
      "Training epoch: 70, train loss: 0.02210, val loss: 0.02251\n",
      "Training epoch: 71, train loss: 0.02157, val loss: 0.02207\n",
      "Training epoch: 72, train loss: 0.02201, val loss: 0.02233\n",
      "Training epoch: 73, train loss: 0.02172, val loss: 0.02213\n",
      "Training epoch: 74, train loss: 0.02128, val loss: 0.02171\n",
      "Training epoch: 75, train loss: 0.02127, val loss: 0.02170\n",
      "Training epoch: 76, train loss: 0.02196, val loss: 0.02235\n",
      "Training epoch: 77, train loss: 0.02139, val loss: 0.02185\n",
      "Training epoch: 78, train loss: 0.02117, val loss: 0.02163\n",
      "Training epoch: 79, train loss: 0.02113, val loss: 0.02145\n",
      "Training epoch: 80, train loss: 0.02107, val loss: 0.02149\n",
      "Training epoch: 81, train loss: 0.02117, val loss: 0.02167\n",
      "Training epoch: 82, train loss: 0.02104, val loss: 0.02153\n",
      "Training epoch: 83, train loss: 0.02119, val loss: 0.02169\n",
      "Training epoch: 84, train loss: 0.02120, val loss: 0.02164\n",
      "Training epoch: 85, train loss: 0.02086, val loss: 0.02128\n",
      "Training epoch: 86, train loss: 0.02103, val loss: 0.02152\n",
      "Training epoch: 87, train loss: 0.02097, val loss: 0.02140\n",
      "Training epoch: 88, train loss: 0.02114, val loss: 0.02166\n",
      "Training epoch: 89, train loss: 0.02094, val loss: 0.02137\n",
      "Training epoch: 90, train loss: 0.02103, val loss: 0.02148\n",
      "Training epoch: 91, train loss: 0.02078, val loss: 0.02123\n",
      "Training epoch: 92, train loss: 0.02069, val loss: 0.02117\n",
      "Training epoch: 93, train loss: 0.02075, val loss: 0.02113\n",
      "Training epoch: 94, train loss: 0.02070, val loss: 0.02115\n",
      "Training epoch: 95, train loss: 0.02103, val loss: 0.02139\n",
      "Training epoch: 96, train loss: 0.02168, val loss: 0.02211\n",
      "Training epoch: 97, train loss: 0.02069, val loss: 0.02112\n",
      "Training epoch: 98, train loss: 0.02099, val loss: 0.02142\n",
      "Training epoch: 99, train loss: 0.02063, val loss: 0.02106\n",
      "Training epoch: 100, train loss: 0.02091, val loss: 0.02142\n",
      "Training epoch: 101, train loss: 0.02058, val loss: 0.02108\n",
      "Training epoch: 102, train loss: 0.02081, val loss: 0.02137\n",
      "Training epoch: 103, train loss: 0.02073, val loss: 0.02113\n",
      "Training epoch: 104, train loss: 0.02057, val loss: 0.02107\n",
      "Training epoch: 105, train loss: 0.02045, val loss: 0.02098\n",
      "Training epoch: 106, train loss: 0.02070, val loss: 0.02109\n",
      "Training epoch: 107, train loss: 0.02077, val loss: 0.02126\n",
      "Training epoch: 108, train loss: 0.02040, val loss: 0.02090\n",
      "Training epoch: 109, train loss: 0.02043, val loss: 0.02088\n",
      "Training epoch: 110, train loss: 0.02051, val loss: 0.02102\n",
      "Training epoch: 111, train loss: 0.02036, val loss: 0.02085\n",
      "Training epoch: 112, train loss: 0.02072, val loss: 0.02112\n",
      "Training epoch: 113, train loss: 0.02076, val loss: 0.02123\n",
      "Training epoch: 114, train loss: 0.02029, val loss: 0.02074\n",
      "Training epoch: 115, train loss: 0.02036, val loss: 0.02094\n",
      "Training epoch: 116, train loss: 0.02026, val loss: 0.02067\n",
      "Training epoch: 117, train loss: 0.02057, val loss: 0.02102\n",
      "Training epoch: 118, train loss: 0.02042, val loss: 0.02089\n",
      "Training epoch: 119, train loss: 0.02053, val loss: 0.02092\n",
      "Training epoch: 120, train loss: 0.02030, val loss: 0.02073\n",
      "Training epoch: 121, train loss: 0.02057, val loss: 0.02104\n",
      "Training epoch: 122, train loss: 0.02018, val loss: 0.02074\n",
      "Training epoch: 123, train loss: 0.02116, val loss: 0.02158\n",
      "Training epoch: 124, train loss: 0.02046, val loss: 0.02100\n",
      "Training epoch: 125, train loss: 0.02084, val loss: 0.02151\n",
      "Training epoch: 126, train loss: 0.02116, val loss: 0.02184\n",
      "Training epoch: 127, train loss: 0.02028, val loss: 0.02078\n",
      "Training epoch: 128, train loss: 0.02006, val loss: 0.02061\n",
      "Training epoch: 129, train loss: 0.02056, val loss: 0.02110\n",
      "Training epoch: 130, train loss: 0.02039, val loss: 0.02098\n",
      "Training epoch: 131, train loss: 0.02034, val loss: 0.02081\n",
      "Training epoch: 132, train loss: 0.02032, val loss: 0.02083\n",
      "Training epoch: 133, train loss: 0.02000, val loss: 0.02053\n",
      "Training epoch: 134, train loss: 0.02005, val loss: 0.02063\n",
      "Training epoch: 135, train loss: 0.02015, val loss: 0.02071\n",
      "Training epoch: 136, train loss: 0.02004, val loss: 0.02058\n",
      "Training epoch: 137, train loss: 0.02041, val loss: 0.02100\n",
      "Training epoch: 138, train loss: 0.01996, val loss: 0.02046\n",
      "Training epoch: 139, train loss: 0.01989, val loss: 0.02045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 140, train loss: 0.02012, val loss: 0.02056\n",
      "Training epoch: 141, train loss: 0.02019, val loss: 0.02076\n",
      "Training epoch: 142, train loss: 0.01987, val loss: 0.02043\n",
      "Training epoch: 143, train loss: 0.01997, val loss: 0.02046\n",
      "Training epoch: 144, train loss: 0.01988, val loss: 0.02035\n",
      "Training epoch: 145, train loss: 0.01988, val loss: 0.02046\n",
      "Training epoch: 146, train loss: 0.01991, val loss: 0.02040\n",
      "Training epoch: 147, train loss: 0.01995, val loss: 0.02058\n",
      "Training epoch: 148, train loss: 0.02026, val loss: 0.02087\n",
      "Training epoch: 149, train loss: 0.01972, val loss: 0.02024\n",
      "Training epoch: 150, train loss: 0.02097, val loss: 0.02165\n",
      "Training epoch: 151, train loss: 0.01993, val loss: 0.02054\n",
      "Training epoch: 152, train loss: 0.02037, val loss: 0.02087\n",
      "Training epoch: 153, train loss: 0.01970, val loss: 0.02024\n",
      "Training epoch: 154, train loss: 0.02011, val loss: 0.02082\n",
      "Training epoch: 155, train loss: 0.01970, val loss: 0.02024\n",
      "Training epoch: 156, train loss: 0.01988, val loss: 0.02050\n",
      "Training epoch: 157, train loss: 0.01963, val loss: 0.02016\n",
      "Training epoch: 158, train loss: 0.01995, val loss: 0.02059\n",
      "Training epoch: 159, train loss: 0.01971, val loss: 0.02024\n",
      "Training epoch: 160, train loss: 0.02027, val loss: 0.02081\n",
      "Training epoch: 161, train loss: 0.01959, val loss: 0.02017\n",
      "Training epoch: 162, train loss: 0.01970, val loss: 0.02024\n",
      "Training epoch: 163, train loss: 0.01978, val loss: 0.02046\n",
      "Training epoch: 164, train loss: 0.01968, val loss: 0.02032\n",
      "Training epoch: 165, train loss: 0.01962, val loss: 0.02018\n",
      "Training epoch: 166, train loss: 0.01969, val loss: 0.02037\n",
      "Training epoch: 167, train loss: 0.01964, val loss: 0.02014\n",
      "Training epoch: 168, train loss: 0.01977, val loss: 0.02027\n",
      "Training epoch: 169, train loss: 0.01996, val loss: 0.02058\n",
      "Training epoch: 170, train loss: 0.01975, val loss: 0.02024\n",
      "Training epoch: 171, train loss: 0.01984, val loss: 0.02056\n",
      "Training epoch: 172, train loss: 0.01955, val loss: 0.02017\n",
      "Training epoch: 173, train loss: 0.01968, val loss: 0.02026\n",
      "Training epoch: 174, train loss: 0.01960, val loss: 0.02010\n",
      "Training epoch: 175, train loss: 0.02007, val loss: 0.02085\n",
      "Training epoch: 176, train loss: 0.01938, val loss: 0.01997\n",
      "Training epoch: 177, train loss: 0.02073, val loss: 0.02118\n",
      "Training epoch: 178, train loss: 0.01971, val loss: 0.02031\n",
      "Training epoch: 179, train loss: 0.01969, val loss: 0.02022\n",
      "Training epoch: 180, train loss: 0.01938, val loss: 0.01998\n",
      "Training epoch: 181, train loss: 0.01933, val loss: 0.01998\n",
      "Training epoch: 182, train loss: 0.01950, val loss: 0.01999\n",
      "Training epoch: 183, train loss: 0.01977, val loss: 0.02028\n",
      "Training epoch: 184, train loss: 0.01961, val loss: 0.02027\n",
      "Training epoch: 185, train loss: 0.01964, val loss: 0.02039\n",
      "Training epoch: 186, train loss: 0.01928, val loss: 0.01990\n",
      "Training epoch: 187, train loss: 0.01976, val loss: 0.02051\n",
      "Training epoch: 188, train loss: 0.01936, val loss: 0.02002\n",
      "Training epoch: 189, train loss: 0.01941, val loss: 0.02006\n",
      "Training epoch: 190, train loss: 0.01927, val loss: 0.01981\n",
      "Training epoch: 191, train loss: 0.02006, val loss: 0.02060\n",
      "Training epoch: 192, train loss: 0.01964, val loss: 0.02017\n",
      "Training epoch: 193, train loss: 0.01946, val loss: 0.02000\n",
      "Training epoch: 194, train loss: 0.01938, val loss: 0.01990\n",
      "Training epoch: 195, train loss: 0.01924, val loss: 0.01993\n",
      "Training epoch: 196, train loss: 0.01952, val loss: 0.02023\n",
      "Training epoch: 197, train loss: 0.02008, val loss: 0.02054\n",
      "Training epoch: 198, train loss: 0.01918, val loss: 0.01973\n",
      "Training epoch: 199, train loss: 0.01936, val loss: 0.01988\n",
      "Training epoch: 200, train loss: 0.01926, val loss: 0.01999\n",
      "Training epoch: 201, train loss: 0.01936, val loss: 0.01991\n",
      "Training epoch: 202, train loss: 0.01965, val loss: 0.02043\n",
      "Training epoch: 203, train loss: 0.01957, val loss: 0.02026\n",
      "Training epoch: 204, train loss: 0.01907, val loss: 0.01960\n",
      "Training epoch: 205, train loss: 0.01902, val loss: 0.01965\n",
      "Training epoch: 206, train loss: 0.01931, val loss: 0.01991\n",
      "Training epoch: 207, train loss: 0.01904, val loss: 0.01968\n",
      "Training epoch: 208, train loss: 0.01895, val loss: 0.01959\n",
      "Training epoch: 209, train loss: 0.01893, val loss: 0.01959\n",
      "Training epoch: 210, train loss: 0.01917, val loss: 0.01975\n",
      "Training epoch: 211, train loss: 0.01912, val loss: 0.01977\n",
      "Training epoch: 212, train loss: 0.02022, val loss: 0.02107\n",
      "Training epoch: 213, train loss: 0.01902, val loss: 0.01972\n",
      "Training epoch: 214, train loss: 0.01885, val loss: 0.01945\n",
      "Training epoch: 215, train loss: 0.01887, val loss: 0.01949\n",
      "Training epoch: 216, train loss: 0.01922, val loss: 0.01976\n",
      "Training epoch: 217, train loss: 0.01913, val loss: 0.01985\n",
      "Training epoch: 218, train loss: 0.01909, val loss: 0.01972\n",
      "Training epoch: 219, train loss: 0.01888, val loss: 0.01954\n",
      "Training epoch: 220, train loss: 0.01918, val loss: 0.01995\n",
      "Training epoch: 221, train loss: 0.02130, val loss: 0.02227\n",
      "Training epoch: 222, train loss: 0.01902, val loss: 0.01972\n",
      "Training epoch: 223, train loss: 0.01939, val loss: 0.02019\n",
      "Training epoch: 224, train loss: 0.01899, val loss: 0.01974\n",
      "Training epoch: 225, train loss: 0.01913, val loss: 0.01988\n",
      "Training epoch: 226, train loss: 0.01902, val loss: 0.01974\n",
      "Training epoch: 227, train loss: 0.01873, val loss: 0.01948\n",
      "Training epoch: 228, train loss: 0.01870, val loss: 0.01933\n",
      "Training epoch: 229, train loss: 0.01917, val loss: 0.01987\n",
      "Training epoch: 230, train loss: 0.01929, val loss: 0.02003\n",
      "Training epoch: 231, train loss: 0.01867, val loss: 0.01932\n",
      "Training epoch: 232, train loss: 0.01863, val loss: 0.01927\n",
      "Training epoch: 233, train loss: 0.01853, val loss: 0.01920\n",
      "Training epoch: 234, train loss: 0.01895, val loss: 0.01945\n",
      "Training epoch: 235, train loss: 0.01878, val loss: 0.01928\n",
      "Training epoch: 236, train loss: 0.01855, val loss: 0.01920\n",
      "Training epoch: 237, train loss: 0.01872, val loss: 0.01943\n",
      "Training epoch: 238, train loss: 0.01887, val loss: 0.01958\n",
      "Training epoch: 239, train loss: 0.01864, val loss: 0.01940\n",
      "Training epoch: 240, train loss: 0.01857, val loss: 0.01915\n",
      "Training epoch: 241, train loss: 0.01868, val loss: 0.01933\n",
      "Training epoch: 242, train loss: 0.01838, val loss: 0.01907\n",
      "Training epoch: 243, train loss: 0.01854, val loss: 0.01917\n",
      "Training epoch: 244, train loss: 0.01868, val loss: 0.01940\n",
      "Training epoch: 245, train loss: 0.01930, val loss: 0.02020\n",
      "Training epoch: 246, train loss: 0.01875, val loss: 0.01955\n",
      "Training epoch: 247, train loss: 0.01869, val loss: 0.01949\n",
      "Training epoch: 248, train loss: 0.01834, val loss: 0.01901\n",
      "Training epoch: 249, train loss: 0.01852, val loss: 0.01929\n",
      "Training epoch: 250, train loss: 0.01872, val loss: 0.01923\n",
      "Training epoch: 251, train loss: 0.01874, val loss: 0.01943\n",
      "Training epoch: 252, train loss: 0.01843, val loss: 0.01900\n",
      "Training epoch: 253, train loss: 0.01838, val loss: 0.01896\n",
      "Training epoch: 254, train loss: 0.01856, val loss: 0.01921\n",
      "Training epoch: 255, train loss: 0.01835, val loss: 0.01915\n",
      "Training epoch: 256, train loss: 0.01815, val loss: 0.01885\n",
      "Training epoch: 257, train loss: 0.01887, val loss: 0.01971\n",
      "Training epoch: 258, train loss: 0.01812, val loss: 0.01883\n",
      "Training epoch: 259, train loss: 0.01826, val loss: 0.01885\n",
      "Training epoch: 260, train loss: 0.01826, val loss: 0.01888\n",
      "Training epoch: 261, train loss: 0.01807, val loss: 0.01877\n",
      "Training epoch: 262, train loss: 0.01858, val loss: 0.01915\n",
      "Training epoch: 263, train loss: 0.01931, val loss: 0.02029\n",
      "Training epoch: 264, train loss: 0.01809, val loss: 0.01878\n",
      "Training epoch: 265, train loss: 0.01811, val loss: 0.01874\n",
      "Training epoch: 266, train loss: 0.01822, val loss: 0.01900\n",
      "Training epoch: 267, train loss: 0.01831, val loss: 0.01894\n",
      "Training epoch: 268, train loss: 0.01856, val loss: 0.01909\n",
      "Training epoch: 269, train loss: 0.01872, val loss: 0.01945\n",
      "Training epoch: 270, train loss: 0.01791, val loss: 0.01862\n",
      "Training epoch: 271, train loss: 0.01807, val loss: 0.01878\n",
      "Training epoch: 272, train loss: 0.01829, val loss: 0.01888\n",
      "Training epoch: 273, train loss: 0.01803, val loss: 0.01868\n",
      "Training epoch: 274, train loss: 0.01798, val loss: 0.01862\n",
      "Training epoch: 275, train loss: 0.01851, val loss: 0.01919\n",
      "Training epoch: 276, train loss: 0.01792, val loss: 0.01859\n",
      "Training epoch: 277, train loss: 0.01799, val loss: 0.01862\n",
      "Training epoch: 278, train loss: 0.01796, val loss: 0.01860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 279, train loss: 0.01776, val loss: 0.01848\n",
      "Training epoch: 280, train loss: 0.01773, val loss: 0.01839\n",
      "Training epoch: 281, train loss: 0.01813, val loss: 0.01881\n",
      "Training epoch: 282, train loss: 0.01782, val loss: 0.01853\n",
      "Training epoch: 283, train loss: 0.01800, val loss: 0.01867\n",
      "Training epoch: 284, train loss: 0.01795, val loss: 0.01855\n",
      "Training epoch: 285, train loss: 0.01767, val loss: 0.01831\n",
      "Training epoch: 286, train loss: 0.01791, val loss: 0.01875\n",
      "Training epoch: 287, train loss: 0.01810, val loss: 0.01886\n",
      "Training epoch: 288, train loss: 0.01813, val loss: 0.01887\n",
      "Training epoch: 289, train loss: 0.01770, val loss: 0.01835\n",
      "Training epoch: 290, train loss: 0.01797, val loss: 0.01858\n",
      "Training epoch: 291, train loss: 0.01809, val loss: 0.01890\n",
      "Training epoch: 292, train loss: 0.01807, val loss: 0.01889\n",
      "Training epoch: 293, train loss: 0.01761, val loss: 0.01832\n",
      "Training epoch: 294, train loss: 0.01783, val loss: 0.01867\n",
      "Training epoch: 295, train loss: 0.01821, val loss: 0.01909\n",
      "Training epoch: 296, train loss: 0.01758, val loss: 0.01834\n",
      "Training epoch: 297, train loss: 0.01767, val loss: 0.01839\n",
      "Training epoch: 298, train loss: 0.01821, val loss: 0.01882\n",
      "Training epoch: 299, train loss: 0.01768, val loss: 0.01840\n",
      "Training epoch: 300, train loss: 0.01781, val loss: 0.01863\n",
      "Training epoch: 301, train loss: 0.01745, val loss: 0.01820\n",
      "Training epoch: 302, train loss: 0.01812, val loss: 0.01892\n",
      "Training epoch: 303, train loss: 0.01781, val loss: 0.01849\n",
      "Training epoch: 304, train loss: 0.01783, val loss: 0.01868\n",
      "Training epoch: 305, train loss: 0.01794, val loss: 0.01871\n",
      "Training epoch: 306, train loss: 0.01789, val loss: 0.01879\n",
      "Training epoch: 307, train loss: 0.01768, val loss: 0.01844\n",
      "Training epoch: 308, train loss: 0.01787, val loss: 0.01854\n",
      "Training epoch: 309, train loss: 0.01755, val loss: 0.01822\n",
      "Training epoch: 310, train loss: 0.01751, val loss: 0.01820\n",
      "Training epoch: 311, train loss: 0.01760, val loss: 0.01832\n",
      "Training epoch: 312, train loss: 0.01821, val loss: 0.01912\n",
      "Training epoch: 313, train loss: 0.01792, val loss: 0.01856\n",
      "Training epoch: 314, train loss: 0.01727, val loss: 0.01797\n",
      "Training epoch: 315, train loss: 0.01736, val loss: 0.01806\n",
      "Training epoch: 316, train loss: 0.01797, val loss: 0.01862\n",
      "Training epoch: 317, train loss: 0.01771, val loss: 0.01841\n",
      "Training epoch: 318, train loss: 0.01730, val loss: 0.01798\n",
      "Training epoch: 319, train loss: 0.01735, val loss: 0.01811\n",
      "Training epoch: 320, train loss: 0.01729, val loss: 0.01799\n",
      "Training epoch: 321, train loss: 0.01733, val loss: 0.01797\n",
      "Training epoch: 322, train loss: 0.01761, val loss: 0.01829\n",
      "Training epoch: 323, train loss: 0.01815, val loss: 0.01880\n",
      "Training epoch: 324, train loss: 0.01716, val loss: 0.01785\n",
      "Training epoch: 325, train loss: 0.01717, val loss: 0.01795\n",
      "Training epoch: 326, train loss: 0.01716, val loss: 0.01784\n",
      "Training epoch: 327, train loss: 0.01721, val loss: 0.01791\n",
      "Training epoch: 328, train loss: 0.01749, val loss: 0.01815\n",
      "Training epoch: 329, train loss: 0.01712, val loss: 0.01783\n",
      "Training epoch: 330, train loss: 0.01711, val loss: 0.01782\n",
      "Training epoch: 331, train loss: 0.01767, val loss: 0.01841\n",
      "Training epoch: 332, train loss: 0.01762, val loss: 0.01847\n",
      "Training epoch: 333, train loss: 0.01956, val loss: 0.02053\n",
      "Training epoch: 334, train loss: 0.01763, val loss: 0.01820\n",
      "Training epoch: 335, train loss: 0.01785, val loss: 0.01841\n",
      "Training epoch: 336, train loss: 0.01710, val loss: 0.01777\n",
      "Training epoch: 337, train loss: 0.01777, val loss: 0.01856\n",
      "Training epoch: 338, train loss: 0.01817, val loss: 0.01871\n",
      "Training epoch: 339, train loss: 0.01698, val loss: 0.01770\n",
      "Training epoch: 340, train loss: 0.01746, val loss: 0.01815\n",
      "Training epoch: 341, train loss: 0.01880, val loss: 0.01972\n",
      "Training epoch: 342, train loss: 0.01873, val loss: 0.01965\n",
      "Training epoch: 343, train loss: 0.01741, val loss: 0.01814\n",
      "Training epoch: 344, train loss: 0.01760, val loss: 0.01822\n",
      "Training epoch: 345, train loss: 0.01720, val loss: 0.01785\n",
      "Training epoch: 346, train loss: 0.01739, val loss: 0.01819\n",
      "Training epoch: 347, train loss: 0.01748, val loss: 0.01817\n",
      "Training epoch: 348, train loss: 0.01713, val loss: 0.01794\n",
      "Training epoch: 349, train loss: 0.01717, val loss: 0.01794\n",
      "Training epoch: 350, train loss: 0.01690, val loss: 0.01759\n",
      "Training epoch: 351, train loss: 0.01731, val loss: 0.01814\n",
      "Training epoch: 352, train loss: 0.01705, val loss: 0.01781\n",
      "Training epoch: 353, train loss: 0.01743, val loss: 0.01822\n",
      "Training epoch: 354, train loss: 0.01708, val loss: 0.01789\n",
      "Training epoch: 355, train loss: 0.01770, val loss: 0.01835\n",
      "Training epoch: 356, train loss: 0.01746, val loss: 0.01809\n",
      "Training epoch: 357, train loss: 0.01704, val loss: 0.01771\n",
      "Training epoch: 358, train loss: 0.01734, val loss: 0.01810\n",
      "Training epoch: 359, train loss: 0.01726, val loss: 0.01787\n",
      "Training epoch: 360, train loss: 0.01692, val loss: 0.01758\n",
      "Training epoch: 361, train loss: 0.01727, val loss: 0.01806\n",
      "Training epoch: 362, train loss: 0.01717, val loss: 0.01787\n",
      "Training epoch: 363, train loss: 0.01722, val loss: 0.01798\n",
      "Training epoch: 364, train loss: 0.01694, val loss: 0.01774\n",
      "Training epoch: 365, train loss: 0.01697, val loss: 0.01771\n",
      "Training epoch: 366, train loss: 0.01698, val loss: 0.01766\n",
      "Training epoch: 367, train loss: 0.01675, val loss: 0.01750\n",
      "Training epoch: 368, train loss: 0.01699, val loss: 0.01774\n",
      "Training epoch: 369, train loss: 0.01701, val loss: 0.01779\n",
      "Training epoch: 370, train loss: 0.01709, val loss: 0.01768\n",
      "Training epoch: 371, train loss: 0.01708, val loss: 0.01772\n",
      "Training epoch: 372, train loss: 0.01681, val loss: 0.01755\n",
      "Training epoch: 373, train loss: 0.01695, val loss: 0.01767\n",
      "Training epoch: 374, train loss: 0.01697, val loss: 0.01763\n",
      "Training epoch: 375, train loss: 0.01713, val loss: 0.01777\n",
      "Training epoch: 376, train loss: 0.01715, val loss: 0.01784\n",
      "Training epoch: 377, train loss: 0.01669, val loss: 0.01742\n",
      "Training epoch: 378, train loss: 0.01695, val loss: 0.01762\n",
      "Training epoch: 379, train loss: 0.01691, val loss: 0.01768\n",
      "Training epoch: 380, train loss: 0.01682, val loss: 0.01748\n",
      "Training epoch: 381, train loss: 0.01669, val loss: 0.01739\n",
      "Training epoch: 382, train loss: 0.01664, val loss: 0.01729\n",
      "Training epoch: 383, train loss: 0.01722, val loss: 0.01794\n",
      "Training epoch: 384, train loss: 0.01714, val loss: 0.01777\n",
      "Training epoch: 385, train loss: 0.01687, val loss: 0.01754\n",
      "Training epoch: 386, train loss: 0.01811, val loss: 0.01884\n",
      "Training epoch: 387, train loss: 0.01701, val loss: 0.01764\n",
      "Training epoch: 388, train loss: 0.01692, val loss: 0.01760\n",
      "Training epoch: 389, train loss: 0.01680, val loss: 0.01749\n",
      "Training epoch: 390, train loss: 0.01667, val loss: 0.01732\n",
      "Training epoch: 391, train loss: 0.01678, val loss: 0.01745\n",
      "Training epoch: 392, train loss: 0.01670, val loss: 0.01737\n",
      "Training epoch: 393, train loss: 0.01684, val loss: 0.01745\n",
      "Training epoch: 394, train loss: 0.01706, val loss: 0.01780\n",
      "Training epoch: 395, train loss: 0.01690, val loss: 0.01768\n",
      "Training epoch: 396, train loss: 0.01664, val loss: 0.01734\n",
      "Training epoch: 397, train loss: 0.01699, val loss: 0.01760\n",
      "Training epoch: 398, train loss: 0.01778, val loss: 0.01844\n",
      "Training epoch: 399, train loss: 0.01738, val loss: 0.01813\n",
      "Training epoch: 400, train loss: 0.01668, val loss: 0.01742\n",
      "Training epoch: 401, train loss: 0.01670, val loss: 0.01731\n",
      "Training epoch: 402, train loss: 0.01673, val loss: 0.01745\n",
      "Training epoch: 403, train loss: 0.01710, val loss: 0.01778\n",
      "Training epoch: 404, train loss: 0.01650, val loss: 0.01716\n",
      "Training epoch: 405, train loss: 0.01656, val loss: 0.01717\n",
      "Training epoch: 406, train loss: 0.01651, val loss: 0.01717\n",
      "Training epoch: 407, train loss: 0.01671, val loss: 0.01743\n",
      "Training epoch: 408, train loss: 0.01673, val loss: 0.01738\n",
      "Training epoch: 409, train loss: 0.01719, val loss: 0.01792\n",
      "Training epoch: 410, train loss: 0.01669, val loss: 0.01730\n",
      "Training epoch: 411, train loss: 0.01695, val loss: 0.01760\n",
      "Training epoch: 412, train loss: 0.01669, val loss: 0.01735\n",
      "Training epoch: 413, train loss: 0.01682, val loss: 0.01752\n",
      "Training epoch: 414, train loss: 0.01649, val loss: 0.01710\n",
      "Training epoch: 415, train loss: 0.01746, val loss: 0.01815\n",
      "Training epoch: 416, train loss: 0.01670, val loss: 0.01743\n",
      "Training epoch: 417, train loss: 0.01705, val loss: 0.01775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 418, train loss: 0.01654, val loss: 0.01716\n",
      "Training epoch: 419, train loss: 0.01676, val loss: 0.01737\n",
      "Training epoch: 420, train loss: 0.01824, val loss: 0.01884\n",
      "Training epoch: 421, train loss: 0.01682, val loss: 0.01748\n",
      "Training epoch: 422, train loss: 0.01670, val loss: 0.01739\n",
      "Training epoch: 423, train loss: 0.01684, val loss: 0.01747\n",
      "Training epoch: 424, train loss: 0.01662, val loss: 0.01727\n",
      "Training epoch: 425, train loss: 0.01688, val loss: 0.01757\n",
      "Training epoch: 426, train loss: 0.01641, val loss: 0.01703\n",
      "Training epoch: 427, train loss: 0.01650, val loss: 0.01716\n",
      "Training epoch: 428, train loss: 0.01677, val loss: 0.01744\n",
      "Training epoch: 429, train loss: 0.01677, val loss: 0.01750\n",
      "Training epoch: 430, train loss: 0.01828, val loss: 0.01911\n",
      "Training epoch: 431, train loss: 0.01657, val loss: 0.01719\n",
      "Training epoch: 432, train loss: 0.01645, val loss: 0.01707\n",
      "Training epoch: 433, train loss: 0.01660, val loss: 0.01726\n",
      "Training epoch: 434, train loss: 0.01664, val loss: 0.01735\n",
      "Training epoch: 435, train loss: 0.01712, val loss: 0.01777\n",
      "Training epoch: 436, train loss: 0.01638, val loss: 0.01702\n",
      "Training epoch: 437, train loss: 0.01711, val loss: 0.01769\n",
      "Training epoch: 438, train loss: 0.01673, val loss: 0.01728\n",
      "Training epoch: 439, train loss: 0.01658, val loss: 0.01718\n",
      "Training epoch: 440, train loss: 0.01652, val loss: 0.01715\n",
      "Training epoch: 441, train loss: 0.01675, val loss: 0.01745\n",
      "Training epoch: 442, train loss: 0.01672, val loss: 0.01737\n",
      "Training epoch: 443, train loss: 0.01653, val loss: 0.01710\n",
      "Training epoch: 444, train loss: 0.01661, val loss: 0.01727\n",
      "Training epoch: 445, train loss: 0.01717, val loss: 0.01791\n",
      "Training epoch: 446, train loss: 0.01660, val loss: 0.01713\n",
      "Training epoch: 447, train loss: 0.01663, val loss: 0.01732\n",
      "Training epoch: 448, train loss: 0.01650, val loss: 0.01715\n",
      "Training epoch: 449, train loss: 0.01626, val loss: 0.01685\n",
      "Training epoch: 450, train loss: 0.01650, val loss: 0.01707\n",
      "Training epoch: 451, train loss: 0.01688, val loss: 0.01753\n",
      "Training epoch: 452, train loss: 0.01685, val loss: 0.01754\n",
      "Training epoch: 453, train loss: 0.01656, val loss: 0.01715\n",
      "Training epoch: 454, train loss: 0.01715, val loss: 0.01784\n",
      "Training epoch: 455, train loss: 0.01640, val loss: 0.01702\n",
      "Training epoch: 456, train loss: 0.01646, val loss: 0.01706\n",
      "Training epoch: 457, train loss: 0.01729, val loss: 0.01794\n",
      "Training epoch: 458, train loss: 0.01704, val loss: 0.01765\n",
      "Training epoch: 459, train loss: 0.01725, val loss: 0.01781\n",
      "Training epoch: 460, train loss: 0.01703, val loss: 0.01771\n",
      "Training epoch: 461, train loss: 0.01707, val loss: 0.01777\n",
      "Training epoch: 462, train loss: 0.01655, val loss: 0.01710\n",
      "Training epoch: 463, train loss: 0.01632, val loss: 0.01697\n",
      "Training epoch: 464, train loss: 0.01647, val loss: 0.01714\n",
      "Training epoch: 465, train loss: 0.01672, val loss: 0.01729\n",
      "Training epoch: 466, train loss: 0.01643, val loss: 0.01705\n",
      "Training epoch: 467, train loss: 0.01658, val loss: 0.01720\n",
      "Training epoch: 468, train loss: 0.01647, val loss: 0.01707\n",
      "Training epoch: 469, train loss: 0.01713, val loss: 0.01777\n",
      "Training epoch: 470, train loss: 0.01691, val loss: 0.01760\n",
      "Training epoch: 471, train loss: 0.01627, val loss: 0.01692\n",
      "Training epoch: 472, train loss: 0.01703, val loss: 0.01774\n",
      "Training epoch: 473, train loss: 0.01642, val loss: 0.01701\n",
      "Training epoch: 474, train loss: 0.01861, val loss: 0.01919\n",
      "Training epoch: 475, train loss: 0.01683, val loss: 0.01746\n",
      "Training epoch: 476, train loss: 0.01631, val loss: 0.01698\n",
      "Training epoch: 477, train loss: 0.01652, val loss: 0.01718\n",
      "Training epoch: 478, train loss: 0.01642, val loss: 0.01711\n",
      "Training epoch: 479, train loss: 0.01618, val loss: 0.01680\n",
      "Training epoch: 480, train loss: 0.01652, val loss: 0.01711\n",
      "Training epoch: 481, train loss: 0.01625, val loss: 0.01690\n",
      "Training epoch: 482, train loss: 0.01652, val loss: 0.01712\n",
      "Training epoch: 483, train loss: 0.01702, val loss: 0.01779\n",
      "Training epoch: 484, train loss: 0.01647, val loss: 0.01702\n",
      "Training epoch: 485, train loss: 0.01782, val loss: 0.01841\n",
      "Training epoch: 486, train loss: 0.01635, val loss: 0.01699\n",
      "Training epoch: 487, train loss: 0.01656, val loss: 0.01718\n",
      "Training epoch: 488, train loss: 0.01619, val loss: 0.01683\n",
      "Training epoch: 489, train loss: 0.01631, val loss: 0.01704\n",
      "Training epoch: 490, train loss: 0.01761, val loss: 0.01811\n",
      "Training epoch: 491, train loss: 0.01637, val loss: 0.01695\n",
      "Training epoch: 492, train loss: 0.01680, val loss: 0.01741\n",
      "Training epoch: 493, train loss: 0.01646, val loss: 0.01704\n",
      "Training epoch: 494, train loss: 0.01622, val loss: 0.01689\n",
      "Training epoch: 495, train loss: 0.01620, val loss: 0.01678\n",
      "Training epoch: 496, train loss: 0.01649, val loss: 0.01720\n",
      "Training epoch: 497, train loss: 0.01635, val loss: 0.01691\n",
      "Training epoch: 498, train loss: 0.01617, val loss: 0.01680\n",
      "Training epoch: 499, train loss: 0.01618, val loss: 0.01681\n",
      "Training epoch: 500, train loss: 0.01618, val loss: 0.01677\n",
      "Training epoch: 501, train loss: 0.01639, val loss: 0.01700\n",
      "Training epoch: 502, train loss: 0.01611, val loss: 0.01675\n",
      "Training epoch: 503, train loss: 0.01714, val loss: 0.01787\n",
      "Training epoch: 504, train loss: 0.01745, val loss: 0.01808\n",
      "Training epoch: 505, train loss: 0.01727, val loss: 0.01791\n",
      "Training epoch: 506, train loss: 0.01638, val loss: 0.01697\n",
      "Training epoch: 507, train loss: 0.01617, val loss: 0.01673\n",
      "Training epoch: 508, train loss: 0.01654, val loss: 0.01717\n",
      "Training epoch: 509, train loss: 0.01655, val loss: 0.01716\n",
      "Training epoch: 510, train loss: 0.01642, val loss: 0.01705\n",
      "Training epoch: 511, train loss: 0.01623, val loss: 0.01680\n",
      "Training epoch: 512, train loss: 0.01627, val loss: 0.01696\n",
      "Training epoch: 513, train loss: 0.01622, val loss: 0.01687\n",
      "Training epoch: 514, train loss: 0.01619, val loss: 0.01672\n",
      "Training epoch: 515, train loss: 0.01621, val loss: 0.01687\n",
      "Training epoch: 516, train loss: 0.01643, val loss: 0.01707\n",
      "Training epoch: 517, train loss: 0.01621, val loss: 0.01685\n",
      "Training epoch: 518, train loss: 0.01707, val loss: 0.01770\n",
      "Training epoch: 519, train loss: 0.01646, val loss: 0.01705\n",
      "Training epoch: 520, train loss: 0.01620, val loss: 0.01682\n",
      "Training epoch: 521, train loss: 0.01729, val loss: 0.01808\n",
      "Training epoch: 522, train loss: 0.01615, val loss: 0.01671\n",
      "Training epoch: 523, train loss: 0.01620, val loss: 0.01686\n",
      "Training epoch: 524, train loss: 0.01632, val loss: 0.01690\n",
      "Training epoch: 525, train loss: 0.01634, val loss: 0.01699\n",
      "Training epoch: 526, train loss: 0.01604, val loss: 0.01665\n",
      "Training epoch: 527, train loss: 0.01612, val loss: 0.01674\n",
      "Training epoch: 528, train loss: 0.01607, val loss: 0.01663\n",
      "Training epoch: 529, train loss: 0.01613, val loss: 0.01672\n",
      "Training epoch: 530, train loss: 0.01805, val loss: 0.01869\n",
      "Training epoch: 531, train loss: 0.01704, val loss: 0.01767\n",
      "Training epoch: 532, train loss: 0.01640, val loss: 0.01705\n",
      "Training epoch: 533, train loss: 0.01620, val loss: 0.01685\n",
      "Training epoch: 534, train loss: 0.01661, val loss: 0.01732\n",
      "Training epoch: 535, train loss: 0.01666, val loss: 0.01734\n",
      "Training epoch: 536, train loss: 0.01613, val loss: 0.01672\n",
      "Training epoch: 537, train loss: 0.01845, val loss: 0.01910\n",
      "Training epoch: 538, train loss: 0.01617, val loss: 0.01676\n",
      "Training epoch: 539, train loss: 0.01652, val loss: 0.01716\n",
      "Training epoch: 540, train loss: 0.01637, val loss: 0.01702\n",
      "Training epoch: 541, train loss: 0.01660, val loss: 0.01733\n",
      "Training epoch: 542, train loss: 0.01761, val loss: 0.01835\n",
      "Training epoch: 543, train loss: 0.01718, val loss: 0.01793\n",
      "Training epoch: 544, train loss: 0.01716, val loss: 0.01772\n",
      "Training epoch: 545, train loss: 0.01658, val loss: 0.01714\n",
      "Training epoch: 546, train loss: 0.01615, val loss: 0.01682\n",
      "Training epoch: 547, train loss: 0.01606, val loss: 0.01673\n",
      "Training epoch: 548, train loss: 0.01708, val loss: 0.01766\n",
      "Training epoch: 549, train loss: 0.01615, val loss: 0.01676\n",
      "Training epoch: 550, train loss: 0.01612, val loss: 0.01673\n",
      "Training epoch: 551, train loss: 0.01632, val loss: 0.01685\n",
      "Training epoch: 552, train loss: 0.01630, val loss: 0.01696\n",
      "Training epoch: 553, train loss: 0.01600, val loss: 0.01660\n",
      "Training epoch: 554, train loss: 0.01604, val loss: 0.01672\n",
      "Training epoch: 555, train loss: 0.01614, val loss: 0.01673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 556, train loss: 0.01623, val loss: 0.01681\n",
      "Training epoch: 557, train loss: 0.01632, val loss: 0.01702\n",
      "Training epoch: 558, train loss: 0.01607, val loss: 0.01668\n",
      "Training epoch: 559, train loss: 0.01639, val loss: 0.01693\n",
      "Training epoch: 560, train loss: 0.01638, val loss: 0.01702\n",
      "Training epoch: 561, train loss: 0.01661, val loss: 0.01731\n",
      "Training epoch: 562, train loss: 0.01613, val loss: 0.01679\n",
      "Training epoch: 563, train loss: 0.01722, val loss: 0.01802\n",
      "Training epoch: 564, train loss: 0.01617, val loss: 0.01679\n",
      "Training epoch: 565, train loss: 0.01674, val loss: 0.01727\n",
      "Training epoch: 566, train loss: 0.01683, val loss: 0.01751\n",
      "Training epoch: 567, train loss: 0.01655, val loss: 0.01713\n",
      "Training epoch: 568, train loss: 0.01641, val loss: 0.01707\n",
      "Training epoch: 569, train loss: 0.01635, val loss: 0.01706\n",
      "Training epoch: 570, train loss: 0.01617, val loss: 0.01678\n",
      "Training epoch: 571, train loss: 0.01610, val loss: 0.01674\n",
      "Training epoch: 572, train loss: 0.01610, val loss: 0.01667\n",
      "Training epoch: 573, train loss: 0.01601, val loss: 0.01668\n",
      "Training epoch: 574, train loss: 0.01693, val loss: 0.01763\n",
      "Training epoch: 575, train loss: 0.01624, val loss: 0.01694\n",
      "Training epoch: 576, train loss: 0.01600, val loss: 0.01662\n",
      "Training epoch: 577, train loss: 0.01644, val loss: 0.01698\n",
      "Training epoch: 578, train loss: 0.01607, val loss: 0.01666\n",
      "Training epoch: 579, train loss: 0.01615, val loss: 0.01677\n",
      "Training epoch: 580, train loss: 0.01736, val loss: 0.01799\n",
      "Training epoch: 581, train loss: 0.01630, val loss: 0.01687\n",
      "Training epoch: 582, train loss: 0.01637, val loss: 0.01695\n",
      "Training epoch: 583, train loss: 0.01649, val loss: 0.01717\n",
      "Training epoch: 584, train loss: 0.01629, val loss: 0.01696\n",
      "Training epoch: 585, train loss: 0.01659, val loss: 0.01715\n",
      "Training epoch: 586, train loss: 0.01686, val loss: 0.01759\n",
      "Training epoch: 587, train loss: 0.01615, val loss: 0.01682\n",
      "Training epoch: 588, train loss: 0.01644, val loss: 0.01702\n",
      "Training epoch: 589, train loss: 0.01631, val loss: 0.01686\n",
      "Training epoch: 590, train loss: 0.01701, val loss: 0.01774\n",
      "Training epoch: 591, train loss: 0.01664, val loss: 0.01715\n",
      "Training epoch: 592, train loss: 0.01610, val loss: 0.01674\n",
      "Training epoch: 593, train loss: 0.01601, val loss: 0.01659\n",
      "Training epoch: 594, train loss: 0.01611, val loss: 0.01677\n",
      "Training epoch: 595, train loss: 0.01625, val loss: 0.01681\n",
      "Training epoch: 596, train loss: 0.01671, val loss: 0.01746\n",
      "Training epoch: 597, train loss: 0.01662, val loss: 0.01719\n",
      "Training epoch: 598, train loss: 0.01597, val loss: 0.01663\n",
      "Training epoch: 599, train loss: 0.01689, val loss: 0.01739\n",
      "Training epoch: 600, train loss: 0.01612, val loss: 0.01675\n",
      "Training epoch: 601, train loss: 0.01631, val loss: 0.01693\n",
      "Training epoch: 602, train loss: 0.01641, val loss: 0.01705\n",
      "Training epoch: 603, train loss: 0.01607, val loss: 0.01668\n",
      "Training epoch: 604, train loss: 0.01598, val loss: 0.01655\n",
      "Training epoch: 605, train loss: 0.01613, val loss: 0.01679\n",
      "Training epoch: 606, train loss: 0.01648, val loss: 0.01721\n",
      "Training epoch: 607, train loss: 0.01653, val loss: 0.01717\n",
      "Training epoch: 608, train loss: 0.01602, val loss: 0.01663\n",
      "Training epoch: 609, train loss: 0.01624, val loss: 0.01690\n",
      "Training epoch: 610, train loss: 0.01678, val loss: 0.01742\n",
      "Training epoch: 611, train loss: 0.01805, val loss: 0.01888\n",
      "Training epoch: 612, train loss: 0.01626, val loss: 0.01690\n",
      "Training epoch: 613, train loss: 0.01599, val loss: 0.01658\n",
      "Training epoch: 614, train loss: 0.01742, val loss: 0.01811\n",
      "Training epoch: 615, train loss: 0.01647, val loss: 0.01708\n",
      "Training epoch: 616, train loss: 0.01660, val loss: 0.01722\n",
      "Training epoch: 617, train loss: 0.01675, val loss: 0.01738\n",
      "Training epoch: 618, train loss: 0.01654, val loss: 0.01718\n",
      "Training epoch: 619, train loss: 0.01649, val loss: 0.01707\n",
      "Training epoch: 620, train loss: 0.01602, val loss: 0.01666\n",
      "Training epoch: 621, train loss: 0.01643, val loss: 0.01703\n",
      "Training epoch: 622, train loss: 0.01607, val loss: 0.01669\n",
      "Training epoch: 623, train loss: 0.01595, val loss: 0.01661\n",
      "Training epoch: 624, train loss: 0.01606, val loss: 0.01668\n",
      "Training epoch: 625, train loss: 0.01610, val loss: 0.01678\n",
      "Training epoch: 626, train loss: 0.01630, val loss: 0.01685\n",
      "Training epoch: 627, train loss: 0.01628, val loss: 0.01696\n",
      "Training epoch: 628, train loss: 0.01642, val loss: 0.01716\n",
      "Training epoch: 629, train loss: 0.01597, val loss: 0.01667\n",
      "Training epoch: 630, train loss: 0.01627, val loss: 0.01694\n",
      "Training epoch: 631, train loss: 0.01622, val loss: 0.01680\n",
      "Training epoch: 632, train loss: 0.01647, val loss: 0.01717\n",
      "Training epoch: 633, train loss: 0.01626, val loss: 0.01687\n",
      "Training epoch: 634, train loss: 0.01593, val loss: 0.01656\n",
      "Training epoch: 635, train loss: 0.01693, val loss: 0.01751\n",
      "Training epoch: 636, train loss: 0.01607, val loss: 0.01673\n",
      "Training epoch: 637, train loss: 0.01605, val loss: 0.01675\n",
      "Training epoch: 638, train loss: 0.01627, val loss: 0.01692\n",
      "Training epoch: 639, train loss: 0.01605, val loss: 0.01666\n",
      "Training epoch: 640, train loss: 0.01610, val loss: 0.01674\n",
      "Training epoch: 641, train loss: 0.01617, val loss: 0.01688\n",
      "Training epoch: 642, train loss: 0.01611, val loss: 0.01667\n",
      "Training epoch: 643, train loss: 0.01613, val loss: 0.01675\n",
      "Training epoch: 644, train loss: 0.01590, val loss: 0.01655\n",
      "Training epoch: 645, train loss: 0.01592, val loss: 0.01661\n",
      "Training epoch: 646, train loss: 0.01620, val loss: 0.01677\n",
      "Training epoch: 647, train loss: 0.01600, val loss: 0.01660\n",
      "Training epoch: 648, train loss: 0.01597, val loss: 0.01660\n",
      "Training epoch: 649, train loss: 0.01638, val loss: 0.01707\n",
      "Training epoch: 650, train loss: 0.01747, val loss: 0.01824\n",
      "Training epoch: 651, train loss: 0.01627, val loss: 0.01687\n",
      "Training epoch: 652, train loss: 0.01630, val loss: 0.01693\n",
      "Training epoch: 653, train loss: 0.01596, val loss: 0.01656\n",
      "Training epoch: 654, train loss: 0.01597, val loss: 0.01665\n",
      "Training epoch: 655, train loss: 0.01643, val loss: 0.01701\n",
      "Training epoch: 656, train loss: 0.01703, val loss: 0.01761\n",
      "Training epoch: 657, train loss: 0.01599, val loss: 0.01666\n",
      "Training epoch: 658, train loss: 0.01605, val loss: 0.01674\n",
      "Training epoch: 659, train loss: 0.01653, val loss: 0.01720\n",
      "Training epoch: 660, train loss: 0.01607, val loss: 0.01671\n",
      "Training epoch: 661, train loss: 0.01594, val loss: 0.01655\n",
      "Training epoch: 662, train loss: 0.01617, val loss: 0.01690\n",
      "Training epoch: 663, train loss: 0.01602, val loss: 0.01672\n",
      "Training epoch: 664, train loss: 0.01597, val loss: 0.01661\n",
      "Training epoch: 665, train loss: 0.01614, val loss: 0.01669\n",
      "Training epoch: 666, train loss: 0.01618, val loss: 0.01683\n",
      "Training epoch: 667, train loss: 0.01596, val loss: 0.01665\n",
      "Training epoch: 668, train loss: 0.01623, val loss: 0.01690\n",
      "Training epoch: 669, train loss: 0.01604, val loss: 0.01664\n",
      "Training epoch: 670, train loss: 0.01663, val loss: 0.01738\n",
      "Training epoch: 671, train loss: 0.01696, val loss: 0.01772\n",
      "Training epoch: 672, train loss: 0.01629, val loss: 0.01698\n",
      "Training epoch: 673, train loss: 0.01666, val loss: 0.01728\n",
      "Training epoch: 674, train loss: 0.01606, val loss: 0.01678\n",
      "Training epoch: 675, train loss: 0.01606, val loss: 0.01671\n",
      "Training epoch: 676, train loss: 0.01617, val loss: 0.01685\n",
      "Training epoch: 677, train loss: 0.01613, val loss: 0.01674\n",
      "Training epoch: 678, train loss: 0.01596, val loss: 0.01659\n",
      "Training epoch: 679, train loss: 0.01594, val loss: 0.01658\n",
      "Training epoch: 680, train loss: 0.01595, val loss: 0.01657\n",
      "Training epoch: 681, train loss: 0.01605, val loss: 0.01672\n",
      "Training epoch: 682, train loss: 0.01602, val loss: 0.01672\n",
      "Training epoch: 683, train loss: 0.01646, val loss: 0.01713\n",
      "Training epoch: 684, train loss: 0.01614, val loss: 0.01679\n",
      "Training epoch: 685, train loss: 0.01608, val loss: 0.01674\n",
      "Training epoch: 686, train loss: 0.01579, val loss: 0.01648\n",
      "Training epoch: 687, train loss: 0.01669, val loss: 0.01740\n",
      "Training epoch: 688, train loss: 0.01840, val loss: 0.01903\n",
      "Training epoch: 689, train loss: 0.01614, val loss: 0.01677\n",
      "Training epoch: 690, train loss: 0.01640, val loss: 0.01706\n",
      "Training epoch: 691, train loss: 0.01593, val loss: 0.01654\n",
      "Training epoch: 692, train loss: 0.01584, val loss: 0.01656\n",
      "Training epoch: 693, train loss: 0.01652, val loss: 0.01715\n",
      "Training epoch: 694, train loss: 0.01628, val loss: 0.01698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 695, train loss: 0.01604, val loss: 0.01668\n",
      "Training epoch: 696, train loss: 0.01587, val loss: 0.01653\n",
      "Training epoch: 697, train loss: 0.01607, val loss: 0.01674\n",
      "Training epoch: 698, train loss: 0.01602, val loss: 0.01670\n",
      "Training epoch: 699, train loss: 0.01611, val loss: 0.01679\n",
      "Training epoch: 700, train loss: 0.01587, val loss: 0.01662\n",
      "Training epoch: 701, train loss: 0.01594, val loss: 0.01661\n",
      "Training epoch: 702, train loss: 0.01657, val loss: 0.01719\n",
      "Training epoch: 703, train loss: 0.01659, val loss: 0.01727\n",
      "Training epoch: 704, train loss: 0.01616, val loss: 0.01681\n",
      "Training epoch: 705, train loss: 0.01602, val loss: 0.01670\n",
      "Training epoch: 706, train loss: 0.01604, val loss: 0.01663\n",
      "Training epoch: 707, train loss: 0.01624, val loss: 0.01684\n",
      "Training epoch: 708, train loss: 0.01604, val loss: 0.01670\n",
      "Training epoch: 709, train loss: 0.01618, val loss: 0.01689\n",
      "Training epoch: 710, train loss: 0.01597, val loss: 0.01668\n",
      "Training epoch: 711, train loss: 0.01624, val loss: 0.01693\n",
      "Training epoch: 712, train loss: 0.01668, val loss: 0.01741\n",
      "Training epoch: 713, train loss: 0.01683, val loss: 0.01751\n",
      "Training epoch: 714, train loss: 0.01627, val loss: 0.01691\n",
      "Training epoch: 715, train loss: 0.01641, val loss: 0.01708\n",
      "Training epoch: 716, train loss: 0.01596, val loss: 0.01660\n",
      "Training epoch: 717, train loss: 0.01622, val loss: 0.01690\n",
      "Training epoch: 718, train loss: 0.01620, val loss: 0.01694\n",
      "Training epoch: 719, train loss: 0.01630, val loss: 0.01700\n",
      "Training epoch: 720, train loss: 0.01602, val loss: 0.01670\n",
      "Training epoch: 721, train loss: 0.01624, val loss: 0.01692\n",
      "Training epoch: 722, train loss: 0.01582, val loss: 0.01647\n",
      "Training epoch: 723, train loss: 0.01580, val loss: 0.01646\n",
      "Training epoch: 724, train loss: 0.01629, val loss: 0.01698\n",
      "Training epoch: 725, train loss: 0.01591, val loss: 0.01660\n",
      "Training epoch: 726, train loss: 0.01601, val loss: 0.01665\n",
      "Training epoch: 727, train loss: 0.01717, val loss: 0.01775\n",
      "Training epoch: 728, train loss: 0.01613, val loss: 0.01678\n",
      "Training epoch: 729, train loss: 0.01579, val loss: 0.01651\n",
      "Training epoch: 730, train loss: 0.01588, val loss: 0.01660\n",
      "Training epoch: 731, train loss: 0.01635, val loss: 0.01704\n",
      "Training epoch: 732, train loss: 0.01593, val loss: 0.01657\n",
      "Training epoch: 733, train loss: 0.01584, val loss: 0.01651\n",
      "Training epoch: 734, train loss: 0.01583, val loss: 0.01650\n",
      "Training epoch: 735, train loss: 0.01634, val loss: 0.01707\n",
      "Training epoch: 736, train loss: 0.01598, val loss: 0.01665\n",
      "Training epoch: 737, train loss: 0.01579, val loss: 0.01643\n",
      "Training epoch: 738, train loss: 0.01590, val loss: 0.01655\n",
      "Training epoch: 739, train loss: 0.01668, val loss: 0.01744\n",
      "Training epoch: 740, train loss: 0.01670, val loss: 0.01733\n",
      "Training epoch: 741, train loss: 0.01578, val loss: 0.01645\n",
      "Training epoch: 742, train loss: 0.01588, val loss: 0.01654\n",
      "Training epoch: 743, train loss: 0.01599, val loss: 0.01663\n",
      "Training epoch: 744, train loss: 0.01578, val loss: 0.01643\n",
      "Training epoch: 745, train loss: 0.01664, val loss: 0.01736\n",
      "Training epoch: 746, train loss: 0.01625, val loss: 0.01691\n",
      "Training epoch: 747, train loss: 0.01643, val loss: 0.01703\n",
      "Training epoch: 748, train loss: 0.01605, val loss: 0.01676\n",
      "Training epoch: 749, train loss: 0.01615, val loss: 0.01681\n",
      "Training epoch: 750, train loss: 0.01574, val loss: 0.01640\n",
      "Training epoch: 751, train loss: 0.01609, val loss: 0.01674\n",
      "Training epoch: 752, train loss: 0.01658, val loss: 0.01723\n",
      "Training epoch: 753, train loss: 0.01576, val loss: 0.01643\n",
      "Training epoch: 754, train loss: 0.01589, val loss: 0.01655\n",
      "Training epoch: 755, train loss: 0.01637, val loss: 0.01705\n",
      "Training epoch: 756, train loss: 0.01585, val loss: 0.01650\n",
      "Training epoch: 757, train loss: 0.01591, val loss: 0.01659\n",
      "Training epoch: 758, train loss: 0.01580, val loss: 0.01650\n",
      "Training epoch: 759, train loss: 0.01594, val loss: 0.01657\n",
      "Training epoch: 760, train loss: 0.01597, val loss: 0.01664\n",
      "Training epoch: 761, train loss: 0.01641, val loss: 0.01712\n",
      "Training epoch: 762, train loss: 0.01739, val loss: 0.01823\n",
      "Training epoch: 763, train loss: 0.01596, val loss: 0.01655\n",
      "Training epoch: 764, train loss: 0.01614, val loss: 0.01680\n",
      "Training epoch: 765, train loss: 0.01672, val loss: 0.01739\n",
      "Training epoch: 766, train loss: 0.01584, val loss: 0.01654\n",
      "Training epoch: 767, train loss: 0.01607, val loss: 0.01674\n",
      "Training epoch: 768, train loss: 0.01599, val loss: 0.01667\n",
      "Training epoch: 769, train loss: 0.01579, val loss: 0.01642\n",
      "Training epoch: 770, train loss: 0.01612, val loss: 0.01683\n",
      "Training epoch: 771, train loss: 0.01631, val loss: 0.01695\n",
      "Training epoch: 772, train loss: 0.01589, val loss: 0.01658\n",
      "Training epoch: 773, train loss: 0.01596, val loss: 0.01660\n",
      "Training epoch: 774, train loss: 0.01603, val loss: 0.01674\n",
      "Training epoch: 775, train loss: 0.01567, val loss: 0.01635\n",
      "Training epoch: 776, train loss: 0.01621, val loss: 0.01689\n",
      "Training epoch: 777, train loss: 0.01631, val loss: 0.01695\n",
      "Training epoch: 778, train loss: 0.01637, val loss: 0.01706\n",
      "Training epoch: 779, train loss: 0.01582, val loss: 0.01653\n",
      "Training epoch: 780, train loss: 0.01610, val loss: 0.01681\n",
      "Training epoch: 781, train loss: 0.01599, val loss: 0.01669\n",
      "Training epoch: 782, train loss: 0.01567, val loss: 0.01631\n",
      "Training epoch: 783, train loss: 0.01577, val loss: 0.01643\n",
      "Training epoch: 784, train loss: 0.01567, val loss: 0.01633\n",
      "Training epoch: 785, train loss: 0.01574, val loss: 0.01643\n",
      "Training epoch: 786, train loss: 0.01588, val loss: 0.01656\n",
      "Training epoch: 787, train loss: 0.01581, val loss: 0.01650\n",
      "Training epoch: 788, train loss: 0.01633, val loss: 0.01692\n",
      "Training epoch: 789, train loss: 0.01604, val loss: 0.01668\n",
      "Training epoch: 790, train loss: 0.01584, val loss: 0.01652\n",
      "Training epoch: 791, train loss: 0.01576, val loss: 0.01642\n",
      "Training epoch: 792, train loss: 0.01583, val loss: 0.01650\n",
      "Training epoch: 793, train loss: 0.01611, val loss: 0.01677\n",
      "Training epoch: 794, train loss: 0.01580, val loss: 0.01648\n",
      "Training epoch: 795, train loss: 0.01563, val loss: 0.01630\n",
      "Training epoch: 796, train loss: 0.01573, val loss: 0.01640\n",
      "Training epoch: 797, train loss: 0.01585, val loss: 0.01650\n",
      "Training epoch: 798, train loss: 0.01678, val loss: 0.01749\n",
      "Training epoch: 799, train loss: 0.01567, val loss: 0.01637\n",
      "Training epoch: 800, train loss: 0.01621, val loss: 0.01684\n",
      "Training epoch: 801, train loss: 0.01575, val loss: 0.01642\n",
      "Training epoch: 802, train loss: 0.01641, val loss: 0.01704\n",
      "Training epoch: 803, train loss: 0.01574, val loss: 0.01639\n",
      "Training epoch: 804, train loss: 0.01582, val loss: 0.01647\n",
      "Training epoch: 805, train loss: 0.01649, val loss: 0.01717\n",
      "Training epoch: 806, train loss: 0.01647, val loss: 0.01713\n",
      "Training epoch: 807, train loss: 0.01616, val loss: 0.01686\n",
      "Training epoch: 808, train loss: 0.01593, val loss: 0.01656\n",
      "Training epoch: 809, train loss: 0.01633, val loss: 0.01703\n",
      "Training epoch: 810, train loss: 0.01597, val loss: 0.01662\n",
      "Training epoch: 811, train loss: 0.01564, val loss: 0.01631\n",
      "Training epoch: 812, train loss: 0.01565, val loss: 0.01636\n",
      "Training epoch: 813, train loss: 0.01611, val loss: 0.01675\n",
      "Training epoch: 814, train loss: 0.01580, val loss: 0.01645\n",
      "Training epoch: 815, train loss: 0.01581, val loss: 0.01649\n",
      "Training epoch: 816, train loss: 0.01569, val loss: 0.01635\n",
      "Training epoch: 817, train loss: 0.01614, val loss: 0.01678\n",
      "Training epoch: 818, train loss: 0.01633, val loss: 0.01702\n",
      "Training epoch: 819, train loss: 0.01592, val loss: 0.01658\n",
      "Training epoch: 820, train loss: 0.01634, val loss: 0.01695\n",
      "Training epoch: 821, train loss: 0.01603, val loss: 0.01674\n",
      "Training epoch: 822, train loss: 0.01605, val loss: 0.01671\n",
      "Training epoch: 823, train loss: 0.01644, val loss: 0.01715\n",
      "Training epoch: 824, train loss: 0.01573, val loss: 0.01642\n",
      "Training epoch: 825, train loss: 0.01628, val loss: 0.01691\n",
      "Training epoch: 826, train loss: 0.01659, val loss: 0.01729\n",
      "Training epoch: 827, train loss: 0.01670, val loss: 0.01741\n",
      "Training epoch: 828, train loss: 0.01587, val loss: 0.01651\n",
      "Training epoch: 829, train loss: 0.01607, val loss: 0.01673\n",
      "Training epoch: 830, train loss: 0.01580, val loss: 0.01649\n",
      "Training epoch: 831, train loss: 0.01567, val loss: 0.01634\n",
      "Training epoch: 832, train loss: 0.01572, val loss: 0.01637\n",
      "Training epoch: 833, train loss: 0.01566, val loss: 0.01636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 834, train loss: 0.01561, val loss: 0.01624\n",
      "Training epoch: 835, train loss: 0.01567, val loss: 0.01634\n",
      "Training epoch: 836, train loss: 0.01577, val loss: 0.01647\n",
      "Training epoch: 837, train loss: 0.01588, val loss: 0.01647\n",
      "Training epoch: 838, train loss: 0.01591, val loss: 0.01655\n",
      "Training epoch: 839, train loss: 0.01563, val loss: 0.01629\n",
      "Training epoch: 840, train loss: 0.01587, val loss: 0.01657\n",
      "Training epoch: 841, train loss: 0.01625, val loss: 0.01692\n",
      "Training epoch: 842, train loss: 0.01564, val loss: 0.01625\n",
      "Training epoch: 843, train loss: 0.01563, val loss: 0.01633\n",
      "Training epoch: 844, train loss: 0.01571, val loss: 0.01638\n",
      "Training epoch: 845, train loss: 0.01559, val loss: 0.01622\n",
      "Training epoch: 846, train loss: 0.01565, val loss: 0.01629\n",
      "Training epoch: 847, train loss: 0.01562, val loss: 0.01629\n",
      "Training epoch: 848, train loss: 0.01598, val loss: 0.01665\n",
      "Training epoch: 849, train loss: 0.01614, val loss: 0.01690\n",
      "Training epoch: 850, train loss: 0.01561, val loss: 0.01628\n",
      "Training epoch: 851, train loss: 0.01774, val loss: 0.01844\n",
      "Training epoch: 852, train loss: 0.01625, val loss: 0.01686\n",
      "Training epoch: 853, train loss: 0.01595, val loss: 0.01664\n",
      "Training epoch: 854, train loss: 0.01553, val loss: 0.01617\n",
      "Training epoch: 855, train loss: 0.01599, val loss: 0.01666\n",
      "Training epoch: 856, train loss: 0.01593, val loss: 0.01660\n",
      "Training epoch: 857, train loss: 0.01594, val loss: 0.01659\n",
      "Training epoch: 858, train loss: 0.01574, val loss: 0.01641\n",
      "Training epoch: 859, train loss: 0.01568, val loss: 0.01632\n",
      "Training epoch: 860, train loss: 0.01570, val loss: 0.01637\n",
      "Training epoch: 861, train loss: 0.01581, val loss: 0.01645\n",
      "Training epoch: 862, train loss: 0.01557, val loss: 0.01623\n",
      "Training epoch: 863, train loss: 0.01575, val loss: 0.01640\n",
      "Training epoch: 864, train loss: 0.01583, val loss: 0.01651\n",
      "Training epoch: 865, train loss: 0.01573, val loss: 0.01643\n",
      "Training epoch: 866, train loss: 0.01567, val loss: 0.01631\n",
      "Training epoch: 867, train loss: 0.01558, val loss: 0.01621\n",
      "Training epoch: 868, train loss: 0.01559, val loss: 0.01624\n",
      "Training epoch: 869, train loss: 0.01563, val loss: 0.01627\n",
      "Training epoch: 870, train loss: 0.01578, val loss: 0.01645\n",
      "Training epoch: 871, train loss: 0.01563, val loss: 0.01625\n",
      "Training epoch: 872, train loss: 0.01560, val loss: 0.01621\n",
      "Training epoch: 873, train loss: 0.01575, val loss: 0.01635\n",
      "Training epoch: 874, train loss: 0.01560, val loss: 0.01626\n",
      "Training epoch: 875, train loss: 0.01596, val loss: 0.01660\n",
      "Training epoch: 876, train loss: 0.01564, val loss: 0.01631\n",
      "Training epoch: 877, train loss: 0.01551, val loss: 0.01617\n",
      "Training epoch: 878, train loss: 0.01554, val loss: 0.01617\n",
      "Training epoch: 879, train loss: 0.01593, val loss: 0.01658\n",
      "Training epoch: 880, train loss: 0.01584, val loss: 0.01654\n",
      "Training epoch: 881, train loss: 0.01567, val loss: 0.01632\n",
      "Training epoch: 882, train loss: 0.01592, val loss: 0.01657\n",
      "Training epoch: 883, train loss: 0.01561, val loss: 0.01628\n",
      "Training epoch: 884, train loss: 0.01589, val loss: 0.01656\n",
      "Training epoch: 885, train loss: 0.01583, val loss: 0.01648\n",
      "Training epoch: 886, train loss: 0.01584, val loss: 0.01650\n",
      "Training epoch: 887, train loss: 0.01553, val loss: 0.01616\n",
      "Training epoch: 888, train loss: 0.01555, val loss: 0.01619\n",
      "Training epoch: 889, train loss: 0.01603, val loss: 0.01670\n",
      "Training epoch: 890, train loss: 0.01565, val loss: 0.01632\n",
      "Training epoch: 891, train loss: 0.01564, val loss: 0.01631\n",
      "Training epoch: 892, train loss: 0.01620, val loss: 0.01674\n",
      "Training epoch: 893, train loss: 0.01626, val loss: 0.01688\n",
      "Training epoch: 894, train loss: 0.01554, val loss: 0.01619\n",
      "Training epoch: 895, train loss: 0.01575, val loss: 0.01639\n",
      "Training epoch: 896, train loss: 0.01596, val loss: 0.01663\n",
      "Training epoch: 897, train loss: 0.01590, val loss: 0.01660\n",
      "Training epoch: 898, train loss: 0.01617, val loss: 0.01678\n",
      "Training epoch: 899, train loss: 0.01548, val loss: 0.01608\n",
      "Training epoch: 900, train loss: 0.01546, val loss: 0.01611\n",
      "Training epoch: 901, train loss: 0.01595, val loss: 0.01659\n",
      "Training epoch: 902, train loss: 0.01570, val loss: 0.01627\n",
      "Training epoch: 903, train loss: 0.01687, val loss: 0.01752\n",
      "Training epoch: 904, train loss: 0.01572, val loss: 0.01634\n",
      "Training epoch: 905, train loss: 0.01560, val loss: 0.01625\n",
      "Training epoch: 906, train loss: 0.01554, val loss: 0.01619\n",
      "Training epoch: 907, train loss: 0.01593, val loss: 0.01659\n",
      "Training epoch: 908, train loss: 0.01571, val loss: 0.01632\n",
      "Training epoch: 909, train loss: 0.01560, val loss: 0.01623\n",
      "Training epoch: 910, train loss: 0.01748, val loss: 0.01807\n",
      "Training epoch: 911, train loss: 0.01599, val loss: 0.01664\n",
      "Training epoch: 912, train loss: 0.01592, val loss: 0.01656\n",
      "Training epoch: 913, train loss: 0.01578, val loss: 0.01640\n",
      "Training epoch: 914, train loss: 0.01549, val loss: 0.01615\n",
      "Training epoch: 915, train loss: 0.01546, val loss: 0.01611\n",
      "Training epoch: 916, train loss: 0.01558, val loss: 0.01622\n",
      "Training epoch: 917, train loss: 0.01605, val loss: 0.01671\n",
      "Training epoch: 918, train loss: 0.01549, val loss: 0.01609\n",
      "Training epoch: 919, train loss: 0.01547, val loss: 0.01612\n",
      "Training epoch: 920, train loss: 0.01541, val loss: 0.01606\n",
      "Training epoch: 921, train loss: 0.01616, val loss: 0.01676\n",
      "Training epoch: 922, train loss: 0.01550, val loss: 0.01614\n",
      "Training epoch: 923, train loss: 0.01552, val loss: 0.01618\n",
      "Training epoch: 924, train loss: 0.01591, val loss: 0.01655\n",
      "Training epoch: 925, train loss: 0.01603, val loss: 0.01668\n",
      "Training epoch: 926, train loss: 0.01669, val loss: 0.01730\n",
      "Training epoch: 927, train loss: 0.01647, val loss: 0.01710\n",
      "Training epoch: 928, train loss: 0.01587, val loss: 0.01645\n",
      "Training epoch: 929, train loss: 0.01552, val loss: 0.01612\n",
      "Training epoch: 930, train loss: 0.01588, val loss: 0.01654\n",
      "Training epoch: 931, train loss: 0.01548, val loss: 0.01612\n",
      "Training epoch: 932, train loss: 0.01587, val loss: 0.01647\n",
      "Training epoch: 933, train loss: 0.01580, val loss: 0.01642\n",
      "Training epoch: 934, train loss: 0.01549, val loss: 0.01610\n",
      "Training epoch: 935, train loss: 0.01551, val loss: 0.01615\n",
      "Training epoch: 936, train loss: 0.01568, val loss: 0.01631\n",
      "Training epoch: 937, train loss: 0.01545, val loss: 0.01609\n",
      "Training epoch: 938, train loss: 0.01575, val loss: 0.01634\n",
      "Training epoch: 939, train loss: 0.01585, val loss: 0.01648\n",
      "Training epoch: 940, train loss: 0.01589, val loss: 0.01648\n",
      "Training epoch: 941, train loss: 0.01549, val loss: 0.01610\n",
      "Training epoch: 942, train loss: 0.01652, val loss: 0.01712\n",
      "Training epoch: 943, train loss: 0.01562, val loss: 0.01621\n",
      "Training epoch: 944, train loss: 0.01563, val loss: 0.01624\n",
      "Training epoch: 945, train loss: 0.01543, val loss: 0.01606\n",
      "Training epoch: 946, train loss: 0.01557, val loss: 0.01619\n",
      "Training epoch: 947, train loss: 0.01549, val loss: 0.01612\n",
      "Training epoch: 948, train loss: 0.01557, val loss: 0.01622\n",
      "Training epoch: 949, train loss: 0.01557, val loss: 0.01620\n",
      "Training epoch: 950, train loss: 0.01545, val loss: 0.01607\n",
      "Training epoch: 951, train loss: 0.01574, val loss: 0.01637\n",
      "Training epoch: 952, train loss: 0.01563, val loss: 0.01621\n",
      "Training epoch: 953, train loss: 0.01563, val loss: 0.01625\n",
      "Training epoch: 954, train loss: 0.01558, val loss: 0.01620\n",
      "Training epoch: 955, train loss: 0.01567, val loss: 0.01626\n",
      "Training epoch: 956, train loss: 0.01551, val loss: 0.01615\n",
      "Training epoch: 957, train loss: 0.01566, val loss: 0.01623\n",
      "Training epoch: 958, train loss: 0.01579, val loss: 0.01641\n",
      "Training epoch: 959, train loss: 0.01563, val loss: 0.01618\n",
      "Training epoch: 960, train loss: 0.01634, val loss: 0.01692\n",
      "Training epoch: 961, train loss: 0.01538, val loss: 0.01601\n",
      "Training epoch: 962, train loss: 0.01552, val loss: 0.01613\n",
      "Training epoch: 963, train loss: 0.01545, val loss: 0.01606\n",
      "Training epoch: 964, train loss: 0.01551, val loss: 0.01611\n",
      "Training epoch: 965, train loss: 0.01535, val loss: 0.01595\n",
      "Training epoch: 966, train loss: 0.01553, val loss: 0.01613\n",
      "Training epoch: 967, train loss: 0.01555, val loss: 0.01614\n",
      "Training epoch: 968, train loss: 0.01554, val loss: 0.01614\n",
      "Training epoch: 969, train loss: 0.01569, val loss: 0.01632\n",
      "Training epoch: 970, train loss: 0.01546, val loss: 0.01607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 971, train loss: 0.01554, val loss: 0.01615\n",
      "Training epoch: 972, train loss: 0.01557, val loss: 0.01618\n",
      "Training epoch: 973, train loss: 0.01556, val loss: 0.01617\n",
      "Training epoch: 974, train loss: 0.01550, val loss: 0.01610\n",
      "Training epoch: 975, train loss: 0.01617, val loss: 0.01674\n",
      "Training epoch: 976, train loss: 0.01589, val loss: 0.01648\n",
      "Training epoch: 977, train loss: 0.01592, val loss: 0.01653\n",
      "Training epoch: 978, train loss: 0.01581, val loss: 0.01646\n",
      "Training epoch: 979, train loss: 0.01537, val loss: 0.01599\n",
      "Training epoch: 980, train loss: 0.01560, val loss: 0.01623\n",
      "Training epoch: 981, train loss: 0.01664, val loss: 0.01729\n",
      "Training epoch: 982, train loss: 0.01558, val loss: 0.01615\n",
      "Training epoch: 983, train loss: 0.01572, val loss: 0.01630\n",
      "Training epoch: 984, train loss: 0.01583, val loss: 0.01640\n",
      "Training epoch: 985, train loss: 0.01574, val loss: 0.01635\n",
      "Training epoch: 986, train loss: 0.01544, val loss: 0.01608\n",
      "Training epoch: 987, train loss: 0.01576, val loss: 0.01637\n",
      "Training epoch: 988, train loss: 0.01547, val loss: 0.01606\n",
      "Training epoch: 989, train loss: 0.01538, val loss: 0.01600\n",
      "Training epoch: 990, train loss: 0.01536, val loss: 0.01600\n",
      "Training epoch: 991, train loss: 0.01543, val loss: 0.01602\n",
      "Training epoch: 992, train loss: 0.01561, val loss: 0.01617\n",
      "Training epoch: 993, train loss: 0.01548, val loss: 0.01610\n",
      "Training epoch: 994, train loss: 0.01544, val loss: 0.01603\n",
      "Training epoch: 995, train loss: 0.01544, val loss: 0.01607\n",
      "Training epoch: 996, train loss: 0.01565, val loss: 0.01623\n",
      "Training epoch: 997, train loss: 0.01571, val loss: 0.01628\n",
      "Training epoch: 998, train loss: 0.01550, val loss: 0.01612\n",
      "Training epoch: 999, train loss: 0.01537, val loss: 0.01595\n",
      "Training epoch: 1000, train loss: 0.01681, val loss: 0.01750\n",
      "Training epoch: 1001, train loss: 0.01569, val loss: 0.01630\n",
      "Training epoch: 1002, train loss: 0.01585, val loss: 0.01642\n",
      "Training epoch: 1003, train loss: 0.01692, val loss: 0.01748\n",
      "Training epoch: 1004, train loss: 0.01563, val loss: 0.01625\n",
      "Training epoch: 1005, train loss: 0.01552, val loss: 0.01613\n",
      "Training epoch: 1006, train loss: 0.01712, val loss: 0.01782\n",
      "Training epoch: 1007, train loss: 0.01599, val loss: 0.01662\n",
      "Training epoch: 1008, train loss: 0.01547, val loss: 0.01603\n",
      "Training epoch: 1009, train loss: 0.01539, val loss: 0.01601\n",
      "Training epoch: 1010, train loss: 0.01556, val loss: 0.01618\n",
      "Training epoch: 1011, train loss: 0.01541, val loss: 0.01602\n",
      "Training epoch: 1012, train loss: 0.01553, val loss: 0.01609\n",
      "Training epoch: 1013, train loss: 0.01584, val loss: 0.01643\n",
      "Training epoch: 1014, train loss: 0.01589, val loss: 0.01653\n",
      "Training epoch: 1015, train loss: 0.01598, val loss: 0.01662\n",
      "Training epoch: 1016, train loss: 0.01551, val loss: 0.01607\n",
      "Training epoch: 1017, train loss: 0.01574, val loss: 0.01631\n",
      "Training epoch: 1018, train loss: 0.01538, val loss: 0.01599\n",
      "Training epoch: 1019, train loss: 0.01546, val loss: 0.01607\n",
      "Training epoch: 1020, train loss: 0.01540, val loss: 0.01602\n",
      "Training epoch: 1021, train loss: 0.01552, val loss: 0.01612\n",
      "Training epoch: 1022, train loss: 0.01568, val loss: 0.01626\n",
      "Training epoch: 1023, train loss: 0.01535, val loss: 0.01594\n",
      "Training epoch: 1024, train loss: 0.01553, val loss: 0.01614\n",
      "Training epoch: 1025, train loss: 0.01571, val loss: 0.01633\n",
      "Training epoch: 1026, train loss: 0.01547, val loss: 0.01607\n",
      "Training epoch: 1027, train loss: 0.01541, val loss: 0.01596\n",
      "Training epoch: 1028, train loss: 0.01541, val loss: 0.01603\n",
      "Training epoch: 1029, train loss: 0.01579, val loss: 0.01641\n",
      "Training epoch: 1030, train loss: 0.01566, val loss: 0.01625\n",
      "Training epoch: 1031, train loss: 0.01545, val loss: 0.01602\n",
      "Training epoch: 1032, train loss: 0.01565, val loss: 0.01623\n",
      "Training epoch: 1033, train loss: 0.01542, val loss: 0.01600\n",
      "Training epoch: 1034, train loss: 0.01540, val loss: 0.01596\n",
      "Training epoch: 1035, train loss: 0.01582, val loss: 0.01635\n",
      "Training epoch: 1036, train loss: 0.01591, val loss: 0.01656\n",
      "Training epoch: 1037, train loss: 0.01601, val loss: 0.01660\n",
      "Training epoch: 1038, train loss: 0.01562, val loss: 0.01619\n",
      "Training epoch: 1039, train loss: 0.01541, val loss: 0.01604\n",
      "Training epoch: 1040, train loss: 0.01562, val loss: 0.01619\n",
      "Training epoch: 1041, train loss: 0.01565, val loss: 0.01623\n",
      "Training epoch: 1042, train loss: 0.01636, val loss: 0.01694\n",
      "Training epoch: 1043, train loss: 0.01540, val loss: 0.01596\n",
      "Training epoch: 1044, train loss: 0.01552, val loss: 0.01611\n",
      "Training epoch: 1045, train loss: 0.01540, val loss: 0.01596\n",
      "Training epoch: 1046, train loss: 0.01536, val loss: 0.01597\n",
      "Training epoch: 1047, train loss: 0.01539, val loss: 0.01602\n",
      "Training epoch: 1048, train loss: 0.01546, val loss: 0.01605\n",
      "Training epoch: 1049, train loss: 0.01574, val loss: 0.01633\n",
      "Training epoch: 1050, train loss: 0.01588, val loss: 0.01649\n",
      "Training epoch: 1051, train loss: 0.01614, val loss: 0.01673\n",
      "Training epoch: 1052, train loss: 0.01558, val loss: 0.01614\n",
      "Training epoch: 1053, train loss: 0.01549, val loss: 0.01605\n",
      "Training epoch: 1054, train loss: 0.01577, val loss: 0.01639\n",
      "Training epoch: 1055, train loss: 0.01548, val loss: 0.01606\n",
      "Training epoch: 1056, train loss: 0.01580, val loss: 0.01640\n",
      "Training epoch: 1057, train loss: 0.01586, val loss: 0.01647\n",
      "Training epoch: 1058, train loss: 0.01540, val loss: 0.01599\n",
      "Training epoch: 1059, train loss: 0.01540, val loss: 0.01596\n",
      "Training epoch: 1060, train loss: 0.01550, val loss: 0.01611\n",
      "Training epoch: 1061, train loss: 0.01541, val loss: 0.01602\n",
      "Training epoch: 1062, train loss: 0.01545, val loss: 0.01602\n",
      "Training epoch: 1063, train loss: 0.01537, val loss: 0.01596\n",
      "Training epoch: 1064, train loss: 0.01550, val loss: 0.01610\n",
      "Training epoch: 1065, train loss: 0.01584, val loss: 0.01642\n",
      "Training epoch: 1066, train loss: 0.01567, val loss: 0.01626\n",
      "Training epoch: 1067, train loss: 0.01568, val loss: 0.01626\n",
      "Training epoch: 1068, train loss: 0.01602, val loss: 0.01660\n",
      "Training epoch: 1069, train loss: 0.01547, val loss: 0.01604\n",
      "Training epoch: 1070, train loss: 0.01545, val loss: 0.01600\n",
      "Training epoch: 1071, train loss: 0.01552, val loss: 0.01608\n",
      "Training epoch: 1072, train loss: 0.01597, val loss: 0.01654\n",
      "Training epoch: 1073, train loss: 0.01561, val loss: 0.01622\n",
      "Training epoch: 1074, train loss: 0.01555, val loss: 0.01611\n",
      "Training epoch: 1075, train loss: 0.01590, val loss: 0.01648\n",
      "Training epoch: 1076, train loss: 0.01619, val loss: 0.01677\n",
      "Training epoch: 1077, train loss: 0.01558, val loss: 0.01616\n",
      "Training epoch: 1078, train loss: 0.01535, val loss: 0.01596\n",
      "Training epoch: 1079, train loss: 0.01556, val loss: 0.01613\n",
      "Training epoch: 1080, train loss: 0.01534, val loss: 0.01593\n",
      "Training epoch: 1081, train loss: 0.01533, val loss: 0.01587\n",
      "Training epoch: 1082, train loss: 0.01531, val loss: 0.01586\n",
      "Training epoch: 1083, train loss: 0.01574, val loss: 0.01631\n",
      "Training epoch: 1084, train loss: 0.01555, val loss: 0.01609\n",
      "Training epoch: 1085, train loss: 0.01537, val loss: 0.01598\n",
      "Training epoch: 1086, train loss: 0.01544, val loss: 0.01599\n",
      "Training epoch: 1087, train loss: 0.01531, val loss: 0.01588\n",
      "Training epoch: 1088, train loss: 0.01539, val loss: 0.01599\n",
      "Training epoch: 1089, train loss: 0.01533, val loss: 0.01590\n",
      "Training epoch: 1090, train loss: 0.01558, val loss: 0.01615\n",
      "Training epoch: 1091, train loss: 0.01532, val loss: 0.01586\n",
      "Training epoch: 1092, train loss: 0.01542, val loss: 0.01602\n",
      "Training epoch: 1093, train loss: 0.01588, val loss: 0.01648\n",
      "Training epoch: 1094, train loss: 0.01592, val loss: 0.01649\n",
      "Training epoch: 1095, train loss: 0.01557, val loss: 0.01608\n",
      "Training epoch: 1096, train loss: 0.01625, val loss: 0.01676\n",
      "Training epoch: 1097, train loss: 0.01574, val loss: 0.01631\n",
      "Training epoch: 1098, train loss: 0.01617, val loss: 0.01674\n",
      "Training epoch: 1099, train loss: 0.01551, val loss: 0.01610\n",
      "Training epoch: 1100, train loss: 0.01537, val loss: 0.01592\n",
      "Training epoch: 1101, train loss: 0.01552, val loss: 0.01609\n",
      "Training epoch: 1102, train loss: 0.01609, val loss: 0.01669\n",
      "Training epoch: 1103, train loss: 0.01587, val loss: 0.01642\n",
      "Training epoch: 1104, train loss: 0.01640, val loss: 0.01701\n",
      "Training epoch: 1105, train loss: 0.01573, val loss: 0.01631\n",
      "Training epoch: 1106, train loss: 0.01546, val loss: 0.01599\n",
      "Training epoch: 1107, train loss: 0.01575, val loss: 0.01634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1108, train loss: 0.01550, val loss: 0.01608\n",
      "Training epoch: 1109, train loss: 0.01575, val loss: 0.01630\n",
      "Training epoch: 1110, train loss: 0.01558, val loss: 0.01616\n",
      "Training epoch: 1111, train loss: 0.01532, val loss: 0.01589\n",
      "Training epoch: 1112, train loss: 0.01539, val loss: 0.01596\n",
      "Training epoch: 1113, train loss: 0.01568, val loss: 0.01627\n",
      "Training epoch: 1114, train loss: 0.01537, val loss: 0.01590\n",
      "Training epoch: 1115, train loss: 0.01558, val loss: 0.01615\n",
      "Training epoch: 1116, train loss: 0.01547, val loss: 0.01605\n",
      "Training epoch: 1117, train loss: 0.01552, val loss: 0.01605\n",
      "Training epoch: 1118, train loss: 0.01544, val loss: 0.01600\n",
      "Training epoch: 1119, train loss: 0.01552, val loss: 0.01610\n",
      "Training epoch: 1120, train loss: 0.01534, val loss: 0.01591\n",
      "Training epoch: 1121, train loss: 0.01533, val loss: 0.01588\n",
      "Training epoch: 1122, train loss: 0.01547, val loss: 0.01602\n",
      "Training epoch: 1123, train loss: 0.01526, val loss: 0.01582\n",
      "Training epoch: 1124, train loss: 0.01549, val loss: 0.01604\n",
      "Training epoch: 1125, train loss: 0.01581, val loss: 0.01635\n",
      "Training epoch: 1126, train loss: 0.01554, val loss: 0.01607\n",
      "Training epoch: 1127, train loss: 0.01585, val loss: 0.01641\n",
      "Training epoch: 1128, train loss: 0.01553, val loss: 0.01606\n",
      "Training epoch: 1129, train loss: 0.01573, val loss: 0.01632\n",
      "Training epoch: 1130, train loss: 0.01633, val loss: 0.01688\n",
      "Training epoch: 1131, train loss: 0.01612, val loss: 0.01668\n",
      "Training epoch: 1132, train loss: 0.01542, val loss: 0.01596\n",
      "Training epoch: 1133, train loss: 0.01551, val loss: 0.01602\n",
      "Training epoch: 1134, train loss: 0.01550, val loss: 0.01603\n",
      "Training epoch: 1135, train loss: 0.01552, val loss: 0.01606\n",
      "Training epoch: 1136, train loss: 0.01539, val loss: 0.01593\n",
      "Training epoch: 1137, train loss: 0.01531, val loss: 0.01587\n",
      "Training epoch: 1138, train loss: 0.01553, val loss: 0.01611\n",
      "Training epoch: 1139, train loss: 0.01535, val loss: 0.01589\n",
      "Training epoch: 1140, train loss: 0.01546, val loss: 0.01604\n",
      "Training epoch: 1141, train loss: 0.01545, val loss: 0.01602\n",
      "Training epoch: 1142, train loss: 0.01563, val loss: 0.01614\n",
      "Training epoch: 1143, train loss: 0.01537, val loss: 0.01587\n",
      "Training epoch: 1144, train loss: 0.01650, val loss: 0.01706\n",
      "Training epoch: 1145, train loss: 0.01551, val loss: 0.01606\n",
      "Training epoch: 1146, train loss: 0.01541, val loss: 0.01597\n",
      "Training epoch: 1147, train loss: 0.01538, val loss: 0.01593\n",
      "Training epoch: 1148, train loss: 0.01581, val loss: 0.01635\n",
      "Training epoch: 1149, train loss: 0.01566, val loss: 0.01620\n",
      "Training epoch: 1150, train loss: 0.01559, val loss: 0.01618\n",
      "Training epoch: 1151, train loss: 0.01529, val loss: 0.01579\n",
      "Training epoch: 1152, train loss: 0.01536, val loss: 0.01591\n",
      "Training epoch: 1153, train loss: 0.01530, val loss: 0.01585\n",
      "Training epoch: 1154, train loss: 0.01532, val loss: 0.01587\n",
      "Training epoch: 1155, train loss: 0.01555, val loss: 0.01608\n",
      "Training epoch: 1156, train loss: 0.01531, val loss: 0.01586\n",
      "Training epoch: 1157, train loss: 0.01563, val loss: 0.01617\n",
      "Training epoch: 1158, train loss: 0.01530, val loss: 0.01584\n",
      "Training epoch: 1159, train loss: 0.01631, val loss: 0.01690\n",
      "Training epoch: 1160, train loss: 0.01561, val loss: 0.01618\n",
      "Training epoch: 1161, train loss: 0.01555, val loss: 0.01608\n",
      "Training epoch: 1162, train loss: 0.01555, val loss: 0.01610\n",
      "Training epoch: 1163, train loss: 0.01657, val loss: 0.01714\n",
      "Training epoch: 1164, train loss: 0.01549, val loss: 0.01602\n",
      "Training epoch: 1165, train loss: 0.01606, val loss: 0.01659\n",
      "Training epoch: 1166, train loss: 0.01592, val loss: 0.01637\n",
      "Training epoch: 1167, train loss: 0.01565, val loss: 0.01617\n",
      "Training epoch: 1168, train loss: 0.01579, val loss: 0.01635\n",
      "Training epoch: 1169, train loss: 0.01778, val loss: 0.01833\n",
      "Training epoch: 1170, train loss: 0.01605, val loss: 0.01665\n",
      "Training epoch: 1171, train loss: 0.01557, val loss: 0.01605\n",
      "Training epoch: 1172, train loss: 0.01561, val loss: 0.01614\n",
      "Training epoch: 1173, train loss: 0.01561, val loss: 0.01610\n",
      "Training epoch: 1174, train loss: 0.01541, val loss: 0.01595\n",
      "Training epoch: 1175, train loss: 0.01543, val loss: 0.01594\n",
      "Training epoch: 1176, train loss: 0.01539, val loss: 0.01593\n",
      "Training epoch: 1177, train loss: 0.01531, val loss: 0.01582\n",
      "Training epoch: 1178, train loss: 0.01541, val loss: 0.01595\n",
      "Training epoch: 1179, train loss: 0.01579, val loss: 0.01630\n",
      "Training epoch: 1180, train loss: 0.01530, val loss: 0.01582\n",
      "Training epoch: 1181, train loss: 0.01546, val loss: 0.01601\n",
      "Training epoch: 1182, train loss: 0.01619, val loss: 0.01675\n",
      "Training epoch: 1183, train loss: 0.01526, val loss: 0.01580\n",
      "Training epoch: 1184, train loss: 0.01532, val loss: 0.01586\n",
      "Training epoch: 1185, train loss: 0.01557, val loss: 0.01614\n",
      "Training epoch: 1186, train loss: 0.01532, val loss: 0.01589\n",
      "Training epoch: 1187, train loss: 0.01530, val loss: 0.01581\n",
      "Training epoch: 1188, train loss: 0.01532, val loss: 0.01585\n",
      "Training epoch: 1189, train loss: 0.01535, val loss: 0.01587\n",
      "Training epoch: 1190, train loss: 0.01532, val loss: 0.01583\n",
      "Training epoch: 1191, train loss: 0.01560, val loss: 0.01615\n",
      "Training epoch: 1192, train loss: 0.01533, val loss: 0.01585\n",
      "Training epoch: 1193, train loss: 0.01558, val loss: 0.01606\n",
      "Training epoch: 1194, train loss: 0.01635, val loss: 0.01688\n",
      "Training epoch: 1195, train loss: 0.01556, val loss: 0.01603\n",
      "Training epoch: 1196, train loss: 0.01530, val loss: 0.01582\n",
      "Training epoch: 1197, train loss: 0.01560, val loss: 0.01612\n",
      "Training epoch: 1198, train loss: 0.01546, val loss: 0.01601\n",
      "Training epoch: 1199, train loss: 0.01628, val loss: 0.01683\n",
      "Training epoch: 1200, train loss: 0.01533, val loss: 0.01584\n",
      "Training epoch: 1201, train loss: 0.01531, val loss: 0.01582\n",
      "Training epoch: 1202, train loss: 0.01535, val loss: 0.01586\n",
      "Training epoch: 1203, train loss: 0.01558, val loss: 0.01612\n",
      "Training epoch: 1204, train loss: 0.01619, val loss: 0.01670\n",
      "Training epoch: 1205, train loss: 0.01621, val loss: 0.01669\n",
      "Training epoch: 1206, train loss: 0.01531, val loss: 0.01583\n",
      "Training epoch: 1207, train loss: 0.01535, val loss: 0.01586\n",
      "Training epoch: 1208, train loss: 0.01588, val loss: 0.01645\n",
      "Training epoch: 1209, train loss: 0.01560, val loss: 0.01609\n",
      "Training epoch: 1210, train loss: 0.01538, val loss: 0.01591\n",
      "Training epoch: 1211, train loss: 0.01555, val loss: 0.01602\n",
      "Training epoch: 1212, train loss: 0.01529, val loss: 0.01584\n",
      "Training epoch: 1213, train loss: 0.01542, val loss: 0.01593\n",
      "Training epoch: 1214, train loss: 0.01532, val loss: 0.01583\n",
      "Training epoch: 1215, train loss: 0.01555, val loss: 0.01604\n",
      "Training epoch: 1216, train loss: 0.01528, val loss: 0.01582\n",
      "Training epoch: 1217, train loss: 0.01553, val loss: 0.01604\n",
      "Training epoch: 1218, train loss: 0.01587, val loss: 0.01641\n",
      "Training epoch: 1219, train loss: 0.01560, val loss: 0.01609\n",
      "Training epoch: 1220, train loss: 0.01563, val loss: 0.01610\n",
      "Training epoch: 1221, train loss: 0.01590, val loss: 0.01639\n",
      "Training epoch: 1222, train loss: 0.01542, val loss: 0.01595\n",
      "Training epoch: 1223, train loss: 0.01559, val loss: 0.01609\n",
      "Training epoch: 1224, train loss: 0.01540, val loss: 0.01594\n",
      "Training epoch: 1225, train loss: 0.01548, val loss: 0.01600\n",
      "Training epoch: 1226, train loss: 0.01578, val loss: 0.01632\n",
      "Training epoch: 1227, train loss: 0.01528, val loss: 0.01581\n",
      "Training epoch: 1228, train loss: 0.01532, val loss: 0.01585\n",
      "Training epoch: 1229, train loss: 0.01585, val loss: 0.01637\n",
      "Training epoch: 1230, train loss: 0.01530, val loss: 0.01584\n",
      "Training epoch: 1231, train loss: 0.01543, val loss: 0.01596\n",
      "Training epoch: 1232, train loss: 0.01574, val loss: 0.01625\n",
      "Training epoch: 1233, train loss: 0.01559, val loss: 0.01610\n",
      "Training epoch: 1234, train loss: 0.01569, val loss: 0.01625\n",
      "Training epoch: 1235, train loss: 0.01570, val loss: 0.01616\n",
      "Training epoch: 1236, train loss: 0.01557, val loss: 0.01607\n",
      "Training epoch: 1237, train loss: 0.01570, val loss: 0.01617\n",
      "Training epoch: 1238, train loss: 0.01583, val loss: 0.01634\n",
      "Training epoch: 1239, train loss: 0.01537, val loss: 0.01590\n",
      "Training epoch: 1240, train loss: 0.01619, val loss: 0.01671\n",
      "Training epoch: 1241, train loss: 0.01607, val loss: 0.01654\n",
      "Training epoch: 1242, train loss: 0.01584, val loss: 0.01630\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1243, train loss: 0.01532, val loss: 0.01581\n",
      "Training epoch: 1244, train loss: 0.01543, val loss: 0.01600\n",
      "Training epoch: 1245, train loss: 0.01559, val loss: 0.01613\n",
      "Training epoch: 1246, train loss: 0.01559, val loss: 0.01609\n",
      "Training epoch: 1247, train loss: 0.01618, val loss: 0.01665\n",
      "Training epoch: 1248, train loss: 0.01541, val loss: 0.01593\n",
      "Training epoch: 1249, train loss: 0.01550, val loss: 0.01608\n",
      "Training epoch: 1250, train loss: 0.01539, val loss: 0.01590\n",
      "Training epoch: 1251, train loss: 0.01533, val loss: 0.01585\n",
      "Training epoch: 1252, train loss: 0.01562, val loss: 0.01614\n",
      "Training epoch: 1253, train loss: 0.01524, val loss: 0.01574\n",
      "Training epoch: 1254, train loss: 0.01553, val loss: 0.01601\n",
      "Training epoch: 1255, train loss: 0.01529, val loss: 0.01579\n",
      "Training epoch: 1256, train loss: 0.01534, val loss: 0.01588\n",
      "Training epoch: 1257, train loss: 0.01532, val loss: 0.01581\n",
      "Training epoch: 1258, train loss: 0.01528, val loss: 0.01578\n",
      "Training epoch: 1259, train loss: 0.01640, val loss: 0.01694\n",
      "Training epoch: 1260, train loss: 0.01532, val loss: 0.01582\n",
      "Training epoch: 1261, train loss: 0.01548, val loss: 0.01595\n",
      "Training epoch: 1262, train loss: 0.01532, val loss: 0.01584\n",
      "Training epoch: 1263, train loss: 0.01582, val loss: 0.01637\n",
      "Training epoch: 1264, train loss: 0.01577, val loss: 0.01628\n",
      "Training epoch: 1265, train loss: 0.01554, val loss: 0.01608\n",
      "Training epoch: 1266, train loss: 0.01542, val loss: 0.01595\n",
      "Training epoch: 1267, train loss: 0.01536, val loss: 0.01588\n",
      "Training epoch: 1268, train loss: 0.01549, val loss: 0.01601\n",
      "Training epoch: 1269, train loss: 0.01532, val loss: 0.01588\n",
      "Training epoch: 1270, train loss: 0.01564, val loss: 0.01612\n",
      "Training epoch: 1271, train loss: 0.01569, val loss: 0.01621\n",
      "Training epoch: 1272, train loss: 0.01550, val loss: 0.01596\n",
      "Training epoch: 1273, train loss: 0.01521, val loss: 0.01569\n",
      "Training epoch: 1274, train loss: 0.01537, val loss: 0.01588\n",
      "Training epoch: 1275, train loss: 0.01554, val loss: 0.01604\n",
      "Training epoch: 1276, train loss: 0.01530, val loss: 0.01577\n",
      "Training epoch: 1277, train loss: 0.01549, val loss: 0.01600\n",
      "Training epoch: 1278, train loss: 0.01532, val loss: 0.01583\n",
      "Training epoch: 1279, train loss: 0.01535, val loss: 0.01590\n",
      "Training epoch: 1280, train loss: 0.01580, val loss: 0.01627\n",
      "Training epoch: 1281, train loss: 0.01562, val loss: 0.01609\n",
      "Training epoch: 1282, train loss: 0.01555, val loss: 0.01607\n",
      "Training epoch: 1283, train loss: 0.01527, val loss: 0.01576\n",
      "Training epoch: 1284, train loss: 0.01587, val loss: 0.01637\n",
      "Training epoch: 1285, train loss: 0.01545, val loss: 0.01599\n",
      "Training epoch: 1286, train loss: 0.01536, val loss: 0.01589\n",
      "Training epoch: 1287, train loss: 0.01528, val loss: 0.01576\n",
      "Training epoch: 1288, train loss: 0.01526, val loss: 0.01576\n",
      "Training epoch: 1289, train loss: 0.01522, val loss: 0.01574\n",
      "Training epoch: 1290, train loss: 0.01576, val loss: 0.01628\n",
      "Training epoch: 1291, train loss: 0.01556, val loss: 0.01602\n",
      "Training epoch: 1292, train loss: 0.01552, val loss: 0.01601\n",
      "Training epoch: 1293, train loss: 0.01566, val loss: 0.01619\n",
      "Training epoch: 1294, train loss: 0.01536, val loss: 0.01586\n",
      "Training epoch: 1295, train loss: 0.01560, val loss: 0.01608\n",
      "Training epoch: 1296, train loss: 0.01536, val loss: 0.01591\n",
      "Training epoch: 1297, train loss: 0.01576, val loss: 0.01619\n",
      "Training epoch: 1298, train loss: 0.01543, val loss: 0.01594\n",
      "Training epoch: 1299, train loss: 0.01545, val loss: 0.01593\n",
      "Training epoch: 1300, train loss: 0.01553, val loss: 0.01601\n",
      "Training epoch: 1301, train loss: 0.01569, val loss: 0.01622\n",
      "Training epoch: 1302, train loss: 0.01525, val loss: 0.01575\n",
      "Training epoch: 1303, train loss: 0.01540, val loss: 0.01592\n",
      "Training epoch: 1304, train loss: 0.01545, val loss: 0.01598\n",
      "Training epoch: 1305, train loss: 0.01538, val loss: 0.01590\n",
      "Training epoch: 1306, train loss: 0.01528, val loss: 0.01577\n",
      "Training epoch: 1307, train loss: 0.01546, val loss: 0.01595\n",
      "Training epoch: 1308, train loss: 0.01540, val loss: 0.01587\n",
      "Training epoch: 1309, train loss: 0.01526, val loss: 0.01575\n",
      "Training epoch: 1310, train loss: 0.01534, val loss: 0.01586\n",
      "Training epoch: 1311, train loss: 0.01538, val loss: 0.01587\n",
      "Training epoch: 1312, train loss: 0.01543, val loss: 0.01593\n",
      "Training epoch: 1313, train loss: 0.01596, val loss: 0.01649\n",
      "Training epoch: 1314, train loss: 0.01556, val loss: 0.01610\n",
      "Training epoch: 1315, train loss: 0.01533, val loss: 0.01583\n",
      "Training epoch: 1316, train loss: 0.01524, val loss: 0.01574\n",
      "Training epoch: 1317, train loss: 0.01538, val loss: 0.01589\n",
      "Training epoch: 1318, train loss: 0.01532, val loss: 0.01584\n",
      "Training epoch: 1319, train loss: 0.01535, val loss: 0.01586\n",
      "Training epoch: 1320, train loss: 0.01529, val loss: 0.01576\n",
      "Training epoch: 1321, train loss: 0.01548, val loss: 0.01597\n",
      "Training epoch: 1322, train loss: 0.01535, val loss: 0.01586\n",
      "Training epoch: 1323, train loss: 0.01578, val loss: 0.01620\n",
      "Training epoch: 1324, train loss: 0.01529, val loss: 0.01578\n",
      "Training epoch: 1325, train loss: 0.01563, val loss: 0.01616\n",
      "Training epoch: 1326, train loss: 0.01523, val loss: 0.01574\n",
      "Training epoch: 1327, train loss: 0.01556, val loss: 0.01607\n",
      "Training epoch: 1328, train loss: 0.01570, val loss: 0.01620\n",
      "Training epoch: 1329, train loss: 0.01576, val loss: 0.01622\n",
      "Training epoch: 1330, train loss: 0.01536, val loss: 0.01588\n",
      "Training epoch: 1331, train loss: 0.01568, val loss: 0.01614\n",
      "Training epoch: 1332, train loss: 0.01571, val loss: 0.01618\n",
      "Training epoch: 1333, train loss: 0.01550, val loss: 0.01603\n",
      "Training epoch: 1334, train loss: 0.01538, val loss: 0.01589\n",
      "Training epoch: 1335, train loss: 0.01555, val loss: 0.01604\n",
      "Training epoch: 1336, train loss: 0.01540, val loss: 0.01587\n",
      "Training epoch: 1337, train loss: 0.01527, val loss: 0.01578\n",
      "Training epoch: 1338, train loss: 0.01536, val loss: 0.01585\n",
      "Training epoch: 1339, train loss: 0.01564, val loss: 0.01613\n",
      "Training epoch: 1340, train loss: 0.01541, val loss: 0.01592\n",
      "Training epoch: 1341, train loss: 0.01552, val loss: 0.01598\n",
      "Training epoch: 1342, train loss: 0.01545, val loss: 0.01592\n",
      "Training epoch: 1343, train loss: 0.01534, val loss: 0.01585\n",
      "Training epoch: 1344, train loss: 0.01527, val loss: 0.01576\n",
      "Training epoch: 1345, train loss: 0.01556, val loss: 0.01608\n",
      "Training epoch: 1346, train loss: 0.01541, val loss: 0.01593\n",
      "Training epoch: 1347, train loss: 0.01527, val loss: 0.01575\n",
      "Training epoch: 1348, train loss: 0.01536, val loss: 0.01586\n",
      "Training epoch: 1349, train loss: 0.01550, val loss: 0.01598\n",
      "Training epoch: 1350, train loss: 0.01534, val loss: 0.01582\n",
      "Training epoch: 1351, train loss: 0.01563, val loss: 0.01619\n",
      "Training epoch: 1352, train loss: 0.01533, val loss: 0.01583\n",
      "Training epoch: 1353, train loss: 0.01609, val loss: 0.01660\n",
      "Training epoch: 1354, train loss: 0.01570, val loss: 0.01619\n",
      "Training epoch: 1355, train loss: 0.01539, val loss: 0.01587\n",
      "Training epoch: 1356, train loss: 0.01557, val loss: 0.01609\n",
      "Training epoch: 1357, train loss: 0.01567, val loss: 0.01615\n",
      "Training epoch: 1358, train loss: 0.01556, val loss: 0.01607\n",
      "Training epoch: 1359, train loss: 0.01549, val loss: 0.01596\n",
      "Training epoch: 1360, train loss: 0.01533, val loss: 0.01583\n",
      "Training epoch: 1361, train loss: 0.01557, val loss: 0.01607\n",
      "Training epoch: 1362, train loss: 0.01548, val loss: 0.01599\n",
      "Training epoch: 1363, train loss: 0.01537, val loss: 0.01583\n",
      "Training epoch: 1364, train loss: 0.01586, val loss: 0.01637\n",
      "Training epoch: 1365, train loss: 0.01529, val loss: 0.01582\n",
      "Training epoch: 1366, train loss: 0.01541, val loss: 0.01589\n",
      "Training epoch: 1367, train loss: 0.01549, val loss: 0.01598\n",
      "Training epoch: 1368, train loss: 0.01545, val loss: 0.01593\n",
      "Training epoch: 1369, train loss: 0.01541, val loss: 0.01589\n",
      "Training epoch: 1370, train loss: 0.01527, val loss: 0.01575\n",
      "Training epoch: 1371, train loss: 0.01568, val loss: 0.01620\n",
      "Training epoch: 1372, train loss: 0.01563, val loss: 0.01609\n",
      "Training epoch: 1373, train loss: 0.01545, val loss: 0.01591\n",
      "Training epoch: 1374, train loss: 0.01559, val loss: 0.01606\n",
      "Training epoch: 1375, train loss: 0.01531, val loss: 0.01581\n",
      "Training epoch: 1376, train loss: 0.01535, val loss: 0.01585\n",
      "Training epoch: 1377, train loss: 0.01522, val loss: 0.01567\n",
      "Training epoch: 1378, train loss: 0.01573, val loss: 0.01625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1379, train loss: 0.01559, val loss: 0.01610\n",
      "Training epoch: 1380, train loss: 0.01547, val loss: 0.01596\n",
      "Training epoch: 1381, train loss: 0.01526, val loss: 0.01574\n",
      "Training epoch: 1382, train loss: 0.01528, val loss: 0.01578\n",
      "Training epoch: 1383, train loss: 0.01556, val loss: 0.01605\n",
      "Training epoch: 1384, train loss: 0.01543, val loss: 0.01592\n",
      "Training epoch: 1385, train loss: 0.01563, val loss: 0.01614\n",
      "Training epoch: 1386, train loss: 0.01543, val loss: 0.01592\n",
      "Training epoch: 1387, train loss: 0.01533, val loss: 0.01580\n",
      "Training epoch: 1388, train loss: 0.01534, val loss: 0.01583\n",
      "Training epoch: 1389, train loss: 0.01525, val loss: 0.01575\n",
      "Training epoch: 1390, train loss: 0.01562, val loss: 0.01612\n",
      "Training epoch: 1391, train loss: 0.01537, val loss: 0.01586\n",
      "Training epoch: 1392, train loss: 0.01547, val loss: 0.01599\n",
      "Training epoch: 1393, train loss: 0.01569, val loss: 0.01618\n",
      "Training epoch: 1394, train loss: 0.01536, val loss: 0.01585\n",
      "Training epoch: 1395, train loss: 0.01540, val loss: 0.01591\n",
      "Training epoch: 1396, train loss: 0.01554, val loss: 0.01607\n",
      "Training epoch: 1397, train loss: 0.01552, val loss: 0.01600\n",
      "Training epoch: 1398, train loss: 0.01535, val loss: 0.01580\n",
      "Training epoch: 1399, train loss: 0.01591, val loss: 0.01645\n",
      "Training epoch: 1400, train loss: 0.01532, val loss: 0.01580\n",
      "Training epoch: 1401, train loss: 0.01532, val loss: 0.01579\n",
      "Training epoch: 1402, train loss: 0.01533, val loss: 0.01580\n",
      "Training epoch: 1403, train loss: 0.01540, val loss: 0.01592\n",
      "Training epoch: 1404, train loss: 0.01524, val loss: 0.01572\n",
      "Training epoch: 1405, train loss: 0.01538, val loss: 0.01584\n",
      "Training epoch: 1406, train loss: 0.01533, val loss: 0.01583\n",
      "Training epoch: 1407, train loss: 0.01532, val loss: 0.01579\n",
      "Training epoch: 1408, train loss: 0.01578, val loss: 0.01627\n",
      "Training epoch: 1409, train loss: 0.01598, val loss: 0.01645\n",
      "Training epoch: 1410, train loss: 0.01547, val loss: 0.01597\n",
      "Training epoch: 1411, train loss: 0.01550, val loss: 0.01596\n",
      "Training epoch: 1412, train loss: 0.01523, val loss: 0.01574\n",
      "Training epoch: 1413, train loss: 0.01565, val loss: 0.01614\n",
      "Training epoch: 1414, train loss: 0.01558, val loss: 0.01605\n",
      "Training epoch: 1415, train loss: 0.01595, val loss: 0.01644\n",
      "Training epoch: 1416, train loss: 0.01588, val loss: 0.01638\n",
      "Training epoch: 1417, train loss: 0.01550, val loss: 0.01602\n",
      "Training epoch: 1418, train loss: 0.01627, val loss: 0.01680\n",
      "Training epoch: 1419, train loss: 0.01550, val loss: 0.01600\n",
      "Training epoch: 1420, train loss: 0.01546, val loss: 0.01595\n",
      "Training epoch: 1421, train loss: 0.01544, val loss: 0.01589\n",
      "Training epoch: 1422, train loss: 0.01526, val loss: 0.01575\n",
      "Training epoch: 1423, train loss: 0.01538, val loss: 0.01589\n",
      "Training epoch: 1424, train loss: 0.01540, val loss: 0.01590\n",
      "Training epoch: 1425, train loss: 0.01538, val loss: 0.01587\n",
      "Training epoch: 1426, train loss: 0.01598, val loss: 0.01647\n",
      "Training epoch: 1427, train loss: 0.01632, val loss: 0.01683\n",
      "Training epoch: 1428, train loss: 0.01570, val loss: 0.01615\n",
      "Training epoch: 1429, train loss: 0.01547, val loss: 0.01599\n",
      "Training epoch: 1430, train loss: 0.01565, val loss: 0.01621\n",
      "Training epoch: 1431, train loss: 0.01575, val loss: 0.01626\n",
      "Training epoch: 1432, train loss: 0.01522, val loss: 0.01571\n",
      "Training epoch: 1433, train loss: 0.01552, val loss: 0.01604\n",
      "Training epoch: 1434, train loss: 0.01538, val loss: 0.01586\n",
      "Training epoch: 1435, train loss: 0.01563, val loss: 0.01612\n",
      "Training epoch: 1436, train loss: 0.01580, val loss: 0.01628\n",
      "Training epoch: 1437, train loss: 0.01550, val loss: 0.01600\n",
      "Training epoch: 1438, train loss: 0.01550, val loss: 0.01596\n",
      "Training epoch: 1439, train loss: 0.01532, val loss: 0.01581\n",
      "Training epoch: 1440, train loss: 0.01532, val loss: 0.01583\n",
      "Training epoch: 1441, train loss: 0.01537, val loss: 0.01586\n",
      "Training epoch: 1442, train loss: 0.01546, val loss: 0.01591\n",
      "Training epoch: 1443, train loss: 0.01547, val loss: 0.01597\n",
      "Training epoch: 1444, train loss: 0.01567, val loss: 0.01616\n",
      "Training epoch: 1445, train loss: 0.01539, val loss: 0.01585\n",
      "Training epoch: 1446, train loss: 0.01585, val loss: 0.01636\n",
      "Training epoch: 1447, train loss: 0.01555, val loss: 0.01596\n",
      "Training epoch: 1448, train loss: 0.01614, val loss: 0.01663\n",
      "Training epoch: 1449, train loss: 0.01535, val loss: 0.01584\n",
      "Training epoch: 1450, train loss: 0.01697, val loss: 0.01746\n",
      "Training epoch: 1451, train loss: 0.01609, val loss: 0.01659\n",
      "Training epoch: 1452, train loss: 0.01544, val loss: 0.01590\n",
      "Training epoch: 1453, train loss: 0.01535, val loss: 0.01582\n",
      "Training epoch: 1454, train loss: 0.01571, val loss: 0.01622\n",
      "Training epoch: 1455, train loss: 0.01570, val loss: 0.01618\n",
      "Training epoch: 1456, train loss: 0.01528, val loss: 0.01575\n",
      "Training epoch: 1457, train loss: 0.01534, val loss: 0.01578\n",
      "Training epoch: 1458, train loss: 0.01541, val loss: 0.01590\n",
      "Training epoch: 1459, train loss: 0.01614, val loss: 0.01667\n",
      "Training epoch: 1460, train loss: 0.01590, val loss: 0.01641\n",
      "Training epoch: 1461, train loss: 0.01547, val loss: 0.01595\n",
      "Training epoch: 1462, train loss: 0.01546, val loss: 0.01595\n",
      "Training epoch: 1463, train loss: 0.01538, val loss: 0.01588\n",
      "Training epoch: 1464, train loss: 0.01621, val loss: 0.01670\n",
      "Training epoch: 1465, train loss: 0.01559, val loss: 0.01608\n",
      "Training epoch: 1466, train loss: 0.01595, val loss: 0.01645\n",
      "Training epoch: 1467, train loss: 0.01533, val loss: 0.01581\n",
      "Training epoch: 1468, train loss: 0.01530, val loss: 0.01577\n",
      "Training epoch: 1469, train loss: 0.01558, val loss: 0.01603\n",
      "Training epoch: 1470, train loss: 0.01541, val loss: 0.01588\n",
      "Training epoch: 1471, train loss: 0.01524, val loss: 0.01571\n",
      "Training epoch: 1472, train loss: 0.01544, val loss: 0.01593\n",
      "Training epoch: 1473, train loss: 0.01529, val loss: 0.01579\n",
      "Training epoch: 1474, train loss: 0.01603, val loss: 0.01652\n",
      "Training epoch: 1475, train loss: 0.01575, val loss: 0.01626\n",
      "Training epoch: 1476, train loss: 0.01539, val loss: 0.01585\n",
      "Training epoch: 1477, train loss: 0.01531, val loss: 0.01583\n",
      "Training epoch: 1478, train loss: 0.01532, val loss: 0.01576\n",
      "Training epoch: 1479, train loss: 0.01568, val loss: 0.01614\n",
      "Training epoch: 1480, train loss: 0.01545, val loss: 0.01591\n",
      "Training epoch: 1481, train loss: 0.01610, val loss: 0.01661\n",
      "Training epoch: 1482, train loss: 0.01535, val loss: 0.01582\n",
      "Training epoch: 1483, train loss: 0.01529, val loss: 0.01575\n",
      "Training epoch: 1484, train loss: 0.01565, val loss: 0.01613\n",
      "Training epoch: 1485, train loss: 0.01535, val loss: 0.01581\n",
      "Training epoch: 1486, train loss: 0.01586, val loss: 0.01639\n",
      "Training epoch: 1487, train loss: 0.01544, val loss: 0.01593\n",
      "Training epoch: 1488, train loss: 0.01555, val loss: 0.01604\n",
      "Training epoch: 1489, train loss: 0.01595, val loss: 0.01645\n",
      "Training epoch: 1490, train loss: 0.01559, val loss: 0.01603\n",
      "Training epoch: 1491, train loss: 0.01561, val loss: 0.01605\n",
      "Training epoch: 1492, train loss: 0.01524, val loss: 0.01573\n",
      "Training epoch: 1493, train loss: 0.01547, val loss: 0.01592\n",
      "Training epoch: 1494, train loss: 0.01533, val loss: 0.01580\n",
      "Training epoch: 1495, train loss: 0.01571, val loss: 0.01614\n",
      "Training epoch: 1496, train loss: 0.01541, val loss: 0.01591\n",
      "Training epoch: 1497, train loss: 0.01549, val loss: 0.01595\n",
      "Training epoch: 1498, train loss: 0.01592, val loss: 0.01642\n",
      "Training epoch: 1499, train loss: 0.01556, val loss: 0.01608\n",
      "Training epoch: 1500, train loss: 0.01564, val loss: 0.01612\n",
      "Training epoch: 1501, train loss: 0.01529, val loss: 0.01576\n",
      "Training epoch: 1502, train loss: 0.01552, val loss: 0.01599\n",
      "Training epoch: 1503, train loss: 0.01536, val loss: 0.01585\n",
      "Training epoch: 1504, train loss: 0.01547, val loss: 0.01597\n",
      "Training epoch: 1505, train loss: 0.01546, val loss: 0.01593\n",
      "Training epoch: 1506, train loss: 0.01537, val loss: 0.01584\n",
      "Training epoch: 1507, train loss: 0.01560, val loss: 0.01612\n",
      "Training epoch: 1508, train loss: 0.01541, val loss: 0.01592\n",
      "Training epoch: 1509, train loss: 0.01532, val loss: 0.01577\n",
      "Training epoch: 1510, train loss: 0.01531, val loss: 0.01576\n",
      "Training epoch: 1511, train loss: 0.01529, val loss: 0.01578\n",
      "Training epoch: 1512, train loss: 0.01579, val loss: 0.01629\n",
      "Training epoch: 1513, train loss: 0.01551, val loss: 0.01600\n",
      "Training epoch: 1514, train loss: 0.01538, val loss: 0.01583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1515, train loss: 0.01573, val loss: 0.01619\n",
      "Training epoch: 1516, train loss: 0.01534, val loss: 0.01583\n",
      "Training epoch: 1517, train loss: 0.01527, val loss: 0.01573\n",
      "Training epoch: 1518, train loss: 0.01552, val loss: 0.01602\n",
      "Training epoch: 1519, train loss: 0.01537, val loss: 0.01586\n",
      "Training epoch: 1520, train loss: 0.01564, val loss: 0.01611\n",
      "Training epoch: 1521, train loss: 0.01537, val loss: 0.01585\n",
      "Training epoch: 1522, train loss: 0.01523, val loss: 0.01571\n",
      "Training epoch: 1523, train loss: 0.01559, val loss: 0.01602\n",
      "Training epoch: 1524, train loss: 0.01528, val loss: 0.01574\n",
      "Training epoch: 1525, train loss: 0.01594, val loss: 0.01644\n",
      "Training epoch: 1526, train loss: 0.01524, val loss: 0.01570\n",
      "Training epoch: 1527, train loss: 0.01593, val loss: 0.01634\n",
      "Training epoch: 1528, train loss: 0.01543, val loss: 0.01592\n",
      "Training epoch: 1529, train loss: 0.01553, val loss: 0.01600\n",
      "Training epoch: 1530, train loss: 0.01527, val loss: 0.01574\n",
      "Training epoch: 1531, train loss: 0.01539, val loss: 0.01587\n",
      "Training epoch: 1532, train loss: 0.01577, val loss: 0.01631\n",
      "Training epoch: 1533, train loss: 0.01553, val loss: 0.01602\n",
      "Training epoch: 1534, train loss: 0.01578, val loss: 0.01627\n",
      "Training epoch: 1535, train loss: 0.01556, val loss: 0.01597\n",
      "Training epoch: 1536, train loss: 0.01540, val loss: 0.01586\n",
      "Training epoch: 1537, train loss: 0.01540, val loss: 0.01582\n",
      "Training epoch: 1538, train loss: 0.01550, val loss: 0.01597\n",
      "Training epoch: 1539, train loss: 0.01612, val loss: 0.01659\n",
      "Training epoch: 1540, train loss: 0.01535, val loss: 0.01582\n",
      "Training epoch: 1541, train loss: 0.01596, val loss: 0.01645\n",
      "Training epoch: 1542, train loss: 0.01526, val loss: 0.01571\n",
      "Training epoch: 1543, train loss: 0.01525, val loss: 0.01570\n",
      "Training epoch: 1544, train loss: 0.01561, val loss: 0.01609\n",
      "Training epoch: 1545, train loss: 0.01564, val loss: 0.01615\n",
      "Training epoch: 1546, train loss: 0.01722, val loss: 0.01771\n",
      "Training epoch: 1547, train loss: 0.01538, val loss: 0.01585\n",
      "Training epoch: 1548, train loss: 0.01535, val loss: 0.01579\n",
      "Training epoch: 1549, train loss: 0.01544, val loss: 0.01593\n",
      "Training epoch: 1550, train loss: 0.01544, val loss: 0.01592\n",
      "Training epoch: 1551, train loss: 0.01543, val loss: 0.01589\n",
      "Training epoch: 1552, train loss: 0.01561, val loss: 0.01606\n",
      "Training epoch: 1553, train loss: 0.01566, val loss: 0.01609\n",
      "Training epoch: 1554, train loss: 0.01536, val loss: 0.01585\n",
      "Training epoch: 1555, train loss: 0.01564, val loss: 0.01611\n",
      "Training epoch: 1556, train loss: 0.01561, val loss: 0.01608\n",
      "Training epoch: 1557, train loss: 0.01554, val loss: 0.01595\n",
      "Training epoch: 1558, train loss: 0.01577, val loss: 0.01618\n",
      "Training epoch: 1559, train loss: 0.01539, val loss: 0.01587\n",
      "Training epoch: 1560, train loss: 0.01535, val loss: 0.01583\n",
      "Training epoch: 1561, train loss: 0.01569, val loss: 0.01616\n",
      "Training epoch: 1562, train loss: 0.01554, val loss: 0.01604\n",
      "Training epoch: 1563, train loss: 0.01645, val loss: 0.01694\n",
      "Training epoch: 1564, train loss: 0.01560, val loss: 0.01606\n",
      "Training epoch: 1565, train loss: 0.01526, val loss: 0.01574\n",
      "Training epoch: 1566, train loss: 0.01532, val loss: 0.01579\n",
      "Training epoch: 1567, train loss: 0.01536, val loss: 0.01584\n",
      "Training epoch: 1568, train loss: 0.01573, val loss: 0.01620\n",
      "Training epoch: 1569, train loss: 0.01550, val loss: 0.01596\n",
      "Training epoch: 1570, train loss: 0.01553, val loss: 0.01600\n",
      "Training epoch: 1571, train loss: 0.01544, val loss: 0.01590\n",
      "Training epoch: 1572, train loss: 0.01548, val loss: 0.01595\n",
      "Training epoch: 1573, train loss: 0.01528, val loss: 0.01577\n",
      "Training epoch: 1574, train loss: 0.01521, val loss: 0.01568\n",
      "Training epoch: 1575, train loss: 0.01563, val loss: 0.01611\n",
      "Training epoch: 1576, train loss: 0.01611, val loss: 0.01660\n",
      "Training epoch: 1577, train loss: 0.01562, val loss: 0.01609\n",
      "Training epoch: 1578, train loss: 0.01523, val loss: 0.01571\n",
      "Training epoch: 1579, train loss: 0.01535, val loss: 0.01582\n",
      "Training epoch: 1580, train loss: 0.01555, val loss: 0.01607\n",
      "Training epoch: 1581, train loss: 0.01569, val loss: 0.01619\n",
      "Training epoch: 1582, train loss: 0.01559, val loss: 0.01607\n",
      "Training epoch: 1583, train loss: 0.01527, val loss: 0.01573\n",
      "Training epoch: 1584, train loss: 0.01599, val loss: 0.01655\n",
      "Training epoch: 1585, train loss: 0.01553, val loss: 0.01602\n",
      "Training epoch: 1586, train loss: 0.01554, val loss: 0.01602\n",
      "Training epoch: 1587, train loss: 0.01538, val loss: 0.01586\n",
      "Training epoch: 1588, train loss: 0.01540, val loss: 0.01590\n",
      "Training epoch: 1589, train loss: 0.01523, val loss: 0.01569\n",
      "Training epoch: 1590, train loss: 0.01531, val loss: 0.01577\n",
      "Training epoch: 1591, train loss: 0.01595, val loss: 0.01645\n",
      "Training epoch: 1592, train loss: 0.01533, val loss: 0.01580\n",
      "Training epoch: 1593, train loss: 0.01540, val loss: 0.01582\n",
      "Training epoch: 1594, train loss: 0.01610, val loss: 0.01655\n",
      "Training epoch: 1595, train loss: 0.01636, val loss: 0.01685\n",
      "Training epoch: 1596, train loss: 0.01619, val loss: 0.01668\n",
      "Training epoch: 1597, train loss: 0.01541, val loss: 0.01587\n",
      "Training epoch: 1598, train loss: 0.01538, val loss: 0.01587\n",
      "Training epoch: 1599, train loss: 0.01555, val loss: 0.01594\n",
      "Training epoch: 1600, train loss: 0.01572, val loss: 0.01620\n",
      "Training epoch: 1601, train loss: 0.01534, val loss: 0.01584\n",
      "Training epoch: 1602, train loss: 0.01554, val loss: 0.01606\n",
      "Training epoch: 1603, train loss: 0.01525, val loss: 0.01572\n",
      "Training epoch: 1604, train loss: 0.01586, val loss: 0.01632\n",
      "Training epoch: 1605, train loss: 0.01575, val loss: 0.01625\n",
      "Training epoch: 1606, train loss: 0.01538, val loss: 0.01584\n",
      "Training epoch: 1607, train loss: 0.01540, val loss: 0.01587\n",
      "Training epoch: 1608, train loss: 0.01572, val loss: 0.01621\n",
      "Training epoch: 1609, train loss: 0.01554, val loss: 0.01602\n",
      "Training epoch: 1610, train loss: 0.01551, val loss: 0.01596\n",
      "Training epoch: 1611, train loss: 0.01527, val loss: 0.01571\n",
      "Training epoch: 1612, train loss: 0.01534, val loss: 0.01578\n",
      "Training epoch: 1613, train loss: 0.01547, val loss: 0.01593\n",
      "Training epoch: 1614, train loss: 0.01525, val loss: 0.01571\n",
      "Training epoch: 1615, train loss: 0.01534, val loss: 0.01580\n",
      "Training epoch: 1616, train loss: 0.01588, val loss: 0.01635\n",
      "Training epoch: 1617, train loss: 0.01558, val loss: 0.01602\n",
      "Training epoch: 1618, train loss: 0.01560, val loss: 0.01604\n",
      "Training epoch: 1619, train loss: 0.01565, val loss: 0.01609\n",
      "Training epoch: 1620, train loss: 0.01548, val loss: 0.01594\n",
      "Training epoch: 1621, train loss: 0.01539, val loss: 0.01581\n",
      "Training epoch: 1622, train loss: 0.01574, val loss: 0.01621\n",
      "Training epoch: 1623, train loss: 0.01566, val loss: 0.01615\n",
      "Training epoch: 1624, train loss: 0.01524, val loss: 0.01570\n",
      "Training epoch: 1625, train loss: 0.01533, val loss: 0.01577\n",
      "Training epoch: 1626, train loss: 0.01552, val loss: 0.01595\n",
      "Training epoch: 1627, train loss: 0.01549, val loss: 0.01599\n",
      "Training epoch: 1628, train loss: 0.01527, val loss: 0.01574\n",
      "Training epoch: 1629, train loss: 0.01530, val loss: 0.01573\n",
      "Training epoch: 1630, train loss: 0.01567, val loss: 0.01614\n",
      "Training epoch: 1631, train loss: 0.01545, val loss: 0.01589\n",
      "Training epoch: 1632, train loss: 0.01573, val loss: 0.01621\n",
      "Training epoch: 1633, train loss: 0.01526, val loss: 0.01573\n",
      "Training epoch: 1634, train loss: 0.01594, val loss: 0.01642\n",
      "Training epoch: 1635, train loss: 0.01558, val loss: 0.01605\n",
      "Training epoch: 1636, train loss: 0.01543, val loss: 0.01590\n",
      "Training epoch: 1637, train loss: 0.01572, val loss: 0.01621\n",
      "Training epoch: 1638, train loss: 0.01547, val loss: 0.01592\n",
      "Training epoch: 1639, train loss: 0.01532, val loss: 0.01580\n",
      "Training epoch: 1640, train loss: 0.01529, val loss: 0.01576\n",
      "Training epoch: 1641, train loss: 0.01529, val loss: 0.01573\n",
      "Training epoch: 1642, train loss: 0.01560, val loss: 0.01601\n",
      "Training epoch: 1643, train loss: 0.01541, val loss: 0.01586\n",
      "Training epoch: 1644, train loss: 0.01550, val loss: 0.01597\n",
      "Training epoch: 1645, train loss: 0.01648, val loss: 0.01695\n",
      "Training epoch: 1646, train loss: 0.01579, val loss: 0.01624\n",
      "Training epoch: 1647, train loss: 0.01572, val loss: 0.01615\n",
      "Training epoch: 1648, train loss: 0.01547, val loss: 0.01591\n",
      "Training epoch: 1649, train loss: 0.01530, val loss: 0.01579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1650, train loss: 0.01604, val loss: 0.01654\n",
      "Training epoch: 1651, train loss: 0.01572, val loss: 0.01620\n",
      "Training epoch: 1652, train loss: 0.01549, val loss: 0.01597\n",
      "Training epoch: 1653, train loss: 0.01639, val loss: 0.01675\n",
      "Training epoch: 1654, train loss: 0.01564, val loss: 0.01609\n",
      "Training epoch: 1655, train loss: 0.01528, val loss: 0.01575\n",
      "Training epoch: 1656, train loss: 0.01553, val loss: 0.01599\n",
      "Training epoch: 1657, train loss: 0.01537, val loss: 0.01588\n",
      "Training epoch: 1658, train loss: 0.01537, val loss: 0.01579\n",
      "Training epoch: 1659, train loss: 0.01538, val loss: 0.01583\n",
      "Training epoch: 1660, train loss: 0.01527, val loss: 0.01574\n",
      "Training epoch: 1661, train loss: 0.01533, val loss: 0.01579\n",
      "Training epoch: 1662, train loss: 0.01534, val loss: 0.01579\n",
      "Training epoch: 1663, train loss: 0.01541, val loss: 0.01587\n",
      "Training epoch: 1664, train loss: 0.01537, val loss: 0.01581\n",
      "Training epoch: 1665, train loss: 0.01548, val loss: 0.01595\n",
      "Training epoch: 1666, train loss: 0.01571, val loss: 0.01615\n",
      "Training epoch: 1667, train loss: 0.01530, val loss: 0.01574\n",
      "Training epoch: 1668, train loss: 0.01598, val loss: 0.01649\n",
      "Training epoch: 1669, train loss: 0.01555, val loss: 0.01604\n",
      "Training epoch: 1670, train loss: 0.01556, val loss: 0.01600\n",
      "Training epoch: 1671, train loss: 0.01553, val loss: 0.01600\n",
      "Training epoch: 1672, train loss: 0.01569, val loss: 0.01619\n",
      "Training epoch: 1673, train loss: 0.01541, val loss: 0.01587\n",
      "Training epoch: 1674, train loss: 0.01524, val loss: 0.01570\n",
      "Training epoch: 1675, train loss: 0.01617, val loss: 0.01663\n",
      "Training epoch: 1676, train loss: 0.01529, val loss: 0.01574\n",
      "Training epoch: 1677, train loss: 0.01538, val loss: 0.01588\n",
      "Training epoch: 1678, train loss: 0.01542, val loss: 0.01589\n",
      "Training epoch: 1679, train loss: 0.01546, val loss: 0.01594\n",
      "Training epoch: 1680, train loss: 0.01560, val loss: 0.01604\n",
      "Training epoch: 1681, train loss: 0.01643, val loss: 0.01689\n",
      "Training epoch: 1682, train loss: 0.01662, val loss: 0.01708\n",
      "Training epoch: 1683, train loss: 0.01568, val loss: 0.01619\n",
      "Training epoch: 1684, train loss: 0.01539, val loss: 0.01583\n",
      "Training epoch: 1685, train loss: 0.01529, val loss: 0.01574\n",
      "Training epoch: 1686, train loss: 0.01526, val loss: 0.01575\n",
      "Training epoch: 1687, train loss: 0.01556, val loss: 0.01603\n",
      "Training epoch: 1688, train loss: 0.01543, val loss: 0.01590\n",
      "Training epoch: 1689, train loss: 0.01525, val loss: 0.01569\n",
      "Training epoch: 1690, train loss: 0.01582, val loss: 0.01627\n",
      "Training epoch: 1691, train loss: 0.01546, val loss: 0.01589\n",
      "Training epoch: 1692, train loss: 0.01551, val loss: 0.01596\n",
      "Training epoch: 1693, train loss: 0.01545, val loss: 0.01591\n",
      "Training epoch: 1694, train loss: 0.01540, val loss: 0.01584\n",
      "Training epoch: 1695, train loss: 0.01542, val loss: 0.01589\n",
      "Training epoch: 1696, train loss: 0.01547, val loss: 0.01593\n",
      "Training epoch: 1697, train loss: 0.01536, val loss: 0.01583\n",
      "Training epoch: 1698, train loss: 0.01545, val loss: 0.01593\n",
      "Training epoch: 1699, train loss: 0.01576, val loss: 0.01624\n",
      "Training epoch: 1700, train loss: 0.01563, val loss: 0.01605\n",
      "Training epoch: 1701, train loss: 0.01573, val loss: 0.01615\n",
      "Training epoch: 1702, train loss: 0.01535, val loss: 0.01581\n",
      "Training epoch: 1703, train loss: 0.01558, val loss: 0.01603\n",
      "Training epoch: 1704, train loss: 0.01602, val loss: 0.01652\n",
      "Training epoch: 1705, train loss: 0.01635, val loss: 0.01683\n",
      "Training epoch: 1706, train loss: 0.01535, val loss: 0.01583\n",
      "Training epoch: 1707, train loss: 0.01594, val loss: 0.01645\n",
      "Training epoch: 1708, train loss: 0.01575, val loss: 0.01625\n",
      "Training epoch: 1709, train loss: 0.01545, val loss: 0.01592\n",
      "Training epoch: 1710, train loss: 0.01608, val loss: 0.01658\n",
      "Training epoch: 1711, train loss: 0.01604, val loss: 0.01654\n",
      "Training epoch: 1712, train loss: 0.01530, val loss: 0.01574\n",
      "Training epoch: 1713, train loss: 0.01530, val loss: 0.01574\n",
      "Training epoch: 1714, train loss: 0.01544, val loss: 0.01591\n",
      "Training epoch: 1715, train loss: 0.01565, val loss: 0.01607\n",
      "Training epoch: 1716, train loss: 0.01553, val loss: 0.01595\n",
      "Training epoch: 1717, train loss: 0.01534, val loss: 0.01579\n",
      "Training epoch: 1718, train loss: 0.01525, val loss: 0.01569\n",
      "Training epoch: 1719, train loss: 0.01557, val loss: 0.01604\n",
      "Training epoch: 1720, train loss: 0.01528, val loss: 0.01571\n",
      "Training epoch: 1721, train loss: 0.01537, val loss: 0.01583\n",
      "Training epoch: 1722, train loss: 0.01545, val loss: 0.01595\n",
      "Training epoch: 1723, train loss: 0.01536, val loss: 0.01582\n",
      "Training epoch: 1724, train loss: 0.01550, val loss: 0.01600\n",
      "Training epoch: 1725, train loss: 0.01545, val loss: 0.01594\n",
      "Training epoch: 1726, train loss: 0.01522, val loss: 0.01569\n",
      "Training epoch: 1727, train loss: 0.01533, val loss: 0.01581\n",
      "Training epoch: 1728, train loss: 0.01533, val loss: 0.01579\n",
      "Training epoch: 1729, train loss: 0.01546, val loss: 0.01592\n",
      "Training epoch: 1730, train loss: 0.01542, val loss: 0.01586\n",
      "Training epoch: 1731, train loss: 0.01534, val loss: 0.01579\n",
      "Training epoch: 1732, train loss: 0.01529, val loss: 0.01573\n",
      "Training epoch: 1733, train loss: 0.01562, val loss: 0.01606\n",
      "Training epoch: 1734, train loss: 0.01526, val loss: 0.01571\n",
      "Training epoch: 1735, train loss: 0.01533, val loss: 0.01580\n",
      "Training epoch: 1736, train loss: 0.01622, val loss: 0.01680\n",
      "Training epoch: 1737, train loss: 0.01531, val loss: 0.01578\n",
      "Training epoch: 1738, train loss: 0.01568, val loss: 0.01613\n",
      "Training epoch: 1739, train loss: 0.01545, val loss: 0.01591\n",
      "Training epoch: 1740, train loss: 0.01539, val loss: 0.01584\n",
      "Training epoch: 1741, train loss: 0.01533, val loss: 0.01579\n",
      "Training epoch: 1742, train loss: 0.01541, val loss: 0.01588\n",
      "Training epoch: 1743, train loss: 0.01528, val loss: 0.01573\n",
      "Training epoch: 1744, train loss: 0.01533, val loss: 0.01579\n",
      "Training epoch: 1745, train loss: 0.01558, val loss: 0.01601\n",
      "Training epoch: 1746, train loss: 0.01527, val loss: 0.01572\n",
      "Training epoch: 1747, train loss: 0.01585, val loss: 0.01636\n",
      "Training epoch: 1748, train loss: 0.01525, val loss: 0.01570\n",
      "Training epoch: 1749, train loss: 0.01654, val loss: 0.01688\n",
      "Training epoch: 1750, train loss: 0.01550, val loss: 0.01594\n",
      "Training epoch: 1751, train loss: 0.01538, val loss: 0.01579\n",
      "Training epoch: 1752, train loss: 0.01588, val loss: 0.01635\n",
      "Training epoch: 1753, train loss: 0.01621, val loss: 0.01668\n",
      "Training epoch: 1754, train loss: 0.01551, val loss: 0.01596\n",
      "Training epoch: 1755, train loss: 0.01554, val loss: 0.01597\n",
      "Training epoch: 1756, train loss: 0.01562, val loss: 0.01611\n",
      "Training epoch: 1757, train loss: 0.01553, val loss: 0.01596\n",
      "Training epoch: 1758, train loss: 0.01526, val loss: 0.01569\n",
      "Training epoch: 1759, train loss: 0.01543, val loss: 0.01590\n",
      "Training epoch: 1760, train loss: 0.01540, val loss: 0.01585\n",
      "Training epoch: 1761, train loss: 0.01571, val loss: 0.01620\n",
      "Training epoch: 1762, train loss: 0.01590, val loss: 0.01642\n",
      "Training epoch: 1763, train loss: 0.01588, val loss: 0.01636\n",
      "Training epoch: 1764, train loss: 0.01551, val loss: 0.01596\n",
      "Training epoch: 1765, train loss: 0.01550, val loss: 0.01595\n",
      "Training epoch: 1766, train loss: 0.01537, val loss: 0.01582\n",
      "Training epoch: 1767, train loss: 0.01572, val loss: 0.01619\n",
      "Training epoch: 1768, train loss: 0.01532, val loss: 0.01576\n",
      "Training epoch: 1769, train loss: 0.01544, val loss: 0.01593\n",
      "Training epoch: 1770, train loss: 0.01535, val loss: 0.01582\n",
      "Training epoch: 1771, train loss: 0.01521, val loss: 0.01564\n",
      "Training epoch: 1772, train loss: 0.01565, val loss: 0.01609\n",
      "Training epoch: 1773, train loss: 0.01538, val loss: 0.01584\n",
      "Training epoch: 1774, train loss: 0.01547, val loss: 0.01599\n",
      "Training epoch: 1775, train loss: 0.01545, val loss: 0.01594\n",
      "Training epoch: 1776, train loss: 0.01523, val loss: 0.01567\n",
      "Training epoch: 1777, train loss: 0.01533, val loss: 0.01577\n",
      "Training epoch: 1778, train loss: 0.01544, val loss: 0.01591\n",
      "Training epoch: 1779, train loss: 0.01531, val loss: 0.01580\n",
      "Training epoch: 1780, train loss: 0.01523, val loss: 0.01565\n",
      "Training epoch: 1781, train loss: 0.01579, val loss: 0.01625\n",
      "Training epoch: 1782, train loss: 0.01530, val loss: 0.01578\n",
      "Training epoch: 1783, train loss: 0.01563, val loss: 0.01608\n",
      "Training epoch: 1784, train loss: 0.01524, val loss: 0.01569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1785, train loss: 0.01609, val loss: 0.01651\n",
      "Training epoch: 1786, train loss: 0.01549, val loss: 0.01597\n",
      "Training epoch: 1787, train loss: 0.01572, val loss: 0.01620\n",
      "Training epoch: 1788, train loss: 0.01549, val loss: 0.01596\n",
      "Training epoch: 1789, train loss: 0.01535, val loss: 0.01582\n",
      "Training epoch: 1790, train loss: 0.01575, val loss: 0.01626\n",
      "Training epoch: 1791, train loss: 0.01557, val loss: 0.01605\n",
      "Training epoch: 1792, train loss: 0.01530, val loss: 0.01575\n",
      "Training epoch: 1793, train loss: 0.01536, val loss: 0.01581\n",
      "Training epoch: 1794, train loss: 0.01553, val loss: 0.01596\n",
      "Training epoch: 1795, train loss: 0.01537, val loss: 0.01582\n",
      "Training epoch: 1796, train loss: 0.01557, val loss: 0.01603\n",
      "Training epoch: 1797, train loss: 0.01535, val loss: 0.01578\n",
      "Training epoch: 1798, train loss: 0.01553, val loss: 0.01599\n",
      "Training epoch: 1799, train loss: 0.01635, val loss: 0.01690\n",
      "Training epoch: 1800, train loss: 0.01582, val loss: 0.01630\n",
      "Training epoch: 1801, train loss: 0.01532, val loss: 0.01579\n",
      "Training epoch: 1802, train loss: 0.01584, val loss: 0.01631\n",
      "Training epoch: 1803, train loss: 0.01533, val loss: 0.01576\n",
      "Training epoch: 1804, train loss: 0.01533, val loss: 0.01577\n",
      "Training epoch: 1805, train loss: 0.01533, val loss: 0.01580\n",
      "Training epoch: 1806, train loss: 0.01548, val loss: 0.01598\n",
      "Training epoch: 1807, train loss: 0.01529, val loss: 0.01574\n",
      "Training epoch: 1808, train loss: 0.01564, val loss: 0.01607\n",
      "Training epoch: 1809, train loss: 0.01536, val loss: 0.01582\n",
      "Training epoch: 1810, train loss: 0.01536, val loss: 0.01583\n",
      "Training epoch: 1811, train loss: 0.01544, val loss: 0.01592\n",
      "Training epoch: 1812, train loss: 0.01546, val loss: 0.01595\n",
      "Training epoch: 1813, train loss: 0.01553, val loss: 0.01596\n",
      "Training epoch: 1814, train loss: 0.01550, val loss: 0.01598\n",
      "Training epoch: 1815, train loss: 0.01522, val loss: 0.01568\n",
      "Training epoch: 1816, train loss: 0.01525, val loss: 0.01568\n",
      "Training epoch: 1817, train loss: 0.01551, val loss: 0.01595\n",
      "Training epoch: 1818, train loss: 0.01568, val loss: 0.01615\n",
      "Training epoch: 1819, train loss: 0.01531, val loss: 0.01576\n",
      "Training epoch: 1820, train loss: 0.01548, val loss: 0.01599\n",
      "Training epoch: 1821, train loss: 0.01534, val loss: 0.01578\n",
      "Training epoch: 1822, train loss: 0.01638, val loss: 0.01683\n",
      "Training epoch: 1823, train loss: 0.01576, val loss: 0.01622\n",
      "Training epoch: 1824, train loss: 0.01544, val loss: 0.01588\n",
      "Training epoch: 1825, train loss: 0.01592, val loss: 0.01637\n",
      "Training epoch: 1826, train loss: 0.01600, val loss: 0.01652\n",
      "Training epoch: 1827, train loss: 0.01554, val loss: 0.01602\n",
      "Training epoch: 1828, train loss: 0.01533, val loss: 0.01577\n",
      "Training epoch: 1829, train loss: 0.01539, val loss: 0.01585\n",
      "Training epoch: 1830, train loss: 0.01543, val loss: 0.01590\n",
      "Training epoch: 1831, train loss: 0.01581, val loss: 0.01631\n",
      "Training epoch: 1832, train loss: 0.01536, val loss: 0.01582\n",
      "Training epoch: 1833, train loss: 0.01532, val loss: 0.01572\n",
      "Training epoch: 1834, train loss: 0.01538, val loss: 0.01586\n",
      "Training epoch: 1835, train loss: 0.01590, val loss: 0.01637\n",
      "Training epoch: 1836, train loss: 0.01541, val loss: 0.01591\n",
      "Training epoch: 1837, train loss: 0.01525, val loss: 0.01570\n",
      "Training epoch: 1838, train loss: 0.01550, val loss: 0.01595\n",
      "Training epoch: 1839, train loss: 0.01571, val loss: 0.01617\n",
      "Training epoch: 1840, train loss: 0.01550, val loss: 0.01596\n",
      "Training epoch: 1841, train loss: 0.01569, val loss: 0.01617\n",
      "Training epoch: 1842, train loss: 0.01543, val loss: 0.01585\n",
      "Training epoch: 1843, train loss: 0.01533, val loss: 0.01576\n",
      "Training epoch: 1844, train loss: 0.01585, val loss: 0.01630\n",
      "Training epoch: 1845, train loss: 0.01531, val loss: 0.01574\n",
      "Training epoch: 1846, train loss: 0.01633, val loss: 0.01685\n",
      "Training epoch: 1847, train loss: 0.01543, val loss: 0.01584\n",
      "Training epoch: 1848, train loss: 0.01533, val loss: 0.01578\n",
      "Training epoch: 1849, train loss: 0.01544, val loss: 0.01586\n",
      "Training epoch: 1850, train loss: 0.01540, val loss: 0.01582\n",
      "Training epoch: 1851, train loss: 0.01573, val loss: 0.01621\n",
      "Training epoch: 1852, train loss: 0.01568, val loss: 0.01614\n",
      "Training epoch: 1853, train loss: 0.01552, val loss: 0.01598\n",
      "Training epoch: 1854, train loss: 0.01546, val loss: 0.01592\n",
      "Training epoch: 1855, train loss: 0.01566, val loss: 0.01611\n",
      "Training epoch: 1856, train loss: 0.01572, val loss: 0.01621\n",
      "Training epoch: 1857, train loss: 0.01528, val loss: 0.01573\n",
      "Training epoch: 1858, train loss: 0.01577, val loss: 0.01623\n",
      "Training epoch: 1859, train loss: 0.01544, val loss: 0.01590\n",
      "Training epoch: 1860, train loss: 0.01531, val loss: 0.01574\n",
      "Training epoch: 1861, train loss: 0.01527, val loss: 0.01570\n",
      "Training epoch: 1862, train loss: 0.01551, val loss: 0.01598\n",
      "Training epoch: 1863, train loss: 0.01573, val loss: 0.01620\n",
      "Training epoch: 1864, train loss: 0.01528, val loss: 0.01573\n",
      "Training epoch: 1865, train loss: 0.01549, val loss: 0.01594\n",
      "Training epoch: 1866, train loss: 0.01528, val loss: 0.01573\n",
      "Training epoch: 1867, train loss: 0.01548, val loss: 0.01598\n",
      "Training epoch: 1868, train loss: 0.01554, val loss: 0.01598\n",
      "Training epoch: 1869, train loss: 0.01528, val loss: 0.01570\n",
      "Training epoch: 1870, train loss: 0.01577, val loss: 0.01622\n",
      "Training epoch: 1871, train loss: 0.01528, val loss: 0.01573\n",
      "Training epoch: 1872, train loss: 0.01630, val loss: 0.01676\n",
      "Training epoch: 1873, train loss: 0.01543, val loss: 0.01585\n",
      "Training epoch: 1874, train loss: 0.01526, val loss: 0.01570\n",
      "Training epoch: 1875, train loss: 0.01534, val loss: 0.01579\n",
      "Training epoch: 1876, train loss: 0.01527, val loss: 0.01575\n",
      "Training epoch: 1877, train loss: 0.01559, val loss: 0.01604\n",
      "Training epoch: 1878, train loss: 0.01547, val loss: 0.01592\n",
      "Training epoch: 1879, train loss: 0.01532, val loss: 0.01578\n",
      "Training epoch: 1880, train loss: 0.01539, val loss: 0.01586\n",
      "Training epoch: 1881, train loss: 0.01542, val loss: 0.01586\n",
      "Training epoch: 1882, train loss: 0.01528, val loss: 0.01569\n",
      "Training epoch: 1883, train loss: 0.01547, val loss: 0.01588\n",
      "Training epoch: 1884, train loss: 0.01602, val loss: 0.01650\n",
      "Training epoch: 1885, train loss: 0.01542, val loss: 0.01588\n",
      "Training epoch: 1886, train loss: 0.01534, val loss: 0.01579\n",
      "Training epoch: 1887, train loss: 0.01526, val loss: 0.01572\n",
      "Training epoch: 1888, train loss: 0.01556, val loss: 0.01601\n",
      "Training epoch: 1889, train loss: 0.01533, val loss: 0.01578\n",
      "Training epoch: 1890, train loss: 0.01528, val loss: 0.01573\n",
      "Training epoch: 1891, train loss: 0.01533, val loss: 0.01579\n",
      "Training epoch: 1892, train loss: 0.01542, val loss: 0.01586\n",
      "Training epoch: 1893, train loss: 0.01531, val loss: 0.01570\n",
      "Training epoch: 1894, train loss: 0.01533, val loss: 0.01579\n",
      "Training epoch: 1895, train loss: 0.01556, val loss: 0.01602\n",
      "Training epoch: 1896, train loss: 0.01531, val loss: 0.01570\n",
      "Training epoch: 1897, train loss: 0.01536, val loss: 0.01577\n",
      "Training epoch: 1898, train loss: 0.01564, val loss: 0.01608\n",
      "Training epoch: 1899, train loss: 0.01553, val loss: 0.01596\n",
      "Training epoch: 1900, train loss: 0.01582, val loss: 0.01618\n",
      "Training epoch: 1901, train loss: 0.01540, val loss: 0.01583\n",
      "Training epoch: 1902, train loss: 0.01537, val loss: 0.01580\n",
      "Training epoch: 1903, train loss: 0.01549, val loss: 0.01594\n",
      "Training epoch: 1904, train loss: 0.01564, val loss: 0.01607\n",
      "Training epoch: 1905, train loss: 0.01541, val loss: 0.01585\n",
      "Training epoch: 1906, train loss: 0.01556, val loss: 0.01597\n",
      "Training epoch: 1907, train loss: 0.01521, val loss: 0.01562\n",
      "Training epoch: 1908, train loss: 0.01526, val loss: 0.01570\n",
      "Training epoch: 1909, train loss: 0.01557, val loss: 0.01597\n",
      "Training epoch: 1910, train loss: 0.01561, val loss: 0.01604\n",
      "Training epoch: 1911, train loss: 0.01557, val loss: 0.01602\n",
      "Training epoch: 1912, train loss: 0.01529, val loss: 0.01573\n",
      "Training epoch: 1913, train loss: 0.01547, val loss: 0.01593\n",
      "Training epoch: 1914, train loss: 0.01560, val loss: 0.01607\n",
      "Training epoch: 1915, train loss: 0.01532, val loss: 0.01577\n",
      "Training epoch: 1916, train loss: 0.01530, val loss: 0.01576\n",
      "Training epoch: 1917, train loss: 0.01528, val loss: 0.01570\n",
      "Training epoch: 1918, train loss: 0.01531, val loss: 0.01572\n",
      "Training epoch: 1919, train loss: 0.01546, val loss: 0.01592\n",
      "Training epoch: 1920, train loss: 0.01546, val loss: 0.01591\n",
      "Training epoch: 1921, train loss: 0.01526, val loss: 0.01570\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1922, train loss: 0.01554, val loss: 0.01595\n",
      "Training epoch: 1923, train loss: 0.01522, val loss: 0.01564\n",
      "Training epoch: 1924, train loss: 0.01555, val loss: 0.01598\n",
      "Training epoch: 1925, train loss: 0.01550, val loss: 0.01598\n",
      "Training epoch: 1926, train loss: 0.01541, val loss: 0.01586\n",
      "Training epoch: 1927, train loss: 0.01561, val loss: 0.01606\n",
      "Training epoch: 1928, train loss: 0.01570, val loss: 0.01616\n",
      "Training epoch: 1929, train loss: 0.01624, val loss: 0.01670\n",
      "Training epoch: 1930, train loss: 0.01553, val loss: 0.01599\n",
      "Training epoch: 1931, train loss: 0.01574, val loss: 0.01618\n",
      "Training epoch: 1932, train loss: 0.01619, val loss: 0.01669\n",
      "Training epoch: 1933, train loss: 0.01562, val loss: 0.01609\n",
      "Training epoch: 1934, train loss: 0.01583, val loss: 0.01628\n",
      "Training epoch: 1935, train loss: 0.01598, val loss: 0.01640\n",
      "Training epoch: 1936, train loss: 0.01530, val loss: 0.01574\n",
      "Training epoch: 1937, train loss: 0.01538, val loss: 0.01580\n",
      "Training epoch: 1938, train loss: 0.01544, val loss: 0.01588\n",
      "Training epoch: 1939, train loss: 0.01562, val loss: 0.01599\n",
      "Training epoch: 1940, train loss: 0.01599, val loss: 0.01639\n",
      "Training epoch: 1941, train loss: 0.01527, val loss: 0.01571\n",
      "Training epoch: 1942, train loss: 0.01566, val loss: 0.01611\n",
      "Training epoch: 1943, train loss: 0.01530, val loss: 0.01571\n",
      "Training epoch: 1944, train loss: 0.01544, val loss: 0.01586\n",
      "Training epoch: 1945, train loss: 0.01569, val loss: 0.01613\n",
      "Training epoch: 1946, train loss: 0.01525, val loss: 0.01568\n",
      "Training epoch: 1947, train loss: 0.01539, val loss: 0.01580\n",
      "Training epoch: 1948, train loss: 0.01568, val loss: 0.01608\n",
      "Training epoch: 1949, train loss: 0.01557, val loss: 0.01602\n",
      "Training epoch: 1950, train loss: 0.01602, val loss: 0.01644\n",
      "Training epoch: 1951, train loss: 0.01564, val loss: 0.01605\n",
      "Training epoch: 1952, train loss: 0.01543, val loss: 0.01586\n",
      "Training epoch: 1953, train loss: 0.01583, val loss: 0.01625\n",
      "Training epoch: 1954, train loss: 0.01528, val loss: 0.01570\n",
      "Training epoch: 1955, train loss: 0.01535, val loss: 0.01579\n",
      "Training epoch: 1956, train loss: 0.01544, val loss: 0.01587\n",
      "Training epoch: 1957, train loss: 0.01523, val loss: 0.01563\n",
      "Training epoch: 1958, train loss: 0.01579, val loss: 0.01623\n",
      "Training epoch: 1959, train loss: 0.01662, val loss: 0.01712\n",
      "Training epoch: 1960, train loss: 0.01552, val loss: 0.01594\n",
      "Training epoch: 1961, train loss: 0.01541, val loss: 0.01585\n",
      "Training epoch: 1962, train loss: 0.01539, val loss: 0.01580\n",
      "Training epoch: 1963, train loss: 0.01547, val loss: 0.01591\n",
      "Training epoch: 1964, train loss: 0.01545, val loss: 0.01593\n",
      "Training epoch: 1965, train loss: 0.01560, val loss: 0.01605\n",
      "Training epoch: 1966, train loss: 0.01544, val loss: 0.01586\n",
      "Training epoch: 1967, train loss: 0.01538, val loss: 0.01581\n",
      "Training epoch: 1968, train loss: 0.01558, val loss: 0.01603\n",
      "Training epoch: 1969, train loss: 0.01578, val loss: 0.01623\n",
      "Training epoch: 1970, train loss: 0.01554, val loss: 0.01598\n",
      "Training epoch: 1971, train loss: 0.01554, val loss: 0.01594\n",
      "Training epoch: 1972, train loss: 0.01555, val loss: 0.01603\n",
      "Training epoch: 1973, train loss: 0.01548, val loss: 0.01589\n",
      "Training epoch: 1974, train loss: 0.01534, val loss: 0.01577\n",
      "Training epoch: 1975, train loss: 0.01531, val loss: 0.01575\n",
      "Training epoch: 1976, train loss: 0.01544, val loss: 0.01584\n",
      "Training epoch: 1977, train loss: 0.01528, val loss: 0.01571\n",
      "Training epoch: 1978, train loss: 0.01526, val loss: 0.01569\n",
      "Training epoch: 1979, train loss: 0.01547, val loss: 0.01587\n",
      "Training epoch: 1980, train loss: 0.01531, val loss: 0.01574\n",
      "Training epoch: 1981, train loss: 0.01544, val loss: 0.01588\n",
      "Training epoch: 1982, train loss: 0.01530, val loss: 0.01572\n",
      "Training epoch: 1983, train loss: 0.01535, val loss: 0.01574\n",
      "Training epoch: 1984, train loss: 0.01558, val loss: 0.01594\n",
      "Training epoch: 1985, train loss: 0.01524, val loss: 0.01567\n",
      "Training epoch: 1986, train loss: 0.01535, val loss: 0.01576\n",
      "Training epoch: 1987, train loss: 0.01533, val loss: 0.01574\n",
      "Training epoch: 1988, train loss: 0.01527, val loss: 0.01570\n",
      "Training epoch: 1989, train loss: 0.01549, val loss: 0.01592\n",
      "Training epoch: 1990, train loss: 0.01548, val loss: 0.01591\n",
      "Training epoch: 1991, train loss: 0.01547, val loss: 0.01591\n",
      "Training epoch: 1992, train loss: 0.01532, val loss: 0.01573\n",
      "Training epoch: 1993, train loss: 0.01536, val loss: 0.01580\n",
      "Training epoch: 1994, train loss: 0.01527, val loss: 0.01572\n",
      "Training epoch: 1995, train loss: 0.01557, val loss: 0.01604\n",
      "Training epoch: 1996, train loss: 0.01546, val loss: 0.01585\n",
      "Training epoch: 1997, train loss: 0.01625, val loss: 0.01667\n",
      "Training epoch: 1998, train loss: 0.01524, val loss: 0.01569\n",
      "Training epoch: 1999, train loss: 0.01576, val loss: 0.01620\n",
      "Training epoch: 2000, train loss: 0.01535, val loss: 0.01576\n",
      "Training epoch: 2001, train loss: 0.01582, val loss: 0.01626\n",
      "Training epoch: 2002, train loss: 0.01565, val loss: 0.01606\n",
      "Training epoch: 2003, train loss: 0.01544, val loss: 0.01591\n",
      "Training epoch: 2004, train loss: 0.01543, val loss: 0.01587\n",
      "Training epoch: 2005, train loss: 0.01599, val loss: 0.01642\n",
      "Training epoch: 2006, train loss: 0.01587, val loss: 0.01633\n",
      "Training epoch: 2007, train loss: 0.01546, val loss: 0.01589\n",
      "Training epoch: 2008, train loss: 0.01530, val loss: 0.01573\n",
      "Training epoch: 2009, train loss: 0.01541, val loss: 0.01584\n",
      "Training epoch: 2010, train loss: 0.01545, val loss: 0.01584\n",
      "Training epoch: 2011, train loss: 0.01526, val loss: 0.01568\n",
      "Training epoch: 2012, train loss: 0.01560, val loss: 0.01605\n",
      "Training epoch: 2013, train loss: 0.01534, val loss: 0.01576\n",
      "Training epoch: 2014, train loss: 0.01526, val loss: 0.01570\n",
      "Training epoch: 2015, train loss: 0.01531, val loss: 0.01571\n",
      "Training epoch: 2016, train loss: 0.01529, val loss: 0.01572\n",
      "Training epoch: 2017, train loss: 0.01546, val loss: 0.01592\n",
      "Training epoch: 2018, train loss: 0.01528, val loss: 0.01571\n",
      "Training epoch: 2019, train loss: 0.01540, val loss: 0.01582\n",
      "Training epoch: 2020, train loss: 0.01607, val loss: 0.01651\n",
      "Training epoch: 2021, train loss: 0.01548, val loss: 0.01588\n",
      "Training epoch: 2022, train loss: 0.01531, val loss: 0.01572\n",
      "Training epoch: 2023, train loss: 0.01557, val loss: 0.01602\n",
      "Training epoch: 2024, train loss: 0.01577, val loss: 0.01623\n",
      "Training epoch: 2025, train loss: 0.01533, val loss: 0.01577\n",
      "Training epoch: 2026, train loss: 0.01538, val loss: 0.01582\n",
      "Training epoch: 2027, train loss: 0.01567, val loss: 0.01604\n",
      "Training epoch: 2028, train loss: 0.01552, val loss: 0.01591\n",
      "Training epoch: 2029, train loss: 0.01531, val loss: 0.01574\n",
      "Training epoch: 2030, train loss: 0.01620, val loss: 0.01661\n",
      "Training epoch: 2031, train loss: 0.01557, val loss: 0.01600\n",
      "Training epoch: 2032, train loss: 0.01587, val loss: 0.01629\n",
      "Training epoch: 2033, train loss: 0.01551, val loss: 0.01593\n",
      "Training epoch: 2034, train loss: 0.01537, val loss: 0.01578\n",
      "Training epoch: 2035, train loss: 0.01567, val loss: 0.01610\n",
      "Training epoch: 2036, train loss: 0.01582, val loss: 0.01625\n",
      "Training epoch: 2037, train loss: 0.01532, val loss: 0.01577\n",
      "Training epoch: 2038, train loss: 0.01560, val loss: 0.01604\n",
      "Training epoch: 2039, train loss: 0.01561, val loss: 0.01602\n",
      "Training epoch: 2040, train loss: 0.01571, val loss: 0.01617\n",
      "Training epoch: 2041, train loss: 0.01563, val loss: 0.01608\n",
      "Training epoch: 2042, train loss: 0.01591, val loss: 0.01635\n",
      "Training epoch: 2043, train loss: 0.01562, val loss: 0.01604\n",
      "Training epoch: 2044, train loss: 0.01549, val loss: 0.01594\n",
      "Training epoch: 2045, train loss: 0.01546, val loss: 0.01588\n",
      "Training epoch: 2046, train loss: 0.01555, val loss: 0.01601\n",
      "Training epoch: 2047, train loss: 0.01530, val loss: 0.01572\n",
      "Training epoch: 2048, train loss: 0.01542, val loss: 0.01582\n",
      "Training epoch: 2049, train loss: 0.01529, val loss: 0.01573\n",
      "Training epoch: 2050, train loss: 0.01632, val loss: 0.01677\n",
      "Training epoch: 2051, train loss: 0.01544, val loss: 0.01585\n",
      "Training epoch: 2052, train loss: 0.01531, val loss: 0.01571\n",
      "Training epoch: 2053, train loss: 0.01533, val loss: 0.01575\n",
      "Training epoch: 2054, train loss: 0.01532, val loss: 0.01574\n",
      "Training epoch: 2055, train loss: 0.01541, val loss: 0.01584\n",
      "Training epoch: 2056, train loss: 0.01522, val loss: 0.01562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 2057, train loss: 0.01526, val loss: 0.01567\n",
      "Training epoch: 2058, train loss: 0.01541, val loss: 0.01583\n",
      "Training epoch: 2059, train loss: 0.01591, val loss: 0.01636\n",
      "Training epoch: 2060, train loss: 0.01597, val loss: 0.01634\n",
      "Training epoch: 2061, train loss: 0.01549, val loss: 0.01590\n",
      "Training epoch: 2062, train loss: 0.01529, val loss: 0.01570\n",
      "Training epoch: 2063, train loss: 0.01541, val loss: 0.01584\n",
      "Training epoch: 2064, train loss: 0.01557, val loss: 0.01598\n",
      "Training epoch: 2065, train loss: 0.01554, val loss: 0.01593\n",
      "Training epoch: 2066, train loss: 0.01525, val loss: 0.01565\n",
      "Training epoch: 2067, train loss: 0.01534, val loss: 0.01572\n",
      "Training epoch: 2068, train loss: 0.01555, val loss: 0.01594\n",
      "Training epoch: 2069, train loss: 0.01532, val loss: 0.01575\n",
      "Training epoch: 2070, train loss: 0.01547, val loss: 0.01588\n",
      "Training epoch: 2071, train loss: 0.01549, val loss: 0.01589\n",
      "Training epoch: 2072, train loss: 0.01525, val loss: 0.01567\n",
      "Training epoch: 2073, train loss: 0.01545, val loss: 0.01589\n",
      "Training epoch: 2074, train loss: 0.01553, val loss: 0.01598\n",
      "Training epoch: 2075, train loss: 0.01536, val loss: 0.01580\n",
      "Training epoch: 2076, train loss: 0.01559, val loss: 0.01596\n",
      "Training epoch: 2077, train loss: 0.01540, val loss: 0.01584\n",
      "Training epoch: 2078, train loss: 0.01567, val loss: 0.01608\n",
      "Training epoch: 2079, train loss: 0.01566, val loss: 0.01612\n",
      "Training epoch: 2080, train loss: 0.01561, val loss: 0.01608\n",
      "Training epoch: 2081, train loss: 0.01604, val loss: 0.01651\n",
      "Training epoch: 2082, train loss: 0.01529, val loss: 0.01571\n",
      "Training epoch: 2083, train loss: 0.01549, val loss: 0.01592\n",
      "Training epoch: 2084, train loss: 0.01531, val loss: 0.01573\n",
      "Training epoch: 2085, train loss: 0.01539, val loss: 0.01576\n",
      "Training epoch: 2086, train loss: 0.01570, val loss: 0.01617\n",
      "Training epoch: 2087, train loss: 0.01549, val loss: 0.01592\n",
      "Training epoch: 2088, train loss: 0.01577, val loss: 0.01616\n",
      "Training epoch: 2089, train loss: 0.01554, val loss: 0.01595\n",
      "Training epoch: 2090, train loss: 0.01541, val loss: 0.01586\n",
      "Training epoch: 2091, train loss: 0.01539, val loss: 0.01581\n",
      "Training epoch: 2092, train loss: 0.01558, val loss: 0.01601\n",
      "Training epoch: 2093, train loss: 0.01568, val loss: 0.01614\n",
      "Training epoch: 2094, train loss: 0.01525, val loss: 0.01567\n",
      "Training epoch: 2095, train loss: 0.01529, val loss: 0.01566\n",
      "Training epoch: 2096, train loss: 0.01534, val loss: 0.01573\n",
      "Training epoch: 2097, train loss: 0.01536, val loss: 0.01578\n",
      "Training epoch: 2098, train loss: 0.01581, val loss: 0.01620\n",
      "Training epoch: 2099, train loss: 0.01543, val loss: 0.01583\n",
      "Training epoch: 2100, train loss: 0.01531, val loss: 0.01570\n",
      "Training epoch: 2101, train loss: 0.01555, val loss: 0.01591\n",
      "Training epoch: 2102, train loss: 0.01586, val loss: 0.01630\n",
      "Training epoch: 2103, train loss: 0.01528, val loss: 0.01568\n",
      "Training epoch: 2104, train loss: 0.01538, val loss: 0.01577\n",
      "Training epoch: 2105, train loss: 0.01575, val loss: 0.01619\n",
      "Training epoch: 2106, train loss: 0.01539, val loss: 0.01580\n",
      "Training epoch: 2107, train loss: 0.01561, val loss: 0.01602\n",
      "Training epoch: 2108, train loss: 0.01559, val loss: 0.01601\n",
      "Training epoch: 2109, train loss: 0.01710, val loss: 0.01757\n",
      "Training epoch: 2110, train loss: 0.01567, val loss: 0.01609\n",
      "Training epoch: 2111, train loss: 0.01565, val loss: 0.01607\n",
      "Training epoch: 2112, train loss: 0.01563, val loss: 0.01602\n",
      "Training epoch: 2113, train loss: 0.01555, val loss: 0.01598\n",
      "Training epoch: 2114, train loss: 0.01538, val loss: 0.01577\n",
      "Training epoch: 2115, train loss: 0.01571, val loss: 0.01614\n",
      "Training epoch: 2116, train loss: 0.01582, val loss: 0.01618\n",
      "Training epoch: 2117, train loss: 0.01562, val loss: 0.01604\n",
      "Training epoch: 2118, train loss: 0.01558, val loss: 0.01594\n",
      "Training epoch: 2119, train loss: 0.01547, val loss: 0.01585\n",
      "Training epoch: 2120, train loss: 0.01606, val loss: 0.01648\n",
      "Training epoch: 2121, train loss: 0.01543, val loss: 0.01583\n",
      "Training epoch: 2122, train loss: 0.01522, val loss: 0.01563\n",
      "Training epoch: 2123, train loss: 0.01537, val loss: 0.01578\n",
      "Training epoch: 2124, train loss: 0.01550, val loss: 0.01591\n",
      "Training epoch: 2125, train loss: 0.01553, val loss: 0.01593\n",
      "Training epoch: 2126, train loss: 0.01574, val loss: 0.01616\n",
      "Training epoch: 2127, train loss: 0.01541, val loss: 0.01583\n",
      "Training epoch: 2128, train loss: 0.01533, val loss: 0.01571\n",
      "Training epoch: 2129, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 2130, train loss: 0.01571, val loss: 0.01615\n",
      "Training epoch: 2131, train loss: 0.01543, val loss: 0.01586\n",
      "Training epoch: 2132, train loss: 0.01529, val loss: 0.01566\n",
      "Training epoch: 2133, train loss: 0.01546, val loss: 0.01585\n",
      "Training epoch: 2134, train loss: 0.01549, val loss: 0.01590\n",
      "Training epoch: 2135, train loss: 0.01527, val loss: 0.01567\n",
      "Training epoch: 2136, train loss: 0.01568, val loss: 0.01611\n",
      "Training epoch: 2137, train loss: 0.01535, val loss: 0.01576\n",
      "Training epoch: 2138, train loss: 0.01525, val loss: 0.01566\n",
      "Training epoch: 2139, train loss: 0.01565, val loss: 0.01604\n",
      "Training epoch: 2140, train loss: 0.01524, val loss: 0.01566\n",
      "Training epoch: 2141, train loss: 0.01566, val loss: 0.01604\n",
      "Training epoch: 2142, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 2143, train loss: 0.01554, val loss: 0.01592\n",
      "Training epoch: 2144, train loss: 0.01582, val loss: 0.01620\n",
      "Training epoch: 2145, train loss: 0.01536, val loss: 0.01577\n",
      "Training epoch: 2146, train loss: 0.01566, val loss: 0.01608\n",
      "Training epoch: 2147, train loss: 0.01558, val loss: 0.01593\n",
      "Training epoch: 2148, train loss: 0.01651, val loss: 0.01695\n",
      "Training epoch: 2149, train loss: 0.01536, val loss: 0.01572\n",
      "Training epoch: 2150, train loss: 0.01526, val loss: 0.01562\n",
      "Training epoch: 2151, train loss: 0.01536, val loss: 0.01582\n",
      "Training epoch: 2152, train loss: 0.01525, val loss: 0.01569\n",
      "Training epoch: 2153, train loss: 0.01577, val loss: 0.01617\n",
      "Training epoch: 2154, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 2155, train loss: 0.01536, val loss: 0.01575\n",
      "Training epoch: 2156, train loss: 0.01540, val loss: 0.01581\n",
      "Training epoch: 2157, train loss: 0.01551, val loss: 0.01594\n",
      "Training epoch: 2158, train loss: 0.01608, val loss: 0.01651\n",
      "Training epoch: 2159, train loss: 0.01549, val loss: 0.01589\n",
      "Training epoch: 2160, train loss: 0.01565, val loss: 0.01605\n",
      "Training epoch: 2161, train loss: 0.01525, val loss: 0.01565\n",
      "Training epoch: 2162, train loss: 0.01531, val loss: 0.01566\n",
      "Training epoch: 2163, train loss: 0.01541, val loss: 0.01579\n",
      "Training epoch: 2164, train loss: 0.01594, val loss: 0.01637\n",
      "Training epoch: 2165, train loss: 0.01576, val loss: 0.01620\n",
      "Training epoch: 2166, train loss: 0.01543, val loss: 0.01587\n",
      "Training epoch: 2167, train loss: 0.01582, val loss: 0.01626\n",
      "Training epoch: 2168, train loss: 0.01537, val loss: 0.01577\n",
      "Training epoch: 2169, train loss: 0.01565, val loss: 0.01608\n",
      "Training epoch: 2170, train loss: 0.01561, val loss: 0.01599\n",
      "Training epoch: 2171, train loss: 0.01542, val loss: 0.01585\n",
      "Training epoch: 2172, train loss: 0.01577, val loss: 0.01615\n",
      "Training epoch: 2173, train loss: 0.01541, val loss: 0.01578\n",
      "Training epoch: 2174, train loss: 0.01536, val loss: 0.01576\n",
      "Training epoch: 2175, train loss: 0.01538, val loss: 0.01577\n",
      "Training epoch: 2176, train loss: 0.01544, val loss: 0.01584\n",
      "Training epoch: 2177, train loss: 0.01592, val loss: 0.01635\n",
      "Training epoch: 2178, train loss: 0.01610, val loss: 0.01651\n",
      "Training epoch: 2179, train loss: 0.01583, val loss: 0.01626\n",
      "Training epoch: 2180, train loss: 0.01535, val loss: 0.01578\n",
      "Training epoch: 2181, train loss: 0.01541, val loss: 0.01583\n",
      "Training epoch: 2182, train loss: 0.01535, val loss: 0.01575\n",
      "Training epoch: 2183, train loss: 0.01548, val loss: 0.01590\n",
      "Training epoch: 2184, train loss: 0.01535, val loss: 0.01574\n",
      "Training epoch: 2185, train loss: 0.01536, val loss: 0.01574\n",
      "Training epoch: 2186, train loss: 0.01527, val loss: 0.01566\n",
      "Training epoch: 2187, train loss: 0.01524, val loss: 0.01563\n",
      "Training epoch: 2188, train loss: 0.01548, val loss: 0.01594\n",
      "Training epoch: 2189, train loss: 0.01536, val loss: 0.01575\n",
      "Training epoch: 2190, train loss: 0.01557, val loss: 0.01596\n",
      "Training epoch: 2191, train loss: 0.01538, val loss: 0.01578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 2192, train loss: 0.01543, val loss: 0.01577\n",
      "Training epoch: 2193, train loss: 0.01566, val loss: 0.01606\n",
      "Training epoch: 2194, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 2195, train loss: 0.01578, val loss: 0.01623\n",
      "Training epoch: 2196, train loss: 0.01537, val loss: 0.01573\n",
      "Training epoch: 2197, train loss: 0.01551, val loss: 0.01590\n",
      "Training epoch: 2198, train loss: 0.01528, val loss: 0.01569\n",
      "Training epoch: 2199, train loss: 0.01541, val loss: 0.01583\n",
      "Training epoch: 2200, train loss: 0.01549, val loss: 0.01586\n",
      "Training epoch: 2201, train loss: 0.01533, val loss: 0.01571\n",
      "Training epoch: 2202, train loss: 0.01535, val loss: 0.01572\n",
      "Training epoch: 2203, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 2204, train loss: 0.01545, val loss: 0.01581\n",
      "Training epoch: 2205, train loss: 0.01544, val loss: 0.01584\n",
      "Training epoch: 2206, train loss: 0.01581, val loss: 0.01625\n",
      "Training epoch: 2207, train loss: 0.01565, val loss: 0.01604\n",
      "Training epoch: 2208, train loss: 0.01538, val loss: 0.01577\n",
      "Training epoch: 2209, train loss: 0.01576, val loss: 0.01619\n",
      "Training epoch: 2210, train loss: 0.01531, val loss: 0.01572\n",
      "Training epoch: 2211, train loss: 0.01551, val loss: 0.01590\n",
      "Training epoch: 2212, train loss: 0.01529, val loss: 0.01568\n",
      "Training epoch: 2213, train loss: 0.01530, val loss: 0.01570\n",
      "Training epoch: 2214, train loss: 0.01542, val loss: 0.01580\n",
      "Training epoch: 2215, train loss: 0.01534, val loss: 0.01575\n",
      "Training epoch: 2216, train loss: 0.01525, val loss: 0.01567\n",
      "Training epoch: 2217, train loss: 0.01527, val loss: 0.01564\n",
      "Training epoch: 2218, train loss: 0.01557, val loss: 0.01601\n",
      "Training epoch: 2219, train loss: 0.01539, val loss: 0.01578\n",
      "Training epoch: 2220, train loss: 0.01595, val loss: 0.01639\n",
      "Training epoch: 2221, train loss: 0.01548, val loss: 0.01592\n",
      "Training epoch: 2222, train loss: 0.01550, val loss: 0.01589\n",
      "Training epoch: 2223, train loss: 0.01558, val loss: 0.01597\n",
      "Training epoch: 2224, train loss: 0.01528, val loss: 0.01565\n",
      "Training epoch: 2225, train loss: 0.01542, val loss: 0.01584\n",
      "Training epoch: 2226, train loss: 0.01552, val loss: 0.01590\n",
      "Training epoch: 2227, train loss: 0.01530, val loss: 0.01567\n",
      "Training epoch: 2228, train loss: 0.01560, val loss: 0.01602\n",
      "Training epoch: 2229, train loss: 0.01538, val loss: 0.01578\n",
      "Training epoch: 2230, train loss: 0.01530, val loss: 0.01570\n",
      "Training epoch: 2231, train loss: 0.01534, val loss: 0.01573\n",
      "Training epoch: 2232, train loss: 0.01530, val loss: 0.01568\n",
      "Training epoch: 2233, train loss: 0.01550, val loss: 0.01589\n",
      "Training epoch: 2234, train loss: 0.01530, val loss: 0.01571\n",
      "Training epoch: 2235, train loss: 0.01532, val loss: 0.01570\n",
      "Training epoch: 2236, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 2237, train loss: 0.01530, val loss: 0.01566\n",
      "Training epoch: 2238, train loss: 0.01536, val loss: 0.01573\n",
      "Training epoch: 2239, train loss: 0.01552, val loss: 0.01593\n",
      "Training epoch: 2240, train loss: 0.01561, val loss: 0.01596\n",
      "Training epoch: 2241, train loss: 0.01524, val loss: 0.01559\n",
      "Training epoch: 2242, train loss: 0.01525, val loss: 0.01566\n",
      "Training epoch: 2243, train loss: 0.01552, val loss: 0.01590\n",
      "Training epoch: 2244, train loss: 0.01623, val loss: 0.01665\n",
      "Training epoch: 2245, train loss: 0.01542, val loss: 0.01580\n",
      "Training epoch: 2246, train loss: 0.01528, val loss: 0.01570\n",
      "Training epoch: 2247, train loss: 0.01586, val loss: 0.01623\n",
      "Training epoch: 2248, train loss: 0.01562, val loss: 0.01605\n",
      "Training epoch: 2249, train loss: 0.01552, val loss: 0.01594\n",
      "Training epoch: 2250, train loss: 0.01578, val loss: 0.01617\n",
      "Training epoch: 2251, train loss: 0.01539, val loss: 0.01577\n",
      "Training epoch: 2252, train loss: 0.01597, val loss: 0.01633\n",
      "Training epoch: 2253, train loss: 0.01555, val loss: 0.01597\n",
      "Training epoch: 2254, train loss: 0.01583, val loss: 0.01628\n",
      "Training epoch: 2255, train loss: 0.01560, val loss: 0.01604\n",
      "Training epoch: 2256, train loss: 0.01562, val loss: 0.01603\n",
      "Training epoch: 2257, train loss: 0.01593, val loss: 0.01634\n",
      "Training epoch: 2258, train loss: 0.01536, val loss: 0.01576\n",
      "Training epoch: 2259, train loss: 0.01544, val loss: 0.01586\n",
      "Training epoch: 2260, train loss: 0.01523, val loss: 0.01562\n",
      "Training epoch: 2261, train loss: 0.01547, val loss: 0.01587\n",
      "Training epoch: 2262, train loss: 0.01563, val loss: 0.01606\n",
      "Training epoch: 2263, train loss: 0.01541, val loss: 0.01583\n",
      "Training epoch: 2264, train loss: 0.01584, val loss: 0.01624\n",
      "Training epoch: 2265, train loss: 0.01555, val loss: 0.01595\n",
      "Training epoch: 2266, train loss: 0.01550, val loss: 0.01597\n",
      "Training epoch: 2267, train loss: 0.01563, val loss: 0.01600\n",
      "Training epoch: 2268, train loss: 0.01554, val loss: 0.01591\n",
      "Training epoch: 2269, train loss: 0.01565, val loss: 0.01606\n",
      "Training epoch: 2270, train loss: 0.01552, val loss: 0.01590\n",
      "Training epoch: 2271, train loss: 0.01551, val loss: 0.01587\n",
      "Training epoch: 2272, train loss: 0.01539, val loss: 0.01577\n",
      "Training epoch: 2273, train loss: 0.01528, val loss: 0.01566\n",
      "Training epoch: 2274, train loss: 0.01556, val loss: 0.01596\n",
      "Training epoch: 2275, train loss: 0.01594, val loss: 0.01636\n",
      "Training epoch: 2276, train loss: 0.01542, val loss: 0.01579\n",
      "Training epoch: 2277, train loss: 0.01538, val loss: 0.01577\n",
      "Training epoch: 2278, train loss: 0.01543, val loss: 0.01583\n",
      "Training epoch: 2279, train loss: 0.01538, val loss: 0.01580\n",
      "Training epoch: 2280, train loss: 0.01557, val loss: 0.01595\n",
      "Training epoch: 2281, train loss: 0.01562, val loss: 0.01599\n",
      "Training epoch: 2282, train loss: 0.01542, val loss: 0.01578\n",
      "Training epoch: 2283, train loss: 0.01532, val loss: 0.01572\n",
      "Training epoch: 2284, train loss: 0.01531, val loss: 0.01573\n",
      "Training epoch: 2285, train loss: 0.01540, val loss: 0.01581\n",
      "Training epoch: 2286, train loss: 0.01536, val loss: 0.01575\n",
      "Training epoch: 2287, train loss: 0.01525, val loss: 0.01567\n",
      "Training epoch: 2288, train loss: 0.01535, val loss: 0.01576\n",
      "Training epoch: 2289, train loss: 0.01537, val loss: 0.01573\n",
      "Training epoch: 2290, train loss: 0.01530, val loss: 0.01569\n",
      "Training epoch: 2291, train loss: 0.01537, val loss: 0.01572\n",
      "Training epoch: 2292, train loss: 0.01562, val loss: 0.01601\n",
      "Training epoch: 2293, train loss: 0.01536, val loss: 0.01576\n",
      "Training epoch: 2294, train loss: 0.01535, val loss: 0.01574\n",
      "Training epoch: 2295, train loss: 0.01559, val loss: 0.01600\n",
      "Training epoch: 2296, train loss: 0.01555, val loss: 0.01594\n",
      "Training epoch: 2297, train loss: 0.01530, val loss: 0.01568\n",
      "Training epoch: 2298, train loss: 0.01543, val loss: 0.01581\n",
      "Training epoch: 2299, train loss: 0.01572, val loss: 0.01612\n",
      "Training epoch: 2300, train loss: 0.01535, val loss: 0.01572\n",
      "Training epoch: 2301, train loss: 0.01556, val loss: 0.01596\n",
      "Training epoch: 2302, train loss: 0.01538, val loss: 0.01579\n",
      "Training epoch: 2303, train loss: 0.01533, val loss: 0.01569\n",
      "Training epoch: 2304, train loss: 0.01537, val loss: 0.01578\n",
      "Training epoch: 2305, train loss: 0.01528, val loss: 0.01569\n",
      "Training epoch: 2306, train loss: 0.01553, val loss: 0.01595\n",
      "Training epoch: 2307, train loss: 0.01527, val loss: 0.01568\n",
      "Training epoch: 2308, train loss: 0.01538, val loss: 0.01577\n",
      "Training epoch: 2309, train loss: 0.01569, val loss: 0.01607\n",
      "Training epoch: 2310, train loss: 0.01526, val loss: 0.01562\n",
      "Training epoch: 2311, train loss: 0.01536, val loss: 0.01576\n",
      "Training epoch: 2312, train loss: 0.01541, val loss: 0.01582\n",
      "Training epoch: 2313, train loss: 0.01545, val loss: 0.01587\n",
      "Training epoch: 2314, train loss: 0.01533, val loss: 0.01574\n",
      "Training epoch: 2315, train loss: 0.01548, val loss: 0.01594\n",
      "Training epoch: 2316, train loss: 0.01529, val loss: 0.01572\n",
      "Training epoch: 2317, train loss: 0.01539, val loss: 0.01579\n",
      "Training epoch: 2318, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 2319, train loss: 0.01537, val loss: 0.01577\n",
      "Training epoch: 2320, train loss: 0.01579, val loss: 0.01615\n",
      "Training epoch: 2321, train loss: 0.01544, val loss: 0.01581\n",
      "Training epoch: 2322, train loss: 0.01536, val loss: 0.01575\n",
      "Training epoch: 2323, train loss: 0.01536, val loss: 0.01575\n",
      "Training epoch: 2324, train loss: 0.01534, val loss: 0.01574\n",
      "Training epoch: 2325, train loss: 0.01545, val loss: 0.01583\n",
      "Training epoch: 2326, train loss: 0.01577, val loss: 0.01618\n",
      "Training epoch: 2327, train loss: 0.01547, val loss: 0.01586\n",
      "Training epoch: 2328, train loss: 0.01570, val loss: 0.01612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 2329, train loss: 0.01553, val loss: 0.01592\n",
      "Training epoch: 2330, train loss: 0.01522, val loss: 0.01562\n",
      "Training epoch: 2331, train loss: 0.01530, val loss: 0.01572\n",
      "Training epoch: 2332, train loss: 0.01608, val loss: 0.01650\n",
      "Training epoch: 2333, train loss: 0.01561, val loss: 0.01601\n",
      "Training epoch: 2334, train loss: 0.01566, val loss: 0.01602\n",
      "Training epoch: 2335, train loss: 0.01567, val loss: 0.01603\n",
      "Training epoch: 2336, train loss: 0.01554, val loss: 0.01588\n",
      "Training epoch: 2337, train loss: 0.01548, val loss: 0.01590\n",
      "Training epoch: 2338, train loss: 0.01556, val loss: 0.01592\n",
      "Training epoch: 2339, train loss: 0.01532, val loss: 0.01569\n",
      "Training epoch: 2340, train loss: 0.01579, val loss: 0.01616\n",
      "Training epoch: 2341, train loss: 0.01574, val loss: 0.01611\n",
      "Training epoch: 2342, train loss: 0.01566, val loss: 0.01604\n",
      "Training epoch: 2343, train loss: 0.01550, val loss: 0.01590\n",
      "Training epoch: 2344, train loss: 0.01522, val loss: 0.01562\n",
      "Training epoch: 2345, train loss: 0.01541, val loss: 0.01582\n",
      "Training epoch: 2346, train loss: 0.01576, val loss: 0.01617\n",
      "Training epoch: 2347, train loss: 0.01548, val loss: 0.01587\n",
      "Training epoch: 2348, train loss: 0.01532, val loss: 0.01569\n",
      "Training epoch: 2349, train loss: 0.01625, val loss: 0.01668\n",
      "Training epoch: 2350, train loss: 0.01590, val loss: 0.01632\n",
      "Training epoch: 2351, train loss: 0.01571, val loss: 0.01607\n",
      "Training epoch: 2352, train loss: 0.01543, val loss: 0.01581\n",
      "Training epoch: 2353, train loss: 0.01522, val loss: 0.01562\n",
      "Training epoch: 2354, train loss: 0.01542, val loss: 0.01580\n",
      "Training epoch: 2355, train loss: 0.01556, val loss: 0.01599\n",
      "Training epoch: 2356, train loss: 0.01546, val loss: 0.01582\n",
      "Training epoch: 2357, train loss: 0.01546, val loss: 0.01584\n",
      "Training epoch: 2358, train loss: 0.01526, val loss: 0.01565\n",
      "Training epoch: 2359, train loss: 0.01530, val loss: 0.01572\n",
      "Training epoch: 2360, train loss: 0.01566, val loss: 0.01605\n",
      "Training epoch: 2361, train loss: 0.01524, val loss: 0.01564\n",
      "Training epoch: 2362, train loss: 0.01553, val loss: 0.01590\n",
      "Training epoch: 2363, train loss: 0.01558, val loss: 0.01592\n",
      "Training epoch: 2364, train loss: 0.01563, val loss: 0.01599\n",
      "Training epoch: 2365, train loss: 0.01543, val loss: 0.01585\n",
      "Training epoch: 2366, train loss: 0.01529, val loss: 0.01567\n",
      "Training epoch: 2367, train loss: 0.01576, val loss: 0.01615\n",
      "Training epoch: 2368, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 2369, train loss: 0.01530, val loss: 0.01569\n",
      "Training epoch: 2370, train loss: 0.01543, val loss: 0.01582\n",
      "Training epoch: 2371, train loss: 0.01537, val loss: 0.01574\n",
      "Training epoch: 2372, train loss: 0.01529, val loss: 0.01568\n",
      "Training epoch: 2373, train loss: 0.01541, val loss: 0.01582\n",
      "Training epoch: 2374, train loss: 0.01534, val loss: 0.01574\n",
      "Training epoch: 2375, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 2376, train loss: 0.01529, val loss: 0.01565\n",
      "Training epoch: 2377, train loss: 0.01543, val loss: 0.01586\n",
      "Training epoch: 2378, train loss: 0.01534, val loss: 0.01572\n",
      "Training epoch: 2379, train loss: 0.01543, val loss: 0.01578\n",
      "Training epoch: 2380, train loss: 0.01565, val loss: 0.01601\n",
      "Training epoch: 2381, train loss: 0.01600, val loss: 0.01644\n",
      "Training epoch: 2382, train loss: 0.01556, val loss: 0.01591\n",
      "Training epoch: 2383, train loss: 0.01615, val loss: 0.01658\n",
      "Training epoch: 2384, train loss: 0.01586, val loss: 0.01625\n",
      "Training epoch: 2385, train loss: 0.01530, val loss: 0.01571\n",
      "Training epoch: 2386, train loss: 0.01624, val loss: 0.01662\n",
      "Training epoch: 2387, train loss: 0.01533, val loss: 0.01574\n",
      "Training epoch: 2388, train loss: 0.01547, val loss: 0.01583\n",
      "Training epoch: 2389, train loss: 0.01615, val loss: 0.01653\n",
      "Training epoch: 2390, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 2391, train loss: 0.01554, val loss: 0.01594\n",
      "Training epoch: 2392, train loss: 0.01526, val loss: 0.01568\n",
      "Training epoch: 2393, train loss: 0.01529, val loss: 0.01571\n",
      "Training epoch: 2394, train loss: 0.01546, val loss: 0.01579\n",
      "Training epoch: 2395, train loss: 0.01556, val loss: 0.01593\n",
      "Training epoch: 2396, train loss: 0.01526, val loss: 0.01567\n",
      "Training epoch: 2397, train loss: 0.01550, val loss: 0.01588\n",
      "Training epoch: 2398, train loss: 0.01566, val loss: 0.01606\n",
      "Training epoch: 2399, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 2400, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 2401, train loss: 0.01525, val loss: 0.01565\n",
      "Training epoch: 2402, train loss: 0.01636, val loss: 0.01677\n",
      "Training epoch: 2403, train loss: 0.01531, val loss: 0.01569\n",
      "Training epoch: 2404, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 2405, train loss: 0.01541, val loss: 0.01579\n",
      "Training epoch: 2406, train loss: 0.01577, val loss: 0.01619\n",
      "Training epoch: 2407, train loss: 0.01597, val loss: 0.01638\n",
      "Training epoch: 2408, train loss: 0.01540, val loss: 0.01580\n",
      "Training epoch: 2409, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 2410, train loss: 0.01527, val loss: 0.01563\n",
      "Training epoch: 2411, train loss: 0.01565, val loss: 0.01601\n",
      "Training epoch: 2412, train loss: 0.01583, val loss: 0.01623\n",
      "Training epoch: 2413, train loss: 0.01527, val loss: 0.01567\n",
      "Training epoch: 2414, train loss: 0.01540, val loss: 0.01574\n",
      "Training epoch: 2415, train loss: 0.01535, val loss: 0.01572\n",
      "Training epoch: 2416, train loss: 0.01537, val loss: 0.01578\n",
      "Training epoch: 2417, train loss: 0.01535, val loss: 0.01572\n",
      "Training epoch: 2418, train loss: 0.01538, val loss: 0.01580\n",
      "Training epoch: 2419, train loss: 0.01545, val loss: 0.01588\n",
      "Training epoch: 2420, train loss: 0.01554, val loss: 0.01594\n",
      "Training epoch: 2421, train loss: 0.01534, val loss: 0.01573\n",
      "Training epoch: 2422, train loss: 0.01551, val loss: 0.01595\n",
      "Training epoch: 2423, train loss: 0.01561, val loss: 0.01603\n",
      "Training epoch: 2424, train loss: 0.01568, val loss: 0.01610\n",
      "Training epoch: 2425, train loss: 0.01617, val loss: 0.01660\n",
      "Training epoch: 2426, train loss: 0.01542, val loss: 0.01577\n",
      "Training epoch: 2427, train loss: 0.01544, val loss: 0.01582\n",
      "Training epoch: 2428, train loss: 0.01531, val loss: 0.01572\n",
      "Training epoch: 2429, train loss: 0.01568, val loss: 0.01610\n",
      "Training epoch: 2430, train loss: 0.01537, val loss: 0.01575\n",
      "Training epoch: 2431, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 2432, train loss: 0.01557, val loss: 0.01596\n",
      "Training epoch: 2433, train loss: 0.01564, val loss: 0.01603\n",
      "Training epoch: 2434, train loss: 0.01547, val loss: 0.01584\n",
      "Training epoch: 2435, train loss: 0.01534, val loss: 0.01574\n",
      "Training epoch: 2436, train loss: 0.01548, val loss: 0.01587\n",
      "Training epoch: 2437, train loss: 0.01552, val loss: 0.01591\n",
      "Training epoch: 2438, train loss: 0.01543, val loss: 0.01584\n",
      "Training epoch: 2439, train loss: 0.01574, val loss: 0.01612\n",
      "Training epoch: 2440, train loss: 0.01551, val loss: 0.01591\n",
      "Training epoch: 2441, train loss: 0.01564, val loss: 0.01603\n",
      "Training epoch: 2442, train loss: 0.01531, val loss: 0.01569\n",
      "Training epoch: 2443, train loss: 0.01545, val loss: 0.01584\n",
      "Training epoch: 2444, train loss: 0.01570, val loss: 0.01605\n",
      "Training epoch: 2445, train loss: 0.01583, val loss: 0.01621\n",
      "Training epoch: 2446, train loss: 0.01539, val loss: 0.01576\n",
      "Training epoch: 2447, train loss: 0.01572, val loss: 0.01605\n",
      "Training epoch: 2448, train loss: 0.01531, val loss: 0.01568\n",
      "Training epoch: 2449, train loss: 0.01527, val loss: 0.01567\n",
      "Training epoch: 2450, train loss: 0.01543, val loss: 0.01582\n",
      "Training epoch: 2451, train loss: 0.01537, val loss: 0.01577\n",
      "Training epoch: 2452, train loss: 0.01560, val loss: 0.01598\n",
      "Training epoch: 2453, train loss: 0.01526, val loss: 0.01565\n",
      "Training epoch: 2454, train loss: 0.01541, val loss: 0.01578\n",
      "Training epoch: 2455, train loss: 0.01544, val loss: 0.01581\n",
      "Training epoch: 2456, train loss: 0.01546, val loss: 0.01586\n",
      "Training epoch: 2457, train loss: 0.01534, val loss: 0.01573\n",
      "Training epoch: 2458, train loss: 0.01545, val loss: 0.01583\n",
      "Training epoch: 2459, train loss: 0.01538, val loss: 0.01575\n",
      "Training epoch: 2460, train loss: 0.01570, val loss: 0.01612\n",
      "Training epoch: 2461, train loss: 0.01524, val loss: 0.01563\n",
      "Training epoch: 2462, train loss: 0.01564, val loss: 0.01603\n",
      "Training epoch: 2463, train loss: 0.01566, val loss: 0.01605\n",
      "Training epoch: 2464, train loss: 0.01572, val loss: 0.01609\n",
      "Training epoch: 2465, train loss: 0.01558, val loss: 0.01599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 2466, train loss: 0.01545, val loss: 0.01582\n",
      "Training epoch: 2467, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 2468, train loss: 0.01558, val loss: 0.01598\n",
      "Training epoch: 2469, train loss: 0.01526, val loss: 0.01565\n",
      "Training epoch: 2470, train loss: 0.01544, val loss: 0.01583\n",
      "Training epoch: 2471, train loss: 0.01588, val loss: 0.01628\n",
      "Training epoch: 2472, train loss: 0.01539, val loss: 0.01575\n",
      "Training epoch: 2473, train loss: 0.01573, val loss: 0.01610\n",
      "Training epoch: 2474, train loss: 0.01526, val loss: 0.01566\n",
      "Training epoch: 2475, train loss: 0.01533, val loss: 0.01572\n",
      "Training epoch: 2476, train loss: 0.01543, val loss: 0.01580\n",
      "Training epoch: 2477, train loss: 0.01529, val loss: 0.01568\n",
      "Training epoch: 2478, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 2479, train loss: 0.01528, val loss: 0.01570\n",
      "Training epoch: 2480, train loss: 0.01548, val loss: 0.01582\n",
      "Training epoch: 2481, train loss: 0.01543, val loss: 0.01581\n",
      "Training epoch: 2482, train loss: 0.01548, val loss: 0.01588\n",
      "Training epoch: 2483, train loss: 0.01582, val loss: 0.01621\n",
      "Training epoch: 2484, train loss: 0.01578, val loss: 0.01618\n",
      "Training epoch: 2485, train loss: 0.01537, val loss: 0.01577\n",
      "Training epoch: 2486, train loss: 0.01538, val loss: 0.01580\n",
      "Training epoch: 2487, train loss: 0.01528, val loss: 0.01566\n",
      "Training epoch: 2488, train loss: 0.01540, val loss: 0.01577\n",
      "Training epoch: 2489, train loss: 0.01622, val loss: 0.01665\n",
      "Training epoch: 2490, train loss: 0.01537, val loss: 0.01576\n",
      "Training epoch: 2491, train loss: 0.01562, val loss: 0.01602\n",
      "Training epoch: 2492, train loss: 0.01555, val loss: 0.01594\n",
      "Training epoch: 2493, train loss: 0.01563, val loss: 0.01603\n",
      "Training epoch: 2494, train loss: 0.01564, val loss: 0.01606\n",
      "Training epoch: 2495, train loss: 0.01566, val loss: 0.01605\n",
      "Training epoch: 2496, train loss: 0.01554, val loss: 0.01593\n",
      "Training epoch: 2497, train loss: 0.01560, val loss: 0.01597\n",
      "Training epoch: 2498, train loss: 0.01590, val loss: 0.01630\n",
      "Training epoch: 2499, train loss: 0.01561, val loss: 0.01602\n",
      "Training epoch: 2500, train loss: 0.01540, val loss: 0.01577\n",
      "Training epoch: 2501, train loss: 0.01524, val loss: 0.01564\n",
      "Training epoch: 2502, train loss: 0.01549, val loss: 0.01587\n",
      "Training epoch: 2503, train loss: 0.01577, val loss: 0.01616\n",
      "Training epoch: 2504, train loss: 0.01560, val loss: 0.01598\n",
      "Training epoch: 2505, train loss: 0.01533, val loss: 0.01574\n",
      "Training epoch: 2506, train loss: 0.01671, val loss: 0.01713\n",
      "Training epoch: 2507, train loss: 0.01548, val loss: 0.01586\n",
      "Training epoch: 2508, train loss: 0.01538, val loss: 0.01576\n",
      "Training epoch: 2509, train loss: 0.01540, val loss: 0.01576\n",
      "Training epoch: 2510, train loss: 0.01537, val loss: 0.01575\n",
      "Training epoch: 2511, train loss: 0.01546, val loss: 0.01584\n",
      "Training epoch: 2512, train loss: 0.01550, val loss: 0.01583\n",
      "Training epoch: 2513, train loss: 0.01538, val loss: 0.01575\n",
      "Training epoch: 2514, train loss: 0.01543, val loss: 0.01580\n",
      "Training epoch: 2515, train loss: 0.01534, val loss: 0.01576\n",
      "Training epoch: 2516, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 2517, train loss: 0.01536, val loss: 0.01572\n",
      "Training epoch: 2518, train loss: 0.01535, val loss: 0.01570\n",
      "Training epoch: 2519, train loss: 0.01527, val loss: 0.01566\n",
      "Training epoch: 2520, train loss: 0.01617, val loss: 0.01654\n",
      "Training epoch: 2521, train loss: 0.01591, val loss: 0.01632\n",
      "Training epoch: 2522, train loss: 0.01690, val loss: 0.01732\n",
      "Training epoch: 2523, train loss: 0.01590, val loss: 0.01632\n",
      "Training epoch: 2524, train loss: 0.01532, val loss: 0.01574\n",
      "Training epoch: 2525, train loss: 0.01560, val loss: 0.01600\n",
      "Training epoch: 2526, train loss: 0.01527, val loss: 0.01566\n",
      "Training epoch: 2527, train loss: 0.01535, val loss: 0.01574\n",
      "Training epoch: 2528, train loss: 0.01536, val loss: 0.01575\n",
      "Training epoch: 2529, train loss: 0.01532, val loss: 0.01570\n",
      "Training epoch: 2530, train loss: 0.01576, val loss: 0.01611\n",
      "Training epoch: 2531, train loss: 0.01567, val loss: 0.01610\n",
      "Training epoch: 2532, train loss: 0.01527, val loss: 0.01566\n",
      "Training epoch: 2533, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 2534, train loss: 0.01558, val loss: 0.01594\n",
      "Training epoch: 2535, train loss: 0.01605, val loss: 0.01641\n",
      "Training epoch: 2536, train loss: 0.01594, val loss: 0.01627\n",
      "Training epoch: 2537, train loss: 0.01552, val loss: 0.01588\n",
      "Training epoch: 2538, train loss: 0.01594, val loss: 0.01630\n",
      "Training epoch: 2539, train loss: 0.01593, val loss: 0.01629\n",
      "Training epoch: 2540, train loss: 0.01529, val loss: 0.01563\n",
      "Training epoch: 2541, train loss: 0.01552, val loss: 0.01586\n",
      "Training epoch: 2542, train loss: 0.01537, val loss: 0.01577\n",
      "Training epoch: 2543, train loss: 0.01529, val loss: 0.01568\n",
      "Training epoch: 2544, train loss: 0.01537, val loss: 0.01575\n",
      "Training epoch: 2545, train loss: 0.01557, val loss: 0.01596\n",
      "Training epoch: 2546, train loss: 0.01562, val loss: 0.01603\n",
      "Training epoch: 2547, train loss: 0.01559, val loss: 0.01601\n",
      "Training epoch: 2548, train loss: 0.01548, val loss: 0.01588\n",
      "Training epoch: 2549, train loss: 0.01533, val loss: 0.01573\n",
      "Training epoch: 2550, train loss: 0.01558, val loss: 0.01594\n",
      "Training epoch: 2551, train loss: 0.01523, val loss: 0.01562\n",
      "Training epoch: 2552, train loss: 0.01570, val loss: 0.01603\n",
      "Training epoch: 2553, train loss: 0.01534, val loss: 0.01575\n",
      "Training epoch: 2554, train loss: 0.01532, val loss: 0.01572\n",
      "Training epoch: 2555, train loss: 0.01554, val loss: 0.01589\n",
      "Training epoch: 2556, train loss: 0.01551, val loss: 0.01590\n",
      "Training epoch: 2557, train loss: 0.01528, val loss: 0.01565\n",
      "Training epoch: 2558, train loss: 0.01552, val loss: 0.01590\n",
      "Training epoch: 2559, train loss: 0.01528, val loss: 0.01567\n",
      "Training epoch: 2560, train loss: 0.01551, val loss: 0.01585\n",
      "Training epoch: 2561, train loss: 0.01550, val loss: 0.01589\n",
      "Training epoch: 2562, train loss: 0.01553, val loss: 0.01589\n",
      "Training epoch: 2563, train loss: 0.01578, val loss: 0.01619\n",
      "Training epoch: 2564, train loss: 0.01565, val loss: 0.01606\n",
      "Training epoch: 2565, train loss: 0.01549, val loss: 0.01588\n",
      "Training epoch: 2566, train loss: 0.01528, val loss: 0.01565\n",
      "Training epoch: 2567, train loss: 0.01530, val loss: 0.01571\n",
      "Training epoch: 2568, train loss: 0.01530, val loss: 0.01566\n",
      "Training epoch: 2569, train loss: 0.01549, val loss: 0.01588\n",
      "Training epoch: 2570, train loss: 0.01557, val loss: 0.01598\n",
      "Training epoch: 2571, train loss: 0.01530, val loss: 0.01568\n",
      "Training epoch: 2572, train loss: 0.01564, val loss: 0.01600\n",
      "Training epoch: 2573, train loss: 0.01574, val loss: 0.01609\n",
      "Training epoch: 2574, train loss: 0.01533, val loss: 0.01574\n",
      "Training epoch: 2575, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 2576, train loss: 0.01541, val loss: 0.01580\n",
      "Training epoch: 2577, train loss: 0.01598, val loss: 0.01638\n",
      "Training epoch: 2578, train loss: 0.01545, val loss: 0.01580\n",
      "Training epoch: 2579, train loss: 0.01537, val loss: 0.01577\n",
      "Training epoch: 2580, train loss: 0.01537, val loss: 0.01575\n",
      "Training epoch: 2581, train loss: 0.01535, val loss: 0.01570\n",
      "Training epoch: 2582, train loss: 0.01533, val loss: 0.01573\n",
      "Training epoch: 2583, train loss: 0.01544, val loss: 0.01583\n",
      "Training epoch: 2584, train loss: 0.01531, val loss: 0.01569\n",
      "Training epoch: 2585, train loss: 0.01535, val loss: 0.01574\n",
      "Training epoch: 2586, train loss: 0.01529, val loss: 0.01567\n",
      "Training epoch: 2587, train loss: 0.01537, val loss: 0.01575\n",
      "Training epoch: 2588, train loss: 0.01542, val loss: 0.01581\n",
      "Training epoch: 2589, train loss: 0.01528, val loss: 0.01565\n",
      "Training epoch: 2590, train loss: 0.01534, val loss: 0.01574\n",
      "Training epoch: 2591, train loss: 0.01567, val loss: 0.01602\n",
      "Training epoch: 2592, train loss: 0.01532, val loss: 0.01570\n",
      "Training epoch: 2593, train loss: 0.01528, val loss: 0.01563\n",
      "Training epoch: 2594, train loss: 0.01527, val loss: 0.01564\n",
      "Training epoch: 2595, train loss: 0.01529, val loss: 0.01566\n",
      "Training epoch: 2596, train loss: 0.01533, val loss: 0.01570\n",
      "Training epoch: 2597, train loss: 0.01532, val loss: 0.01572\n",
      "Training epoch: 2598, train loss: 0.01527, val loss: 0.01567\n",
      "Training epoch: 2599, train loss: 0.01544, val loss: 0.01582\n",
      "Training epoch: 2600, train loss: 0.01551, val loss: 0.01590\n",
      "Training epoch: 2601, train loss: 0.01529, val loss: 0.01566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 2602, train loss: 0.01631, val loss: 0.01665\n",
      "Training epoch: 2603, train loss: 0.01535, val loss: 0.01577\n",
      "Training epoch: 2604, train loss: 0.01544, val loss: 0.01585\n",
      "Training epoch: 2605, train loss: 0.01580, val loss: 0.01619\n",
      "Training epoch: 2606, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 2607, train loss: 0.01535, val loss: 0.01575\n",
      "Training epoch: 2608, train loss: 0.01550, val loss: 0.01584\n",
      "Training epoch: 2609, train loss: 0.01540, val loss: 0.01580\n",
      "Training epoch: 2610, train loss: 0.01556, val loss: 0.01595\n",
      "Training epoch: 2611, train loss: 0.01542, val loss: 0.01580\n",
      "Training epoch: 2612, train loss: 0.01546, val loss: 0.01586\n",
      "Training epoch: 2613, train loss: 0.01529, val loss: 0.01568\n",
      "Training epoch: 2614, train loss: 0.01530, val loss: 0.01566\n",
      "Training epoch: 2615, train loss: 0.01539, val loss: 0.01577\n",
      "Training epoch: 2616, train loss: 0.01543, val loss: 0.01582\n",
      "Training epoch: 2617, train loss: 0.01567, val loss: 0.01604\n",
      "Training epoch: 2618, train loss: 0.01561, val loss: 0.01598\n",
      "Training epoch: 2619, train loss: 0.01587, val loss: 0.01621\n",
      "Training epoch: 2620, train loss: 0.01533, val loss: 0.01573\n",
      "Training epoch: 2621, train loss: 0.01527, val loss: 0.01567\n",
      "Training epoch: 2622, train loss: 0.01550, val loss: 0.01588\n",
      "Training epoch: 2623, train loss: 0.01530, val loss: 0.01567\n",
      "Training epoch: 2624, train loss: 0.01549, val loss: 0.01589\n",
      "Training epoch: 2625, train loss: 0.01540, val loss: 0.01578\n",
      "Training epoch: 2626, train loss: 0.01569, val loss: 0.01605\n",
      "Training epoch: 2627, train loss: 0.01598, val loss: 0.01636\n",
      "Training epoch: 2628, train loss: 0.01546, val loss: 0.01584\n",
      "Training epoch: 2629, train loss: 0.01577, val loss: 0.01612\n",
      "Training epoch: 2630, train loss: 0.01538, val loss: 0.01576\n",
      "Training epoch: 2631, train loss: 0.01523, val loss: 0.01562\n",
      "Training epoch: 2632, train loss: 0.01530, val loss: 0.01569\n",
      "Training epoch: 2633, train loss: 0.01545, val loss: 0.01586\n",
      "Training epoch: 2634, train loss: 0.01538, val loss: 0.01578\n",
      "Training epoch: 2635, train loss: 0.01554, val loss: 0.01588\n",
      "Training epoch: 2636, train loss: 0.01539, val loss: 0.01580\n",
      "Training epoch: 2637, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 2638, train loss: 0.01699, val loss: 0.01737\n",
      "Training epoch: 2639, train loss: 0.01544, val loss: 0.01583\n",
      "Training epoch: 2640, train loss: 0.01538, val loss: 0.01573\n",
      "Training epoch: 2641, train loss: 0.01547, val loss: 0.01579\n",
      "Training epoch: 2642, train loss: 0.01538, val loss: 0.01579\n",
      "Training epoch: 2643, train loss: 0.01540, val loss: 0.01580\n",
      "Training epoch: 2644, train loss: 0.01549, val loss: 0.01588\n",
      "Training epoch: 2645, train loss: 0.01547, val loss: 0.01585\n",
      "Training epoch: 2646, train loss: 0.01561, val loss: 0.01599\n",
      "Training epoch: 2647, train loss: 0.01553, val loss: 0.01595\n",
      "Training epoch: 2648, train loss: 0.01545, val loss: 0.01586\n",
      "Training epoch: 2649, train loss: 0.01600, val loss: 0.01647\n",
      "Training epoch: 2650, train loss: 0.01546, val loss: 0.01589\n",
      "Training epoch: 2651, train loss: 0.01544, val loss: 0.01582\n",
      "Training epoch: 2652, train loss: 0.01591, val loss: 0.01632\n",
      "Training epoch: 2653, train loss: 0.01563, val loss: 0.01601\n",
      "Training epoch: 2654, train loss: 0.01557, val loss: 0.01594\n",
      "Training epoch: 2655, train loss: 0.01549, val loss: 0.01588\n",
      "Training epoch: 2656, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 2657, train loss: 0.01544, val loss: 0.01587\n",
      "Training epoch: 2658, train loss: 0.01582, val loss: 0.01622\n",
      "Training epoch: 2659, train loss: 0.01532, val loss: 0.01571\n",
      "Training epoch: 2660, train loss: 0.01547, val loss: 0.01587\n",
      "Training epoch: 2661, train loss: 0.01534, val loss: 0.01571\n",
      "Training epoch: 2662, train loss: 0.01545, val loss: 0.01586\n",
      "Training epoch: 2663, train loss: 0.01535, val loss: 0.01572\n",
      "Training epoch: 2664, train loss: 0.01566, val loss: 0.01610\n",
      "Training epoch: 2665, train loss: 0.01570, val loss: 0.01603\n",
      "Training epoch: 2666, train loss: 0.01567, val loss: 0.01603\n",
      "Training epoch: 2667, train loss: 0.01546, val loss: 0.01584\n",
      "Training epoch: 2668, train loss: 0.01567, val loss: 0.01605\n",
      "Training epoch: 2669, train loss: 0.01526, val loss: 0.01565\n",
      "Training epoch: 2670, train loss: 0.01545, val loss: 0.01586\n",
      "Training epoch: 2671, train loss: 0.01562, val loss: 0.01596\n",
      "Training epoch: 2672, train loss: 0.01530, val loss: 0.01569\n",
      "Training epoch: 2673, train loss: 0.01533, val loss: 0.01573\n",
      "Training epoch: 2674, train loss: 0.01527, val loss: 0.01567\n",
      "Training epoch: 2675, train loss: 0.01532, val loss: 0.01570\n",
      "Training epoch: 2676, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 2677, train loss: 0.01576, val loss: 0.01621\n",
      "Training epoch: 2678, train loss: 0.01534, val loss: 0.01574\n",
      "Training epoch: 2679, train loss: 0.01539, val loss: 0.01579\n",
      "Training epoch: 2680, train loss: 0.01564, val loss: 0.01600\n",
      "Training epoch: 2681, train loss: 0.01536, val loss: 0.01576\n",
      "Training epoch: 2682, train loss: 0.01537, val loss: 0.01575\n",
      "Training epoch: 2683, train loss: 0.01533, val loss: 0.01575\n",
      "Training epoch: 2684, train loss: 0.01540, val loss: 0.01580\n",
      "Training epoch: 2685, train loss: 0.01553, val loss: 0.01588\n",
      "Training epoch: 2686, train loss: 0.01559, val loss: 0.01591\n",
      "Training epoch: 2687, train loss: 0.01633, val loss: 0.01675\n",
      "Training epoch: 2688, train loss: 0.01572, val loss: 0.01609\n",
      "Training epoch: 2689, train loss: 0.01561, val loss: 0.01600\n",
      "Training epoch: 2690, train loss: 0.01553, val loss: 0.01590\n",
      "Training epoch: 2691, train loss: 0.01534, val loss: 0.01572\n",
      "Training epoch: 2692, train loss: 0.01536, val loss: 0.01578\n",
      "Training epoch: 2693, train loss: 0.01528, val loss: 0.01566\n",
      "Training epoch: 2694, train loss: 0.01598, val loss: 0.01640\n",
      "Training epoch: 2695, train loss: 0.01539, val loss: 0.01577\n",
      "Training epoch: 2696, train loss: 0.01527, val loss: 0.01563\n",
      "Training epoch: 2697, train loss: 0.01535, val loss: 0.01573\n",
      "Training epoch: 2698, train loss: 0.01576, val loss: 0.01617\n",
      "Training epoch: 2699, train loss: 0.01537, val loss: 0.01575\n",
      "Training epoch: 2700, train loss: 0.01553, val loss: 0.01589\n",
      "Training epoch: 2701, train loss: 0.01546, val loss: 0.01586\n",
      "Training epoch: 2702, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 2703, train loss: 0.01538, val loss: 0.01577\n",
      "Training epoch: 2704, train loss: 0.01582, val loss: 0.01618\n",
      "Training epoch: 2705, train loss: 0.01542, val loss: 0.01578\n",
      "Training epoch: 2706, train loss: 0.01554, val loss: 0.01593\n",
      "Training epoch: 2707, train loss: 0.01561, val loss: 0.01598\n",
      "Training epoch: 2708, train loss: 0.01560, val loss: 0.01596\n",
      "Training epoch: 2709, train loss: 0.01589, val loss: 0.01625\n",
      "Training epoch: 2710, train loss: 0.01593, val loss: 0.01629\n",
      "Training epoch: 2711, train loss: 0.01578, val loss: 0.01616\n",
      "Training epoch: 2712, train loss: 0.01562, val loss: 0.01602\n",
      "Training epoch: 2713, train loss: 0.01548, val loss: 0.01588\n",
      "Training epoch: 2714, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 2715, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 2716, train loss: 0.01581, val loss: 0.01623\n",
      "Training epoch: 2717, train loss: 0.01532, val loss: 0.01575\n",
      "Training epoch: 2718, train loss: 0.01546, val loss: 0.01580\n",
      "Training epoch: 2719, train loss: 0.01544, val loss: 0.01586\n",
      "Training epoch: 2720, train loss: 0.01536, val loss: 0.01577\n",
      "Training epoch: 2721, train loss: 0.01570, val loss: 0.01612\n",
      "Training epoch: 2722, train loss: 0.01554, val loss: 0.01589\n",
      "Training epoch: 2723, train loss: 0.01530, val loss: 0.01566\n",
      "Training epoch: 2724, train loss: 0.01547, val loss: 0.01588\n",
      "Training epoch: 2725, train loss: 0.01541, val loss: 0.01584\n",
      "Training epoch: 2726, train loss: 0.01533, val loss: 0.01568\n",
      "Training epoch: 2727, train loss: 0.01556, val loss: 0.01593\n",
      "Training epoch: 2728, train loss: 0.01544, val loss: 0.01583\n",
      "Training epoch: 2729, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 2730, train loss: 0.01539, val loss: 0.01580\n",
      "Training epoch: 2731, train loss: 0.01546, val loss: 0.01583\n",
      "Training epoch: 2732, train loss: 0.01544, val loss: 0.01582\n",
      "Training epoch: 2733, train loss: 0.01550, val loss: 0.01588\n",
      "Training epoch: 2734, train loss: 0.01542, val loss: 0.01580\n",
      "Training epoch: 2735, train loss: 0.01564, val loss: 0.01604\n",
      "Training epoch: 2736, train loss: 0.01546, val loss: 0.01580\n",
      "Training epoch: 2737, train loss: 0.01531, val loss: 0.01572\n",
      "Training epoch: 2738, train loss: 0.01524, val loss: 0.01564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 2739, train loss: 0.01537, val loss: 0.01572\n",
      "Training epoch: 2740, train loss: 0.01546, val loss: 0.01582\n",
      "Training epoch: 2741, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 2742, train loss: 0.01566, val loss: 0.01605\n",
      "Training epoch: 2743, train loss: 0.01552, val loss: 0.01596\n",
      "Training epoch: 2744, train loss: 0.01546, val loss: 0.01584\n",
      "Training epoch: 2745, train loss: 0.01597, val loss: 0.01634\n",
      "Training epoch: 2746, train loss: 0.01553, val loss: 0.01595\n",
      "Training epoch: 2747, train loss: 0.01536, val loss: 0.01574\n",
      "Training epoch: 2748, train loss: 0.01545, val loss: 0.01584\n",
      "Training epoch: 2749, train loss: 0.01528, val loss: 0.01567\n",
      "Training epoch: 2750, train loss: 0.01547, val loss: 0.01587\n",
      "Training epoch: 2751, train loss: 0.01529, val loss: 0.01564\n",
      "Training epoch: 2752, train loss: 0.01542, val loss: 0.01581\n",
      "Training epoch: 2753, train loss: 0.01535, val loss: 0.01574\n",
      "Training epoch: 2754, train loss: 0.01522, val loss: 0.01561\n",
      "Training epoch: 2755, train loss: 0.01567, val loss: 0.01604\n",
      "Training epoch: 2756, train loss: 0.01528, val loss: 0.01568\n",
      "Training epoch: 2757, train loss: 0.01525, val loss: 0.01560\n",
      "Training epoch: 2758, train loss: 0.01573, val loss: 0.01614\n",
      "Training epoch: 2759, train loss: 0.01524, val loss: 0.01559\n",
      "Training epoch: 2760, train loss: 0.01529, val loss: 0.01565\n",
      "Training epoch: 2761, train loss: 0.01542, val loss: 0.01582\n",
      "Training epoch: 2762, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 2763, train loss: 0.01531, val loss: 0.01566\n",
      "Training epoch: 2764, train loss: 0.01554, val loss: 0.01590\n",
      "Training epoch: 2765, train loss: 0.01536, val loss: 0.01572\n",
      "Training epoch: 2766, train loss: 0.01522, val loss: 0.01562\n",
      "Training epoch: 2767, train loss: 0.01595, val loss: 0.01633\n",
      "Training epoch: 2768, train loss: 0.01546, val loss: 0.01584\n",
      "Training epoch: 2769, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 2770, train loss: 0.01564, val loss: 0.01599\n",
      "Training epoch: 2771, train loss: 0.01556, val loss: 0.01595\n",
      "Training epoch: 2772, train loss: 0.01544, val loss: 0.01585\n",
      "Training epoch: 2773, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 2774, train loss: 0.01535, val loss: 0.01574\n",
      "Training epoch: 2775, train loss: 0.01538, val loss: 0.01574\n",
      "Training epoch: 2776, train loss: 0.01537, val loss: 0.01573\n",
      "Training epoch: 2777, train loss: 0.01540, val loss: 0.01576\n",
      "Training epoch: 2778, train loss: 0.01575, val loss: 0.01613\n",
      "Training epoch: 2779, train loss: 0.01536, val loss: 0.01573\n",
      "Training epoch: 2780, train loss: 0.01545, val loss: 0.01583\n",
      "Training epoch: 2781, train loss: 0.01547, val loss: 0.01589\n",
      "Training epoch: 2782, train loss: 0.01562, val loss: 0.01600\n",
      "Training epoch: 2783, train loss: 0.01538, val loss: 0.01575\n",
      "Training epoch: 2784, train loss: 0.01536, val loss: 0.01576\n",
      "Training epoch: 2785, train loss: 0.01602, val loss: 0.01643\n",
      "Training epoch: 2786, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 2787, train loss: 0.01524, val loss: 0.01563\n",
      "Training epoch: 2788, train loss: 0.01539, val loss: 0.01580\n",
      "Training epoch: 2789, train loss: 0.01543, val loss: 0.01580\n",
      "Training epoch: 2790, train loss: 0.01563, val loss: 0.01606\n",
      "Training epoch: 2791, train loss: 0.01591, val loss: 0.01634\n",
      "Training epoch: 2792, train loss: 0.01533, val loss: 0.01571\n",
      "Training epoch: 2793, train loss: 0.01537, val loss: 0.01574\n",
      "Training epoch: 2794, train loss: 0.01558, val loss: 0.01596\n",
      "Training epoch: 2795, train loss: 0.01529, val loss: 0.01565\n",
      "Training epoch: 2796, train loss: 0.01539, val loss: 0.01578\n",
      "Training epoch: 2797, train loss: 0.01547, val loss: 0.01587\n",
      "Training epoch: 2798, train loss: 0.01533, val loss: 0.01572\n",
      "Training epoch: 2799, train loss: 0.01575, val loss: 0.01613\n",
      "Training epoch: 2800, train loss: 0.01624, val loss: 0.01663\n",
      "Training epoch: 2801, train loss: 0.01566, val loss: 0.01605\n",
      "Training epoch: 2802, train loss: 0.01572, val loss: 0.01607\n",
      "Training epoch: 2803, train loss: 0.01573, val loss: 0.01610\n",
      "Training epoch: 2804, train loss: 0.01534, val loss: 0.01573\n",
      "Training epoch: 2805, train loss: 0.01540, val loss: 0.01573\n",
      "Training epoch: 2806, train loss: 0.01531, val loss: 0.01572\n",
      "Training epoch: 2807, train loss: 0.01541, val loss: 0.01582\n",
      "Training epoch: 2808, train loss: 0.01526, val loss: 0.01568\n",
      "Training epoch: 2809, train loss: 0.01561, val loss: 0.01596\n",
      "Training epoch: 2810, train loss: 0.01546, val loss: 0.01586\n",
      "Training epoch: 2811, train loss: 0.01537, val loss: 0.01579\n",
      "Training epoch: 2812, train loss: 0.01620, val loss: 0.01661\n",
      "Training epoch: 2813, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 2814, train loss: 0.01542, val loss: 0.01575\n",
      "Training epoch: 2815, train loss: 0.01557, val loss: 0.01598\n",
      "Training epoch: 2816, train loss: 0.01530, val loss: 0.01570\n",
      "Training epoch: 2817, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 2818, train loss: 0.01543, val loss: 0.01579\n",
      "Training epoch: 2819, train loss: 0.01539, val loss: 0.01578\n",
      "Training epoch: 2820, train loss: 0.01548, val loss: 0.01585\n",
      "Training epoch: 2821, train loss: 0.01554, val loss: 0.01593\n",
      "Training epoch: 2822, train loss: 0.01538, val loss: 0.01576\n",
      "Training epoch: 2823, train loss: 0.01587, val loss: 0.01626\n",
      "Training epoch: 2824, train loss: 0.01533, val loss: 0.01571\n",
      "Training epoch: 2825, train loss: 0.01581, val loss: 0.01622\n",
      "Training epoch: 2826, train loss: 0.01558, val loss: 0.01601\n",
      "Training epoch: 2827, train loss: 0.01595, val loss: 0.01628\n",
      "Training epoch: 2828, train loss: 0.01531, val loss: 0.01567\n",
      "Training epoch: 2829, train loss: 0.01532, val loss: 0.01571\n",
      "Training epoch: 2830, train loss: 0.01542, val loss: 0.01580\n",
      "Training epoch: 2831, train loss: 0.01542, val loss: 0.01583\n",
      "Training epoch: 2832, train loss: 0.01530, val loss: 0.01570\n",
      "Training epoch: 2833, train loss: 0.01547, val loss: 0.01587\n",
      "Training epoch: 2834, train loss: 0.01585, val loss: 0.01626\n",
      "Training epoch: 2835, train loss: 0.01597, val loss: 0.01636\n",
      "Training epoch: 2836, train loss: 0.01565, val loss: 0.01604\n",
      "Training epoch: 2837, train loss: 0.01563, val loss: 0.01600\n",
      "Training epoch: 2838, train loss: 0.01533, val loss: 0.01573\n",
      "Training epoch: 2839, train loss: 0.01535, val loss: 0.01575\n",
      "Training epoch: 2840, train loss: 0.01523, val loss: 0.01563\n",
      "Training epoch: 2841, train loss: 0.01553, val loss: 0.01595\n",
      "Training epoch: 2842, train loss: 0.01526, val loss: 0.01568\n",
      "Training epoch: 2843, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 2844, train loss: 0.01539, val loss: 0.01577\n",
      "Training epoch: 2845, train loss: 0.01555, val loss: 0.01590\n",
      "Training epoch: 2846, train loss: 0.01545, val loss: 0.01584\n",
      "Training epoch: 2847, train loss: 0.01593, val loss: 0.01635\n",
      "Training epoch: 2848, train loss: 0.01523, val loss: 0.01562\n",
      "Training epoch: 2849, train loss: 0.01557, val loss: 0.01592\n",
      "Training epoch: 2850, train loss: 0.01544, val loss: 0.01581\n",
      "Training epoch: 2851, train loss: 0.01557, val loss: 0.01597\n",
      "Training epoch: 2852, train loss: 0.01540, val loss: 0.01576\n",
      "Training epoch: 2853, train loss: 0.01526, val loss: 0.01566\n",
      "Training epoch: 2854, train loss: 0.01552, val loss: 0.01592\n",
      "Training epoch: 2855, train loss: 0.01550, val loss: 0.01586\n",
      "Training epoch: 2856, train loss: 0.01524, val loss: 0.01563\n",
      "Training epoch: 2857, train loss: 0.01552, val loss: 0.01591\n",
      "Training epoch: 2858, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 2859, train loss: 0.01538, val loss: 0.01578\n",
      "Training epoch: 2860, train loss: 0.01580, val loss: 0.01620\n",
      "Training epoch: 2861, train loss: 0.01533, val loss: 0.01571\n",
      "Training epoch: 2862, train loss: 0.01548, val loss: 0.01581\n",
      "Training epoch: 2863, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 2864, train loss: 0.01548, val loss: 0.01588\n",
      "Training epoch: 2865, train loss: 0.01528, val loss: 0.01565\n",
      "Training epoch: 2866, train loss: 0.01526, val loss: 0.01560\n",
      "Training epoch: 2867, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 2868, train loss: 0.01541, val loss: 0.01574\n",
      "Training epoch: 2869, train loss: 0.01533, val loss: 0.01572\n",
      "Training epoch: 2870, train loss: 0.01561, val loss: 0.01601\n",
      "Training epoch: 2871, train loss: 0.01536, val loss: 0.01573\n",
      "Training epoch: 2872, train loss: 0.01533, val loss: 0.01570\n",
      "Training epoch: 2873, train loss: 0.01584, val loss: 0.01621\n",
      "Training epoch: 2874, train loss: 0.01555, val loss: 0.01594\n",
      "Training epoch: 2875, train loss: 0.01542, val loss: 0.01578\n",
      "Training epoch: 2876, train loss: 0.01534, val loss: 0.01571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 2877, train loss: 0.01545, val loss: 0.01582\n",
      "Training epoch: 2878, train loss: 0.01531, val loss: 0.01568\n",
      "Training epoch: 2879, train loss: 0.01530, val loss: 0.01574\n",
      "Training epoch: 2880, train loss: 0.01526, val loss: 0.01567\n",
      "Training epoch: 2881, train loss: 0.01544, val loss: 0.01579\n",
      "Training epoch: 2882, train loss: 0.01572, val loss: 0.01611\n",
      "Training epoch: 2883, train loss: 0.01523, val loss: 0.01563\n",
      "Training epoch: 2884, train loss: 0.01536, val loss: 0.01571\n",
      "Training epoch: 2885, train loss: 0.01525, val loss: 0.01566\n",
      "Training epoch: 2886, train loss: 0.01542, val loss: 0.01576\n",
      "Training epoch: 2887, train loss: 0.01544, val loss: 0.01581\n",
      "Training epoch: 2888, train loss: 0.01544, val loss: 0.01586\n",
      "Training epoch: 2889, train loss: 0.01567, val loss: 0.01608\n",
      "Training epoch: 2890, train loss: 0.01530, val loss: 0.01568\n",
      "Training epoch: 2891, train loss: 0.01523, val loss: 0.01562\n",
      "Training epoch: 2892, train loss: 0.01545, val loss: 0.01586\n",
      "Training epoch: 2893, train loss: 0.01552, val loss: 0.01590\n",
      "Training epoch: 2894, train loss: 0.01528, val loss: 0.01567\n",
      "Training epoch: 2895, train loss: 0.01531, val loss: 0.01567\n",
      "Training epoch: 2896, train loss: 0.01551, val loss: 0.01589\n",
      "Training epoch: 2897, train loss: 0.01547, val loss: 0.01588\n",
      "Training epoch: 2898, train loss: 0.01553, val loss: 0.01586\n",
      "Training epoch: 2899, train loss: 0.01545, val loss: 0.01583\n",
      "Training epoch: 2900, train loss: 0.01546, val loss: 0.01585\n",
      "Training epoch: 2901, train loss: 0.01558, val loss: 0.01596\n",
      "Training epoch: 2902, train loss: 0.01537, val loss: 0.01577\n",
      "Training epoch: 2903, train loss: 0.01539, val loss: 0.01578\n",
      "Training epoch: 2904, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 2905, train loss: 0.01528, val loss: 0.01566\n",
      "Training epoch: 2906, train loss: 0.01528, val loss: 0.01565\n",
      "Training epoch: 2907, train loss: 0.01553, val loss: 0.01586\n",
      "Training epoch: 2908, train loss: 0.01548, val loss: 0.01583\n",
      "Training epoch: 2909, train loss: 0.01533, val loss: 0.01568\n",
      "Training epoch: 2910, train loss: 0.01531, val loss: 0.01568\n",
      "Training epoch: 2911, train loss: 0.01538, val loss: 0.01573\n",
      "Training epoch: 2912, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 2913, train loss: 0.01535, val loss: 0.01577\n",
      "Training epoch: 2914, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 2915, train loss: 0.01541, val loss: 0.01581\n",
      "Training epoch: 2916, train loss: 0.01555, val loss: 0.01593\n",
      "Training epoch: 2917, train loss: 0.01543, val loss: 0.01582\n",
      "Training epoch: 2918, train loss: 0.01523, val loss: 0.01562\n",
      "Training epoch: 2919, train loss: 0.01541, val loss: 0.01579\n",
      "Training epoch: 2920, train loss: 0.01531, val loss: 0.01569\n",
      "Training epoch: 2921, train loss: 0.01566, val loss: 0.01605\n",
      "Training epoch: 2922, train loss: 0.01533, val loss: 0.01573\n",
      "Training epoch: 2923, train loss: 0.01548, val loss: 0.01588\n",
      "Training epoch: 2924, train loss: 0.01536, val loss: 0.01574\n",
      "Training epoch: 2925, train loss: 0.01533, val loss: 0.01570\n",
      "Training epoch: 2926, train loss: 0.01557, val loss: 0.01594\n",
      "Training epoch: 2927, train loss: 0.01546, val loss: 0.01583\n",
      "Training epoch: 2928, train loss: 0.01530, val loss: 0.01567\n",
      "Training epoch: 2929, train loss: 0.01548, val loss: 0.01588\n",
      "Training epoch: 2930, train loss: 0.01536, val loss: 0.01574\n",
      "Training epoch: 2931, train loss: 0.01582, val loss: 0.01624\n",
      "Training epoch: 2932, train loss: 0.01550, val loss: 0.01588\n",
      "Training epoch: 2933, train loss: 0.01563, val loss: 0.01601\n",
      "Training epoch: 2934, train loss: 0.01549, val loss: 0.01589\n",
      "Training epoch: 2935, train loss: 0.01554, val loss: 0.01597\n",
      "Training epoch: 2936, train loss: 0.01602, val loss: 0.01640\n",
      "Training epoch: 2937, train loss: 0.01531, val loss: 0.01570\n",
      "Training epoch: 2938, train loss: 0.01549, val loss: 0.01587\n",
      "Training epoch: 2939, train loss: 0.01575, val loss: 0.01615\n",
      "Training epoch: 2940, train loss: 0.01526, val loss: 0.01568\n",
      "Training epoch: 2941, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 2942, train loss: 0.01523, val loss: 0.01557\n",
      "Training epoch: 2943, train loss: 0.01530, val loss: 0.01568\n",
      "Training epoch: 2944, train loss: 0.01564, val loss: 0.01600\n",
      "Training epoch: 2945, train loss: 0.01549, val loss: 0.01589\n",
      "Training epoch: 2946, train loss: 0.01544, val loss: 0.01588\n",
      "Training epoch: 2947, train loss: 0.01536, val loss: 0.01573\n",
      "Training epoch: 2948, train loss: 0.01528, val loss: 0.01565\n",
      "Training epoch: 2949, train loss: 0.01530, val loss: 0.01567\n",
      "Training epoch: 2950, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 2951, train loss: 0.01541, val loss: 0.01579\n",
      "Training epoch: 2952, train loss: 0.01575, val loss: 0.01616\n",
      "Training epoch: 2953, train loss: 0.01573, val loss: 0.01614\n",
      "Training epoch: 2954, train loss: 0.01544, val loss: 0.01581\n",
      "Training epoch: 2955, train loss: 0.01553, val loss: 0.01590\n",
      "Training epoch: 2956, train loss: 0.01595, val loss: 0.01635\n",
      "Training epoch: 2957, train loss: 0.01535, val loss: 0.01575\n",
      "Training epoch: 2958, train loss: 0.01524, val loss: 0.01563\n",
      "Training epoch: 2959, train loss: 0.01522, val loss: 0.01561\n",
      "Training epoch: 2960, train loss: 0.01531, val loss: 0.01572\n",
      "Training epoch: 2961, train loss: 0.01556, val loss: 0.01594\n",
      "Training epoch: 2962, train loss: 0.01540, val loss: 0.01583\n",
      "Training epoch: 2963, train loss: 0.01539, val loss: 0.01578\n",
      "Training epoch: 2964, train loss: 0.01533, val loss: 0.01569\n",
      "Training epoch: 2965, train loss: 0.01546, val loss: 0.01582\n",
      "Training epoch: 2966, train loss: 0.01522, val loss: 0.01564\n",
      "Training epoch: 2967, train loss: 0.01577, val loss: 0.01617\n",
      "Training epoch: 2968, train loss: 0.01539, val loss: 0.01573\n",
      "Training epoch: 2969, train loss: 0.01538, val loss: 0.01579\n",
      "Training epoch: 2970, train loss: 0.01549, val loss: 0.01582\n",
      "Training epoch: 2971, train loss: 0.01542, val loss: 0.01583\n",
      "Training epoch: 2972, train loss: 0.01547, val loss: 0.01589\n",
      "Training epoch: 2973, train loss: 0.01553, val loss: 0.01592\n",
      "Training epoch: 2974, train loss: 0.01541, val loss: 0.01581\n",
      "Training epoch: 2975, train loss: 0.01571, val loss: 0.01610\n",
      "Training epoch: 2976, train loss: 0.01528, val loss: 0.01565\n",
      "Training epoch: 2977, train loss: 0.01540, val loss: 0.01578\n",
      "Training epoch: 2978, train loss: 0.01538, val loss: 0.01577\n",
      "Training epoch: 2979, train loss: 0.01522, val loss: 0.01561\n",
      "Training epoch: 2980, train loss: 0.01530, val loss: 0.01567\n",
      "Training epoch: 2981, train loss: 0.01589, val loss: 0.01624\n",
      "Training epoch: 2982, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 2983, train loss: 0.01547, val loss: 0.01586\n",
      "Training epoch: 2984, train loss: 0.01570, val loss: 0.01610\n",
      "Training epoch: 2985, train loss: 0.01569, val loss: 0.01610\n",
      "Training epoch: 2986, train loss: 0.01527, val loss: 0.01564\n",
      "Training epoch: 2987, train loss: 0.01537, val loss: 0.01573\n",
      "Training epoch: 2988, train loss: 0.01538, val loss: 0.01577\n",
      "Training epoch: 2989, train loss: 0.01533, val loss: 0.01571\n",
      "Training epoch: 2990, train loss: 0.01537, val loss: 0.01577\n",
      "Training epoch: 2991, train loss: 0.01529, val loss: 0.01569\n",
      "Training epoch: 2992, train loss: 0.01536, val loss: 0.01578\n",
      "Training epoch: 2993, train loss: 0.01553, val loss: 0.01589\n",
      "Training epoch: 2994, train loss: 0.01570, val loss: 0.01608\n",
      "Training epoch: 2995, train loss: 0.01546, val loss: 0.01585\n",
      "Training epoch: 2996, train loss: 0.01547, val loss: 0.01586\n",
      "Training epoch: 2997, train loss: 0.01537, val loss: 0.01577\n",
      "Training epoch: 2998, train loss: 0.01553, val loss: 0.01594\n",
      "Training epoch: 2999, train loss: 0.01533, val loss: 0.01571\n",
      "Training epoch: 3000, train loss: 0.01533, val loss: 0.01572\n",
      "Training epoch: 3001, train loss: 0.01543, val loss: 0.01579\n",
      "Training epoch: 3002, train loss: 0.01562, val loss: 0.01595\n",
      "Training epoch: 3003, train loss: 0.01523, val loss: 0.01564\n",
      "Training epoch: 3004, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3005, train loss: 0.01540, val loss: 0.01578\n",
      "Training epoch: 3006, train loss: 0.01553, val loss: 0.01587\n",
      "Training epoch: 3007, train loss: 0.01536, val loss: 0.01570\n",
      "Training epoch: 3008, train loss: 0.01544, val loss: 0.01583\n",
      "Training epoch: 3009, train loss: 0.01539, val loss: 0.01575\n",
      "Training epoch: 3010, train loss: 0.01558, val loss: 0.01596\n",
      "Training epoch: 3011, train loss: 0.01545, val loss: 0.01582\n",
      "Training epoch: 3012, train loss: 0.01543, val loss: 0.01578\n",
      "Training epoch: 3013, train loss: 0.01541, val loss: 0.01582\n",
      "Training epoch: 3014, train loss: 0.01545, val loss: 0.01582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 3015, train loss: 0.01575, val loss: 0.01610\n",
      "Training epoch: 3016, train loss: 0.01535, val loss: 0.01570\n",
      "Training epoch: 3017, train loss: 0.01545, val loss: 0.01585\n",
      "Training epoch: 3018, train loss: 0.01553, val loss: 0.01592\n",
      "Training epoch: 3019, train loss: 0.01527, val loss: 0.01566\n",
      "Training epoch: 3020, train loss: 0.01609, val loss: 0.01649\n",
      "Training epoch: 3021, train loss: 0.01577, val loss: 0.01616\n",
      "Training epoch: 3022, train loss: 0.01547, val loss: 0.01585\n",
      "Training epoch: 3023, train loss: 0.01527, val loss: 0.01566\n",
      "Training epoch: 3024, train loss: 0.01534, val loss: 0.01572\n",
      "Training epoch: 3025, train loss: 0.01549, val loss: 0.01589\n",
      "Training epoch: 3026, train loss: 0.01530, val loss: 0.01570\n",
      "Training epoch: 3027, train loss: 0.01591, val loss: 0.01634\n",
      "Training epoch: 3028, train loss: 0.01532, val loss: 0.01569\n",
      "Training epoch: 3029, train loss: 0.01549, val loss: 0.01587\n",
      "Training epoch: 3030, train loss: 0.01559, val loss: 0.01594\n",
      "Training epoch: 3031, train loss: 0.01536, val loss: 0.01577\n",
      "Training epoch: 3032, train loss: 0.01526, val loss: 0.01559\n",
      "Training epoch: 3033, train loss: 0.01601, val loss: 0.01640\n",
      "Training epoch: 3034, train loss: 0.01533, val loss: 0.01572\n",
      "Training epoch: 3035, train loss: 0.01552, val loss: 0.01589\n",
      "Training epoch: 3036, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 3037, train loss: 0.01544, val loss: 0.01584\n",
      "Training epoch: 3038, train loss: 0.01553, val loss: 0.01591\n",
      "Training epoch: 3039, train loss: 0.01523, val loss: 0.01564\n",
      "Training epoch: 3040, train loss: 0.01549, val loss: 0.01590\n",
      "Training epoch: 3041, train loss: 0.01565, val loss: 0.01605\n",
      "Training epoch: 3042, train loss: 0.01568, val loss: 0.01611\n",
      "Training epoch: 3043, train loss: 0.01577, val loss: 0.01619\n",
      "Training epoch: 3044, train loss: 0.01539, val loss: 0.01580\n",
      "Training epoch: 3045, train loss: 0.01544, val loss: 0.01584\n",
      "Training epoch: 3046, train loss: 0.01541, val loss: 0.01578\n",
      "Training epoch: 3047, train loss: 0.01526, val loss: 0.01562\n",
      "Training epoch: 3048, train loss: 0.01532, val loss: 0.01569\n",
      "Training epoch: 3049, train loss: 0.01531, val loss: 0.01569\n",
      "Training epoch: 3050, train loss: 0.01528, val loss: 0.01566\n",
      "Training epoch: 3051, train loss: 0.01530, val loss: 0.01570\n",
      "Training epoch: 3052, train loss: 0.01525, val loss: 0.01559\n",
      "Training epoch: 3053, train loss: 0.01534, val loss: 0.01572\n",
      "Training epoch: 3054, train loss: 0.01554, val loss: 0.01594\n",
      "Training epoch: 3055, train loss: 0.01565, val loss: 0.01604\n",
      "Training epoch: 3056, train loss: 0.01572, val loss: 0.01613\n",
      "Training epoch: 3057, train loss: 0.01539, val loss: 0.01577\n",
      "Training epoch: 3058, train loss: 0.01585, val loss: 0.01621\n",
      "Training epoch: 3059, train loss: 0.01630, val loss: 0.01669\n",
      "Training epoch: 3060, train loss: 0.01568, val loss: 0.01607\n",
      "Training epoch: 3061, train loss: 0.01555, val loss: 0.01587\n",
      "Training epoch: 3062, train loss: 0.01538, val loss: 0.01574\n",
      "Training epoch: 3063, train loss: 0.01546, val loss: 0.01584\n",
      "Training epoch: 3064, train loss: 0.01582, val loss: 0.01626\n",
      "Training epoch: 3065, train loss: 0.01570, val loss: 0.01611\n",
      "Training epoch: 3066, train loss: 0.01560, val loss: 0.01598\n",
      "Training epoch: 3067, train loss: 0.01537, val loss: 0.01571\n",
      "Training epoch: 3068, train loss: 0.01531, val loss: 0.01567\n",
      "Training epoch: 3069, train loss: 0.01555, val loss: 0.01593\n",
      "Training epoch: 3070, train loss: 0.01529, val loss: 0.01566\n",
      "Training epoch: 3071, train loss: 0.01539, val loss: 0.01574\n",
      "Training epoch: 3072, train loss: 0.01599, val loss: 0.01638\n",
      "Training epoch: 3073, train loss: 0.01535, val loss: 0.01575\n",
      "Training epoch: 3074, train loss: 0.01579, val loss: 0.01617\n",
      "Training epoch: 3075, train loss: 0.01535, val loss: 0.01574\n",
      "Training epoch: 3076, train loss: 0.01552, val loss: 0.01589\n",
      "Training epoch: 3077, train loss: 0.01549, val loss: 0.01592\n",
      "Training epoch: 3078, train loss: 0.01550, val loss: 0.01584\n",
      "Training epoch: 3079, train loss: 0.01556, val loss: 0.01593\n",
      "Training epoch: 3080, train loss: 0.01553, val loss: 0.01593\n",
      "Training epoch: 3081, train loss: 0.01544, val loss: 0.01581\n",
      "Training epoch: 3082, train loss: 0.01533, val loss: 0.01570\n",
      "Training epoch: 3083, train loss: 0.01579, val loss: 0.01618\n",
      "Training epoch: 3084, train loss: 0.01530, val loss: 0.01569\n",
      "Training epoch: 3085, train loss: 0.01537, val loss: 0.01573\n",
      "Training epoch: 3086, train loss: 0.01538, val loss: 0.01573\n",
      "Training epoch: 3087, train loss: 0.01536, val loss: 0.01574\n",
      "Training epoch: 3088, train loss: 0.01535, val loss: 0.01578\n",
      "Training epoch: 3089, train loss: 0.01533, val loss: 0.01570\n",
      "Training epoch: 3090, train loss: 0.01571, val loss: 0.01609\n",
      "Training epoch: 3091, train loss: 0.01548, val loss: 0.01582\n",
      "Training epoch: 3092, train loss: 0.01528, val loss: 0.01570\n",
      "Training epoch: 3093, train loss: 0.01538, val loss: 0.01575\n",
      "Training epoch: 3094, train loss: 0.01543, val loss: 0.01578\n",
      "Training epoch: 3095, train loss: 0.01533, val loss: 0.01571\n",
      "Training epoch: 3096, train loss: 0.01560, val loss: 0.01600\n",
      "Training epoch: 3097, train loss: 0.01532, val loss: 0.01571\n",
      "Training epoch: 3098, train loss: 0.01541, val loss: 0.01576\n",
      "Training epoch: 3099, train loss: 0.01551, val loss: 0.01587\n",
      "Training epoch: 3100, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3101, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 3102, train loss: 0.01526, val loss: 0.01565\n",
      "Training epoch: 3103, train loss: 0.01542, val loss: 0.01581\n",
      "Training epoch: 3104, train loss: 0.01527, val loss: 0.01562\n",
      "Training epoch: 3105, train loss: 0.01533, val loss: 0.01571\n",
      "Training epoch: 3106, train loss: 0.01543, val loss: 0.01584\n",
      "Training epoch: 3107, train loss: 0.01542, val loss: 0.01572\n",
      "Training epoch: 3108, train loss: 0.01565, val loss: 0.01601\n",
      "Training epoch: 3109, train loss: 0.01526, val loss: 0.01567\n",
      "Training epoch: 3110, train loss: 0.01553, val loss: 0.01595\n",
      "Training epoch: 3111, train loss: 0.01532, val loss: 0.01568\n",
      "Training epoch: 3112, train loss: 0.01536, val loss: 0.01577\n",
      "Training epoch: 3113, train loss: 0.01549, val loss: 0.01587\n",
      "Training epoch: 3114, train loss: 0.01563, val loss: 0.01602\n",
      "Training epoch: 3115, train loss: 0.01528, val loss: 0.01569\n",
      "Training epoch: 3116, train loss: 0.01554, val loss: 0.01597\n",
      "Training epoch: 3117, train loss: 0.01540, val loss: 0.01578\n",
      "Training epoch: 3118, train loss: 0.01537, val loss: 0.01577\n",
      "Training epoch: 3119, train loss: 0.01543, val loss: 0.01581\n",
      "Training epoch: 3120, train loss: 0.01585, val loss: 0.01623\n",
      "Training epoch: 3121, train loss: 0.01541, val loss: 0.01578\n",
      "Training epoch: 3122, train loss: 0.01542, val loss: 0.01577\n",
      "Training epoch: 3123, train loss: 0.01547, val loss: 0.01585\n",
      "Training epoch: 3124, train loss: 0.01531, val loss: 0.01568\n",
      "Training epoch: 3125, train loss: 0.01553, val loss: 0.01590\n",
      "Training epoch: 3126, train loss: 0.01552, val loss: 0.01590\n",
      "Training epoch: 3127, train loss: 0.01558, val loss: 0.01600\n",
      "Training epoch: 3128, train loss: 0.01555, val loss: 0.01593\n",
      "Training epoch: 3129, train loss: 0.01545, val loss: 0.01587\n",
      "Training epoch: 3130, train loss: 0.01575, val loss: 0.01616\n",
      "Training epoch: 3131, train loss: 0.01577, val loss: 0.01618\n",
      "Training epoch: 3132, train loss: 0.01564, val loss: 0.01600\n",
      "Training epoch: 3133, train loss: 0.01531, val loss: 0.01569\n",
      "Training epoch: 3134, train loss: 0.01521, val loss: 0.01556\n",
      "Training epoch: 3135, train loss: 0.01538, val loss: 0.01577\n",
      "Training epoch: 3136, train loss: 0.01523, val loss: 0.01565\n",
      "Training epoch: 3137, train loss: 0.01533, val loss: 0.01571\n",
      "Training epoch: 3138, train loss: 0.01554, val loss: 0.01592\n",
      "Training epoch: 3139, train loss: 0.01550, val loss: 0.01590\n",
      "Training epoch: 3140, train loss: 0.01577, val loss: 0.01615\n",
      "Training epoch: 3141, train loss: 0.01550, val loss: 0.01588\n",
      "Training epoch: 3142, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 3143, train loss: 0.01543, val loss: 0.01583\n",
      "Training epoch: 3144, train loss: 0.01573, val loss: 0.01613\n",
      "Training epoch: 3145, train loss: 0.01537, val loss: 0.01571\n",
      "Training epoch: 3146, train loss: 0.01577, val loss: 0.01617\n",
      "Training epoch: 3147, train loss: 0.01609, val loss: 0.01650\n",
      "Training epoch: 3148, train loss: 0.01532, val loss: 0.01567\n",
      "Training epoch: 3149, train loss: 0.01546, val loss: 0.01580\n",
      "Training epoch: 3150, train loss: 0.01523, val loss: 0.01560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 3151, train loss: 0.01528, val loss: 0.01565\n",
      "Training epoch: 3152, train loss: 0.01568, val loss: 0.01607\n",
      "Training epoch: 3153, train loss: 0.01538, val loss: 0.01579\n",
      "Training epoch: 3154, train loss: 0.01557, val loss: 0.01592\n",
      "Training epoch: 3155, train loss: 0.01537, val loss: 0.01574\n",
      "Training epoch: 3156, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3157, train loss: 0.01548, val loss: 0.01587\n",
      "Training epoch: 3158, train loss: 0.01552, val loss: 0.01592\n",
      "Training epoch: 3159, train loss: 0.01553, val loss: 0.01592\n",
      "Training epoch: 3160, train loss: 0.01570, val loss: 0.01615\n",
      "Training epoch: 3161, train loss: 0.01548, val loss: 0.01586\n",
      "Training epoch: 3162, train loss: 0.01541, val loss: 0.01580\n",
      "Training epoch: 3163, train loss: 0.01546, val loss: 0.01585\n",
      "Training epoch: 3164, train loss: 0.01535, val loss: 0.01575\n",
      "Training epoch: 3165, train loss: 0.01542, val loss: 0.01581\n",
      "Training epoch: 3166, train loss: 0.01537, val loss: 0.01571\n",
      "Training epoch: 3167, train loss: 0.01548, val loss: 0.01585\n",
      "Training epoch: 3168, train loss: 0.01529, val loss: 0.01567\n",
      "Training epoch: 3169, train loss: 0.01545, val loss: 0.01580\n",
      "Training epoch: 3170, train loss: 0.01541, val loss: 0.01582\n",
      "Training epoch: 3171, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 3172, train loss: 0.01559, val loss: 0.01595\n",
      "Training epoch: 3173, train loss: 0.01572, val loss: 0.01606\n",
      "Training epoch: 3174, train loss: 0.01577, val loss: 0.01614\n",
      "Training epoch: 3175, train loss: 0.01554, val loss: 0.01589\n",
      "Training epoch: 3176, train loss: 0.01528, val loss: 0.01568\n",
      "Training epoch: 3177, train loss: 0.01529, val loss: 0.01565\n",
      "Training epoch: 3178, train loss: 0.01524, val loss: 0.01565\n",
      "Training epoch: 3179, train loss: 0.01528, val loss: 0.01568\n",
      "Training epoch: 3180, train loss: 0.01537, val loss: 0.01574\n",
      "Training epoch: 3181, train loss: 0.01533, val loss: 0.01574\n",
      "Training epoch: 3182, train loss: 0.01553, val loss: 0.01591\n",
      "Training epoch: 3183, train loss: 0.01548, val loss: 0.01586\n",
      "Training epoch: 3184, train loss: 0.01536, val loss: 0.01574\n",
      "Training epoch: 3185, train loss: 0.01531, val loss: 0.01570\n",
      "Training epoch: 3186, train loss: 0.01529, val loss: 0.01567\n",
      "Training epoch: 3187, train loss: 0.01531, val loss: 0.01573\n",
      "Training epoch: 3188, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 3189, train loss: 0.01539, val loss: 0.01575\n",
      "Training epoch: 3190, train loss: 0.01531, val loss: 0.01570\n",
      "Training epoch: 3191, train loss: 0.01547, val loss: 0.01588\n",
      "Training epoch: 3192, train loss: 0.01585, val loss: 0.01623\n",
      "Training epoch: 3193, train loss: 0.01543, val loss: 0.01578\n",
      "Training epoch: 3194, train loss: 0.01549, val loss: 0.01586\n",
      "Training epoch: 3195, train loss: 0.01529, val loss: 0.01571\n",
      "Training epoch: 3196, train loss: 0.01530, val loss: 0.01568\n",
      "Training epoch: 3197, train loss: 0.01537, val loss: 0.01574\n",
      "Training epoch: 3198, train loss: 0.01622, val loss: 0.01661\n",
      "Training epoch: 3199, train loss: 0.01575, val loss: 0.01615\n",
      "Training epoch: 3200, train loss: 0.01547, val loss: 0.01586\n",
      "Training epoch: 3201, train loss: 0.01533, val loss: 0.01570\n",
      "Training epoch: 3202, train loss: 0.01536, val loss: 0.01573\n",
      "Training epoch: 3203, train loss: 0.01536, val loss: 0.01571\n",
      "Training epoch: 3204, train loss: 0.01528, val loss: 0.01565\n",
      "Training epoch: 3205, train loss: 0.01671, val loss: 0.01714\n",
      "Training epoch: 3206, train loss: 0.01531, val loss: 0.01567\n",
      "Training epoch: 3207, train loss: 0.01546, val loss: 0.01583\n",
      "Training epoch: 3208, train loss: 0.01533, val loss: 0.01567\n",
      "Training epoch: 3209, train loss: 0.01543, val loss: 0.01580\n",
      "Training epoch: 3210, train loss: 0.01589, val loss: 0.01620\n",
      "Training epoch: 3211, train loss: 0.01559, val loss: 0.01597\n",
      "Training epoch: 3212, train loss: 0.01552, val loss: 0.01591\n",
      "Training epoch: 3213, train loss: 0.01553, val loss: 0.01588\n",
      "Training epoch: 3214, train loss: 0.01542, val loss: 0.01580\n",
      "Training epoch: 3215, train loss: 0.01530, val loss: 0.01570\n",
      "Training epoch: 3216, train loss: 0.01575, val loss: 0.01613\n",
      "Training epoch: 3217, train loss: 0.01535, val loss: 0.01571\n",
      "Training epoch: 3218, train loss: 0.01538, val loss: 0.01576\n",
      "Training epoch: 3219, train loss: 0.01534, val loss: 0.01573\n",
      "Training epoch: 3220, train loss: 0.01544, val loss: 0.01584\n",
      "Training epoch: 3221, train loss: 0.01526, val loss: 0.01566\n",
      "Training epoch: 3222, train loss: 0.01542, val loss: 0.01580\n",
      "Training epoch: 3223, train loss: 0.01531, val loss: 0.01567\n",
      "Training epoch: 3224, train loss: 0.01563, val loss: 0.01602\n",
      "Training epoch: 3225, train loss: 0.01602, val loss: 0.01641\n",
      "Training epoch: 3226, train loss: 0.01528, val loss: 0.01566\n",
      "Training epoch: 3227, train loss: 0.01542, val loss: 0.01577\n",
      "Training epoch: 3228, train loss: 0.01539, val loss: 0.01575\n",
      "Training epoch: 3229, train loss: 0.01545, val loss: 0.01586\n",
      "Training epoch: 3230, train loss: 0.01548, val loss: 0.01586\n",
      "Training epoch: 3231, train loss: 0.01549, val loss: 0.01589\n",
      "Training epoch: 3232, train loss: 0.01550, val loss: 0.01590\n",
      "Training epoch: 3233, train loss: 0.01559, val loss: 0.01596\n",
      "Training epoch: 3234, train loss: 0.01534, val loss: 0.01575\n",
      "Training epoch: 3235, train loss: 0.01530, val loss: 0.01567\n",
      "Training epoch: 3236, train loss: 0.01528, val loss: 0.01566\n",
      "Training epoch: 3237, train loss: 0.01591, val loss: 0.01624\n",
      "Training epoch: 3238, train loss: 0.01525, val loss: 0.01561\n",
      "Training epoch: 3239, train loss: 0.01530, val loss: 0.01565\n",
      "Training epoch: 3240, train loss: 0.01531, val loss: 0.01568\n",
      "Training epoch: 3241, train loss: 0.01531, val loss: 0.01571\n",
      "Training epoch: 3242, train loss: 0.01523, val loss: 0.01564\n",
      "Training epoch: 3243, train loss: 0.01533, val loss: 0.01568\n",
      "Training epoch: 3244, train loss: 0.01595, val loss: 0.01633\n",
      "Training epoch: 3245, train loss: 0.01580, val loss: 0.01619\n",
      "Training epoch: 3246, train loss: 0.01572, val loss: 0.01612\n",
      "Training epoch: 3247, train loss: 0.01538, val loss: 0.01573\n",
      "Training epoch: 3248, train loss: 0.01555, val loss: 0.01595\n",
      "Training epoch: 3249, train loss: 0.01555, val loss: 0.01596\n",
      "Training epoch: 3250, train loss: 0.01562, val loss: 0.01599\n",
      "Training epoch: 3251, train loss: 0.01540, val loss: 0.01580\n",
      "Training epoch: 3252, train loss: 0.01536, val loss: 0.01573\n",
      "Training epoch: 3253, train loss: 0.01588, val loss: 0.01628\n",
      "Training epoch: 3254, train loss: 0.01565, val loss: 0.01600\n",
      "Training epoch: 3255, train loss: 0.01601, val loss: 0.01639\n",
      "Training epoch: 3256, train loss: 0.01556, val loss: 0.01590\n",
      "Training epoch: 3257, train loss: 0.01592, val loss: 0.01629\n",
      "Training epoch: 3258, train loss: 0.01543, val loss: 0.01584\n",
      "Training epoch: 3259, train loss: 0.01542, val loss: 0.01581\n",
      "Training epoch: 3260, train loss: 0.01548, val loss: 0.01582\n",
      "Training epoch: 3261, train loss: 0.01552, val loss: 0.01587\n",
      "Training epoch: 3262, train loss: 0.01555, val loss: 0.01593\n",
      "Training epoch: 3263, train loss: 0.01547, val loss: 0.01586\n",
      "Training epoch: 3264, train loss: 0.01528, val loss: 0.01566\n",
      "Training epoch: 3265, train loss: 0.01551, val loss: 0.01592\n",
      "Training epoch: 3266, train loss: 0.01553, val loss: 0.01588\n",
      "Training epoch: 3267, train loss: 0.01528, val loss: 0.01565\n",
      "Training epoch: 3268, train loss: 0.01532, val loss: 0.01571\n",
      "Training epoch: 3269, train loss: 0.01535, val loss: 0.01571\n",
      "Training epoch: 3270, train loss: 0.01539, val loss: 0.01577\n",
      "Training epoch: 3271, train loss: 0.01603, val loss: 0.01636\n",
      "Training epoch: 3272, train loss: 0.01551, val loss: 0.01587\n",
      "Training epoch: 3273, train loss: 0.01547, val loss: 0.01581\n",
      "Training epoch: 3274, train loss: 0.01617, val loss: 0.01658\n",
      "Training epoch: 3275, train loss: 0.01537, val loss: 0.01575\n",
      "Training epoch: 3276, train loss: 0.01526, val loss: 0.01562\n",
      "Training epoch: 3277, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 3278, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3279, train loss: 0.01530, val loss: 0.01569\n",
      "Training epoch: 3280, train loss: 0.01553, val loss: 0.01593\n",
      "Training epoch: 3281, train loss: 0.01584, val loss: 0.01625\n",
      "Training epoch: 3282, train loss: 0.01567, val loss: 0.01605\n",
      "Training epoch: 3283, train loss: 0.01538, val loss: 0.01575\n",
      "Training epoch: 3284, train loss: 0.01530, val loss: 0.01565\n",
      "Training epoch: 3285, train loss: 0.01553, val loss: 0.01587\n",
      "Training epoch: 3286, train loss: 0.01558, val loss: 0.01597\n",
      "Training epoch: 3287, train loss: 0.01561, val loss: 0.01594\n",
      "Training epoch: 3288, train loss: 0.01556, val loss: 0.01595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 3289, train loss: 0.01548, val loss: 0.01587\n",
      "Training epoch: 3290, train loss: 0.01564, val loss: 0.01601\n",
      "Training epoch: 3291, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3292, train loss: 0.01529, val loss: 0.01568\n",
      "Training epoch: 3293, train loss: 0.01556, val loss: 0.01598\n",
      "Training epoch: 3294, train loss: 0.01599, val loss: 0.01636\n",
      "Training epoch: 3295, train loss: 0.01594, val loss: 0.01634\n",
      "Training epoch: 3296, train loss: 0.01560, val loss: 0.01603\n",
      "Training epoch: 3297, train loss: 0.01572, val loss: 0.01613\n",
      "Training epoch: 3298, train loss: 0.01567, val loss: 0.01605\n",
      "Training epoch: 3299, train loss: 0.01570, val loss: 0.01612\n",
      "Training epoch: 3300, train loss: 0.01526, val loss: 0.01561\n",
      "Training epoch: 3301, train loss: 0.01532, val loss: 0.01565\n",
      "Training epoch: 3302, train loss: 0.01547, val loss: 0.01588\n",
      "Training epoch: 3303, train loss: 0.01558, val loss: 0.01593\n",
      "Training epoch: 3304, train loss: 0.01526, val loss: 0.01561\n",
      "Training epoch: 3305, train loss: 0.01542, val loss: 0.01578\n",
      "Training epoch: 3306, train loss: 0.01606, val loss: 0.01641\n",
      "Training epoch: 3307, train loss: 0.01529, val loss: 0.01564\n",
      "Training epoch: 3308, train loss: 0.01539, val loss: 0.01579\n",
      "Training epoch: 3309, train loss: 0.01528, val loss: 0.01567\n",
      "Training epoch: 3310, train loss: 0.01550, val loss: 0.01587\n",
      "Training epoch: 3311, train loss: 0.01528, val loss: 0.01563\n",
      "Training epoch: 3312, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3313, train loss: 0.01564, val loss: 0.01593\n",
      "Training epoch: 3314, train loss: 0.01532, val loss: 0.01567\n",
      "Training epoch: 3315, train loss: 0.01556, val loss: 0.01592\n",
      "Training epoch: 3316, train loss: 0.01549, val loss: 0.01589\n",
      "Training epoch: 3317, train loss: 0.01533, val loss: 0.01575\n",
      "Training epoch: 3318, train loss: 0.01534, val loss: 0.01571\n",
      "Training epoch: 3319, train loss: 0.01531, val loss: 0.01568\n",
      "Training epoch: 3320, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 3321, train loss: 0.01525, val loss: 0.01561\n",
      "Training epoch: 3322, train loss: 0.01550, val loss: 0.01588\n",
      "Training epoch: 3323, train loss: 0.01574, val loss: 0.01616\n",
      "Training epoch: 3324, train loss: 0.01526, val loss: 0.01569\n",
      "Training epoch: 3325, train loss: 0.01531, val loss: 0.01569\n",
      "Training epoch: 3326, train loss: 0.01534, val loss: 0.01573\n",
      "Training epoch: 3327, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 3328, train loss: 0.01577, val loss: 0.01613\n",
      "Training epoch: 3329, train loss: 0.01537, val loss: 0.01576\n",
      "Training epoch: 3330, train loss: 0.01532, val loss: 0.01571\n",
      "Training epoch: 3331, train loss: 0.01537, val loss: 0.01576\n",
      "Training epoch: 3332, train loss: 0.01545, val loss: 0.01583\n",
      "Training epoch: 3333, train loss: 0.01542, val loss: 0.01584\n",
      "Training epoch: 3334, train loss: 0.01541, val loss: 0.01582\n",
      "Training epoch: 3335, train loss: 0.01533, val loss: 0.01574\n",
      "Training epoch: 3336, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3337, train loss: 0.01530, val loss: 0.01566\n",
      "Training epoch: 3338, train loss: 0.01539, val loss: 0.01576\n",
      "Training epoch: 3339, train loss: 0.01538, val loss: 0.01571\n",
      "Training epoch: 3340, train loss: 0.01535, val loss: 0.01575\n",
      "Training epoch: 3341, train loss: 0.01526, val loss: 0.01565\n",
      "Training epoch: 3342, train loss: 0.01524, val loss: 0.01565\n",
      "Training epoch: 3343, train loss: 0.01526, val loss: 0.01565\n",
      "Training epoch: 3344, train loss: 0.01543, val loss: 0.01581\n",
      "Training epoch: 3345, train loss: 0.01558, val loss: 0.01597\n",
      "Training epoch: 3346, train loss: 0.01531, val loss: 0.01570\n",
      "Training epoch: 3347, train loss: 0.01538, val loss: 0.01577\n",
      "Training epoch: 3348, train loss: 0.01526, val loss: 0.01565\n",
      "Training epoch: 3349, train loss: 0.01541, val loss: 0.01579\n",
      "Training epoch: 3350, train loss: 0.01548, val loss: 0.01589\n",
      "Training epoch: 3351, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 3352, train loss: 0.01531, val loss: 0.01573\n",
      "Training epoch: 3353, train loss: 0.01552, val loss: 0.01593\n",
      "Training epoch: 3354, train loss: 0.01568, val loss: 0.01607\n",
      "Training epoch: 3355, train loss: 0.01529, val loss: 0.01565\n",
      "Training epoch: 3356, train loss: 0.01530, val loss: 0.01568\n",
      "Training epoch: 3357, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 3358, train loss: 0.01535, val loss: 0.01568\n",
      "Training epoch: 3359, train loss: 0.01539, val loss: 0.01580\n",
      "Training epoch: 3360, train loss: 0.01531, val loss: 0.01573\n",
      "Training epoch: 3361, train loss: 0.01552, val loss: 0.01590\n",
      "Training epoch: 3362, train loss: 0.01532, val loss: 0.01573\n",
      "Training epoch: 3363, train loss: 0.01524, val loss: 0.01563\n",
      "Training epoch: 3364, train loss: 0.01571, val loss: 0.01610\n",
      "Training epoch: 3365, train loss: 0.01536, val loss: 0.01571\n",
      "Training epoch: 3366, train loss: 0.01601, val loss: 0.01636\n",
      "Training epoch: 3367, train loss: 0.01541, val loss: 0.01575\n",
      "Training epoch: 3368, train loss: 0.01535, val loss: 0.01572\n",
      "Training epoch: 3369, train loss: 0.01609, val loss: 0.01652\n",
      "Training epoch: 3370, train loss: 0.01526, val loss: 0.01566\n",
      "Training epoch: 3371, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 3372, train loss: 0.01524, val loss: 0.01563\n",
      "Training epoch: 3373, train loss: 0.01550, val loss: 0.01588\n",
      "Training epoch: 3374, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 3375, train loss: 0.01535, val loss: 0.01574\n",
      "Training epoch: 3376, train loss: 0.01560, val loss: 0.01603\n",
      "Training epoch: 3377, train loss: 0.01589, val loss: 0.01631\n",
      "Training epoch: 3378, train loss: 0.01610, val loss: 0.01652\n",
      "Training epoch: 3379, train loss: 0.01559, val loss: 0.01597\n",
      "Training epoch: 3380, train loss: 0.01539, val loss: 0.01575\n",
      "Training epoch: 3381, train loss: 0.01603, val loss: 0.01641\n",
      "Training epoch: 3382, train loss: 0.01559, val loss: 0.01596\n",
      "Training epoch: 3383, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 3384, train loss: 0.01529, val loss: 0.01564\n",
      "Training epoch: 3385, train loss: 0.01582, val loss: 0.01614\n",
      "Training epoch: 3386, train loss: 0.01533, val loss: 0.01573\n",
      "Training epoch: 3387, train loss: 0.01527, val loss: 0.01564\n",
      "Training epoch: 3388, train loss: 0.01535, val loss: 0.01575\n",
      "Training epoch: 3389, train loss: 0.01540, val loss: 0.01576\n",
      "Training epoch: 3390, train loss: 0.01536, val loss: 0.01570\n",
      "Training epoch: 3391, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3392, train loss: 0.01530, val loss: 0.01568\n",
      "Training epoch: 3393, train loss: 0.01537, val loss: 0.01575\n",
      "Training epoch: 3394, train loss: 0.01527, val loss: 0.01567\n",
      "Training epoch: 3395, train loss: 0.01524, val loss: 0.01563\n",
      "Training epoch: 3396, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3397, train loss: 0.01531, val loss: 0.01569\n",
      "Training epoch: 3398, train loss: 0.01527, val loss: 0.01566\n",
      "Training epoch: 3399, train loss: 0.01534, val loss: 0.01568\n",
      "Training epoch: 3400, train loss: 0.01535, val loss: 0.01572\n",
      "Training epoch: 3401, train loss: 0.01573, val loss: 0.01611\n",
      "Training epoch: 3402, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 3403, train loss: 0.01529, val loss: 0.01566\n",
      "Training epoch: 3404, train loss: 0.01522, val loss: 0.01562\n",
      "Training epoch: 3405, train loss: 0.01539, val loss: 0.01575\n",
      "Training epoch: 3406, train loss: 0.01535, val loss: 0.01574\n",
      "Training epoch: 3407, train loss: 0.01546, val loss: 0.01583\n",
      "Training epoch: 3408, train loss: 0.01540, val loss: 0.01581\n",
      "Training epoch: 3409, train loss: 0.01538, val loss: 0.01577\n",
      "Training epoch: 3410, train loss: 0.01540, val loss: 0.01579\n",
      "Training epoch: 3411, train loss: 0.01528, val loss: 0.01564\n",
      "Training epoch: 3412, train loss: 0.01540, val loss: 0.01578\n",
      "Training epoch: 3413, train loss: 0.01532, val loss: 0.01573\n",
      "Training epoch: 3414, train loss: 0.01532, val loss: 0.01569\n",
      "Training epoch: 3415, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3416, train loss: 0.01540, val loss: 0.01578\n",
      "Training epoch: 3417, train loss: 0.01529, val loss: 0.01567\n",
      "Training epoch: 3418, train loss: 0.01548, val loss: 0.01586\n",
      "Training epoch: 3419, train loss: 0.01553, val loss: 0.01592\n",
      "Training epoch: 3420, train loss: 0.01558, val loss: 0.01595\n",
      "Training epoch: 3421, train loss: 0.01532, val loss: 0.01569\n",
      "Training epoch: 3422, train loss: 0.01587, val loss: 0.01626\n",
      "Training epoch: 3423, train loss: 0.01537, val loss: 0.01577\n",
      "Training epoch: 3424, train loss: 0.01532, val loss: 0.01570\n",
      "Training epoch: 3425, train loss: 0.01526, val loss: 0.01560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 3426, train loss: 0.01531, val loss: 0.01572\n",
      "Training epoch: 3427, train loss: 0.01531, val loss: 0.01570\n",
      "Training epoch: 3428, train loss: 0.01528, val loss: 0.01566\n",
      "Training epoch: 3429, train loss: 0.01551, val loss: 0.01589\n",
      "Training epoch: 3430, train loss: 0.01538, val loss: 0.01576\n",
      "Training epoch: 3431, train loss: 0.01536, val loss: 0.01576\n",
      "Training epoch: 3432, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 3433, train loss: 0.01543, val loss: 0.01582\n",
      "Training epoch: 3434, train loss: 0.01535, val loss: 0.01573\n",
      "Training epoch: 3435, train loss: 0.01533, val loss: 0.01571\n",
      "Training epoch: 3436, train loss: 0.01557, val loss: 0.01594\n",
      "Training epoch: 3437, train loss: 0.01538, val loss: 0.01578\n",
      "Training epoch: 3438, train loss: 0.01525, val loss: 0.01559\n",
      "Training epoch: 3439, train loss: 0.01535, val loss: 0.01571\n",
      "Training epoch: 3440, train loss: 0.01529, val loss: 0.01565\n",
      "Training epoch: 3441, train loss: 0.01550, val loss: 0.01588\n",
      "Training epoch: 3442, train loss: 0.01577, val loss: 0.01619\n",
      "Training epoch: 3443, train loss: 0.01547, val loss: 0.01587\n",
      "Training epoch: 3444, train loss: 0.01582, val loss: 0.01625\n",
      "Training epoch: 3445, train loss: 0.01550, val loss: 0.01589\n",
      "Training epoch: 3446, train loss: 0.01592, val loss: 0.01631\n",
      "Training epoch: 3447, train loss: 0.01545, val loss: 0.01584\n",
      "Training epoch: 3448, train loss: 0.01552, val loss: 0.01591\n",
      "Training epoch: 3449, train loss: 0.01546, val loss: 0.01579\n",
      "Training epoch: 3450, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3451, train loss: 0.01554, val loss: 0.01593\n",
      "Training epoch: 3452, train loss: 0.01524, val loss: 0.01563\n",
      "Training epoch: 3453, train loss: 0.01556, val loss: 0.01594\n",
      "Training epoch: 3454, train loss: 0.01538, val loss: 0.01576\n",
      "Training epoch: 3455, train loss: 0.01553, val loss: 0.01594\n",
      "Training epoch: 3456, train loss: 0.01546, val loss: 0.01588\n",
      "Training epoch: 3457, train loss: 0.01550, val loss: 0.01589\n",
      "Training epoch: 3458, train loss: 0.01526, val loss: 0.01565\n",
      "Training epoch: 3459, train loss: 0.01550, val loss: 0.01584\n",
      "Training epoch: 3460, train loss: 0.01546, val loss: 0.01583\n",
      "Training epoch: 3461, train loss: 0.01550, val loss: 0.01591\n",
      "Training epoch: 3462, train loss: 0.01524, val loss: 0.01564\n",
      "Training epoch: 3463, train loss: 0.01546, val loss: 0.01585\n",
      "Training epoch: 3464, train loss: 0.01546, val loss: 0.01584\n",
      "Training epoch: 3465, train loss: 0.01524, val loss: 0.01559\n",
      "Training epoch: 3466, train loss: 0.01619, val loss: 0.01656\n",
      "Training epoch: 3467, train loss: 0.01541, val loss: 0.01578\n",
      "Training epoch: 3468, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 3469, train loss: 0.01574, val loss: 0.01611\n",
      "Training epoch: 3470, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 3471, train loss: 0.01532, val loss: 0.01569\n",
      "Training epoch: 3472, train loss: 0.01524, val loss: 0.01559\n",
      "Training epoch: 3473, train loss: 0.01559, val loss: 0.01594\n",
      "Training epoch: 3474, train loss: 0.01571, val loss: 0.01611\n",
      "Training epoch: 3475, train loss: 0.01523, val loss: 0.01563\n",
      "Training epoch: 3476, train loss: 0.01548, val loss: 0.01587\n",
      "Training epoch: 3477, train loss: 0.01530, val loss: 0.01568\n",
      "Training epoch: 3478, train loss: 0.01529, val loss: 0.01563\n",
      "Training epoch: 3479, train loss: 0.01532, val loss: 0.01570\n",
      "Training epoch: 3480, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3481, train loss: 0.01558, val loss: 0.01599\n",
      "Training epoch: 3482, train loss: 0.01572, val loss: 0.01602\n",
      "Training epoch: 3483, train loss: 0.01577, val loss: 0.01611\n",
      "Training epoch: 3484, train loss: 0.01543, val loss: 0.01581\n",
      "Training epoch: 3485, train loss: 0.01531, val loss: 0.01565\n",
      "Training epoch: 3486, train loss: 0.01536, val loss: 0.01574\n",
      "Training epoch: 3487, train loss: 0.01598, val loss: 0.01637\n",
      "Training epoch: 3488, train loss: 0.01538, val loss: 0.01576\n",
      "Training epoch: 3489, train loss: 0.01565, val loss: 0.01606\n",
      "Training epoch: 3490, train loss: 0.01543, val loss: 0.01583\n",
      "Training epoch: 3491, train loss: 0.01530, val loss: 0.01568\n",
      "Training epoch: 3492, train loss: 0.01538, val loss: 0.01573\n",
      "Training epoch: 3493, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 3494, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3495, train loss: 0.01537, val loss: 0.01575\n",
      "Training epoch: 3496, train loss: 0.01530, val loss: 0.01570\n",
      "Training epoch: 3497, train loss: 0.01530, val loss: 0.01565\n",
      "Training epoch: 3498, train loss: 0.01523, val loss: 0.01563\n",
      "Training epoch: 3499, train loss: 0.01530, val loss: 0.01568\n",
      "Training epoch: 3500, train loss: 0.01539, val loss: 0.01577\n",
      "Training epoch: 3501, train loss: 0.01550, val loss: 0.01587\n",
      "Training epoch: 3502, train loss: 0.01542, val loss: 0.01578\n",
      "Training epoch: 3503, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3504, train loss: 0.01538, val loss: 0.01574\n",
      "Training epoch: 3505, train loss: 0.01560, val loss: 0.01597\n",
      "Training epoch: 3506, train loss: 0.01525, val loss: 0.01565\n",
      "Training epoch: 3507, train loss: 0.01529, val loss: 0.01567\n",
      "Training epoch: 3508, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 3509, train loss: 0.01553, val loss: 0.01591\n",
      "Training epoch: 3510, train loss: 0.01539, val loss: 0.01581\n",
      "Training epoch: 3511, train loss: 0.01545, val loss: 0.01581\n",
      "Training epoch: 3512, train loss: 0.01549, val loss: 0.01586\n",
      "Training epoch: 3513, train loss: 0.01568, val loss: 0.01602\n",
      "Training epoch: 3514, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3515, train loss: 0.01543, val loss: 0.01575\n",
      "Training epoch: 3516, train loss: 0.01525, val loss: 0.01560\n",
      "Training epoch: 3517, train loss: 0.01558, val loss: 0.01592\n",
      "Training epoch: 3518, train loss: 0.01538, val loss: 0.01571\n",
      "Training epoch: 3519, train loss: 0.01531, val loss: 0.01571\n",
      "Training epoch: 3520, train loss: 0.01549, val loss: 0.01582\n",
      "Training epoch: 3521, train loss: 0.01543, val loss: 0.01578\n",
      "Training epoch: 3522, train loss: 0.01533, val loss: 0.01573\n",
      "Training epoch: 3523, train loss: 0.01562, val loss: 0.01601\n",
      "Training epoch: 3524, train loss: 0.01547, val loss: 0.01584\n",
      "Training epoch: 3525, train loss: 0.01539, val loss: 0.01579\n",
      "Training epoch: 3526, train loss: 0.01552, val loss: 0.01587\n",
      "Training epoch: 3527, train loss: 0.01620, val loss: 0.01660\n",
      "Training epoch: 3528, train loss: 0.01537, val loss: 0.01574\n",
      "Training epoch: 3529, train loss: 0.01546, val loss: 0.01582\n",
      "Training epoch: 3530, train loss: 0.01559, val loss: 0.01594\n",
      "Training epoch: 3531, train loss: 0.01543, val loss: 0.01580\n",
      "Training epoch: 3532, train loss: 0.01543, val loss: 0.01579\n",
      "Training epoch: 3533, train loss: 0.01525, val loss: 0.01565\n",
      "Training epoch: 3534, train loss: 0.01564, val loss: 0.01607\n",
      "Training epoch: 3535, train loss: 0.01551, val loss: 0.01590\n",
      "Training epoch: 3536, train loss: 0.01535, val loss: 0.01571\n",
      "Training epoch: 3537, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3538, train loss: 0.01540, val loss: 0.01577\n",
      "Training epoch: 3539, train loss: 0.01543, val loss: 0.01575\n",
      "Training epoch: 3540, train loss: 0.01528, val loss: 0.01567\n",
      "Training epoch: 3541, train loss: 0.01542, val loss: 0.01583\n",
      "Training epoch: 3542, train loss: 0.01536, val loss: 0.01572\n",
      "Training epoch: 3543, train loss: 0.01568, val loss: 0.01609\n",
      "Training epoch: 3544, train loss: 0.01573, val loss: 0.01613\n",
      "Training epoch: 3545, train loss: 0.01529, val loss: 0.01565\n",
      "Training epoch: 3546, train loss: 0.01577, val loss: 0.01618\n",
      "Training epoch: 3547, train loss: 0.01527, val loss: 0.01569\n",
      "Training epoch: 3548, train loss: 0.01525, val loss: 0.01561\n",
      "Training epoch: 3549, train loss: 0.01542, val loss: 0.01582\n",
      "Training epoch: 3550, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 3551, train loss: 0.01573, val loss: 0.01611\n",
      "Training epoch: 3552, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 3553, train loss: 0.01530, val loss: 0.01568\n",
      "Training epoch: 3554, train loss: 0.01531, val loss: 0.01570\n",
      "Training epoch: 3555, train loss: 0.01539, val loss: 0.01577\n",
      "Training epoch: 3556, train loss: 0.01530, val loss: 0.01568\n",
      "Training epoch: 3557, train loss: 0.01558, val loss: 0.01598\n",
      "Training epoch: 3558, train loss: 0.01539, val loss: 0.01575\n",
      "Training epoch: 3559, train loss: 0.01528, val loss: 0.01566\n",
      "Training epoch: 3560, train loss: 0.01575, val loss: 0.01612\n",
      "Training epoch: 3561, train loss: 0.01542, val loss: 0.01576\n",
      "Training epoch: 3562, train loss: 0.01541, val loss: 0.01578\n",
      "Training epoch: 3563, train loss: 0.01558, val loss: 0.01594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 3564, train loss: 0.01522, val loss: 0.01563\n",
      "Training epoch: 3565, train loss: 0.01555, val loss: 0.01589\n",
      "Training epoch: 3566, train loss: 0.01582, val loss: 0.01621\n",
      "Training epoch: 3567, train loss: 0.01530, val loss: 0.01567\n",
      "Training epoch: 3568, train loss: 0.01549, val loss: 0.01590\n",
      "Training epoch: 3569, train loss: 0.01539, val loss: 0.01574\n",
      "Training epoch: 3570, train loss: 0.01531, val loss: 0.01570\n",
      "Training epoch: 3571, train loss: 0.01535, val loss: 0.01572\n",
      "Training epoch: 3572, train loss: 0.01557, val loss: 0.01589\n",
      "Training epoch: 3573, train loss: 0.01548, val loss: 0.01585\n",
      "Training epoch: 3574, train loss: 0.01530, val loss: 0.01567\n",
      "Training epoch: 3575, train loss: 0.01566, val loss: 0.01600\n",
      "Training epoch: 3576, train loss: 0.01561, val loss: 0.01595\n",
      "Training epoch: 3577, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 3578, train loss: 0.01546, val loss: 0.01587\n",
      "Training epoch: 3579, train loss: 0.01539, val loss: 0.01574\n",
      "Training epoch: 3580, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3581, train loss: 0.01529, val loss: 0.01568\n",
      "Training epoch: 3582, train loss: 0.01533, val loss: 0.01573\n",
      "Training epoch: 3583, train loss: 0.01541, val loss: 0.01577\n",
      "Training epoch: 3584, train loss: 0.01530, val loss: 0.01570\n",
      "Training epoch: 3585, train loss: 0.01529, val loss: 0.01567\n",
      "Training epoch: 3586, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 3587, train loss: 0.01520, val loss: 0.01558\n",
      "Training epoch: 3588, train loss: 0.01529, val loss: 0.01566\n",
      "Training epoch: 3589, train loss: 0.01624, val loss: 0.01669\n",
      "Training epoch: 3590, train loss: 0.01523, val loss: 0.01556\n",
      "Training epoch: 3591, train loss: 0.01531, val loss: 0.01568\n",
      "Training epoch: 3592, train loss: 0.01546, val loss: 0.01585\n",
      "Training epoch: 3593, train loss: 0.01558, val loss: 0.01595\n",
      "Training epoch: 3594, train loss: 0.01553, val loss: 0.01589\n",
      "Training epoch: 3595, train loss: 0.01537, val loss: 0.01572\n",
      "Training epoch: 3596, train loss: 0.01536, val loss: 0.01574\n",
      "Training epoch: 3597, train loss: 0.01535, val loss: 0.01575\n",
      "Training epoch: 3598, train loss: 0.01529, val loss: 0.01567\n",
      "Training epoch: 3599, train loss: 0.01531, val loss: 0.01564\n",
      "Training epoch: 3600, train loss: 0.01532, val loss: 0.01568\n",
      "Training epoch: 3601, train loss: 0.01566, val loss: 0.01601\n",
      "Training epoch: 3602, train loss: 0.01527, val loss: 0.01566\n",
      "Training epoch: 3603, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3604, train loss: 0.01530, val loss: 0.01568\n",
      "Training epoch: 3605, train loss: 0.01553, val loss: 0.01591\n",
      "Training epoch: 3606, train loss: 0.01530, val loss: 0.01568\n",
      "Training epoch: 3607, train loss: 0.01534, val loss: 0.01574\n",
      "Training epoch: 3608, train loss: 0.01539, val loss: 0.01572\n",
      "Training epoch: 3609, train loss: 0.01567, val loss: 0.01611\n",
      "Training epoch: 3610, train loss: 0.01531, val loss: 0.01569\n",
      "Training epoch: 3611, train loss: 0.01569, val loss: 0.01607\n",
      "Training epoch: 3612, train loss: 0.01532, val loss: 0.01570\n",
      "Training epoch: 3613, train loss: 0.01536, val loss: 0.01575\n",
      "Training epoch: 3614, train loss: 0.01551, val loss: 0.01588\n",
      "Training epoch: 3615, train loss: 0.01526, val loss: 0.01561\n",
      "Training epoch: 3616, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 3617, train loss: 0.01541, val loss: 0.01579\n",
      "Training epoch: 3618, train loss: 0.01531, val loss: 0.01567\n",
      "Training epoch: 3619, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3620, train loss: 0.01528, val loss: 0.01563\n",
      "Training epoch: 3621, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 3622, train loss: 0.01532, val loss: 0.01571\n",
      "Training epoch: 3623, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 3624, train loss: 0.01538, val loss: 0.01573\n",
      "Training epoch: 3625, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 3626, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3627, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 3628, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 3629, train loss: 0.01538, val loss: 0.01576\n",
      "Training epoch: 3630, train loss: 0.01528, val loss: 0.01562\n",
      "Training epoch: 3631, train loss: 0.01555, val loss: 0.01595\n",
      "Training epoch: 3632, train loss: 0.01557, val loss: 0.01595\n",
      "Training epoch: 3633, train loss: 0.01559, val loss: 0.01595\n",
      "Training epoch: 3634, train loss: 0.01554, val loss: 0.01592\n",
      "Training epoch: 3635, train loss: 0.01541, val loss: 0.01575\n",
      "Training epoch: 3636, train loss: 0.01539, val loss: 0.01581\n",
      "Training epoch: 3637, train loss: 0.01539, val loss: 0.01572\n",
      "Training epoch: 3638, train loss: 0.01531, val loss: 0.01567\n",
      "Training epoch: 3639, train loss: 0.01551, val loss: 0.01590\n",
      "Training epoch: 3640, train loss: 0.01551, val loss: 0.01586\n",
      "Training epoch: 3641, train loss: 0.01560, val loss: 0.01597\n",
      "Training epoch: 3642, train loss: 0.01548, val loss: 0.01581\n",
      "Training epoch: 3643, train loss: 0.01556, val loss: 0.01589\n",
      "Training epoch: 3644, train loss: 0.01534, val loss: 0.01573\n",
      "Training epoch: 3645, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3646, train loss: 0.01521, val loss: 0.01556\n",
      "Training epoch: 3647, train loss: 0.01543, val loss: 0.01579\n",
      "Training epoch: 3648, train loss: 0.01544, val loss: 0.01583\n",
      "Training epoch: 3649, train loss: 0.01558, val loss: 0.01597\n",
      "Training epoch: 3650, train loss: 0.01531, val loss: 0.01569\n",
      "Training epoch: 3651, train loss: 0.01565, val loss: 0.01603\n",
      "Training epoch: 3652, train loss: 0.01547, val loss: 0.01580\n",
      "Training epoch: 3653, train loss: 0.01543, val loss: 0.01580\n",
      "Training epoch: 3654, train loss: 0.01549, val loss: 0.01589\n",
      "Training epoch: 3655, train loss: 0.01532, val loss: 0.01569\n",
      "Training epoch: 3656, train loss: 0.01533, val loss: 0.01572\n",
      "Training epoch: 3657, train loss: 0.01530, val loss: 0.01568\n",
      "Training epoch: 3658, train loss: 0.01548, val loss: 0.01583\n",
      "Training epoch: 3659, train loss: 0.01551, val loss: 0.01585\n",
      "Training epoch: 3660, train loss: 0.01568, val loss: 0.01602\n",
      "Training epoch: 3661, train loss: 0.01535, val loss: 0.01572\n",
      "Training epoch: 3662, train loss: 0.01557, val loss: 0.01593\n",
      "Training epoch: 3663, train loss: 0.01534, val loss: 0.01569\n",
      "Training epoch: 3664, train loss: 0.01526, val loss: 0.01560\n",
      "Training epoch: 3665, train loss: 0.01530, val loss: 0.01568\n",
      "Training epoch: 3666, train loss: 0.01555, val loss: 0.01593\n",
      "Training epoch: 3667, train loss: 0.01604, val loss: 0.01643\n",
      "Training epoch: 3668, train loss: 0.01540, val loss: 0.01581\n",
      "Training epoch: 3669, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3670, train loss: 0.01534, val loss: 0.01572\n",
      "Training epoch: 3671, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 3672, train loss: 0.01526, val loss: 0.01566\n",
      "Training epoch: 3673, train loss: 0.01528, val loss: 0.01566\n",
      "Training epoch: 3674, train loss: 0.01535, val loss: 0.01574\n",
      "Training epoch: 3675, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 3676, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 3677, train loss: 0.01529, val loss: 0.01570\n",
      "Training epoch: 3678, train loss: 0.01531, val loss: 0.01566\n",
      "Training epoch: 3679, train loss: 0.01548, val loss: 0.01583\n",
      "Training epoch: 3680, train loss: 0.01551, val loss: 0.01592\n",
      "Training epoch: 3681, train loss: 0.01539, val loss: 0.01578\n",
      "Training epoch: 3682, train loss: 0.01546, val loss: 0.01582\n",
      "Training epoch: 3683, train loss: 0.01521, val loss: 0.01556\n",
      "Training epoch: 3684, train loss: 0.01530, val loss: 0.01569\n",
      "Training epoch: 3685, train loss: 0.01535, val loss: 0.01575\n",
      "Training epoch: 3686, train loss: 0.01524, val loss: 0.01558\n",
      "Training epoch: 3687, train loss: 0.01573, val loss: 0.01611\n",
      "Training epoch: 3688, train loss: 0.01536, val loss: 0.01572\n",
      "Training epoch: 3689, train loss: 0.01694, val loss: 0.01731\n",
      "Training epoch: 3690, train loss: 0.01541, val loss: 0.01579\n",
      "Training epoch: 3691, train loss: 0.01538, val loss: 0.01575\n",
      "Training epoch: 3692, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3693, train loss: 0.01529, val loss: 0.01567\n",
      "Training epoch: 3694, train loss: 0.01550, val loss: 0.01590\n",
      "Training epoch: 3695, train loss: 0.01532, val loss: 0.01572\n",
      "Training epoch: 3696, train loss: 0.01541, val loss: 0.01580\n",
      "Training epoch: 3697, train loss: 0.01538, val loss: 0.01576\n",
      "Training epoch: 3698, train loss: 0.01538, val loss: 0.01575\n",
      "Training epoch: 3699, train loss: 0.01535, val loss: 0.01573\n",
      "Training epoch: 3700, train loss: 0.01561, val loss: 0.01602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 3701, train loss: 0.01544, val loss: 0.01584\n",
      "Training epoch: 3702, train loss: 0.01561, val loss: 0.01600\n",
      "Training epoch: 3703, train loss: 0.01546, val loss: 0.01585\n",
      "Training epoch: 3704, train loss: 0.01579, val loss: 0.01617\n",
      "Training epoch: 3705, train loss: 0.01547, val loss: 0.01582\n",
      "Training epoch: 3706, train loss: 0.01537, val loss: 0.01576\n",
      "Training epoch: 3707, train loss: 0.01546, val loss: 0.01584\n",
      "Training epoch: 3708, train loss: 0.01530, val loss: 0.01569\n",
      "Training epoch: 3709, train loss: 0.01531, val loss: 0.01566\n",
      "Training epoch: 3710, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 3711, train loss: 0.01534, val loss: 0.01573\n",
      "Training epoch: 3712, train loss: 0.01589, val loss: 0.01626\n",
      "Training epoch: 3713, train loss: 0.01592, val loss: 0.01632\n",
      "Training epoch: 3714, train loss: 0.01545, val loss: 0.01584\n",
      "Training epoch: 3715, train loss: 0.01538, val loss: 0.01577\n",
      "Training epoch: 3716, train loss: 0.01601, val loss: 0.01640\n",
      "Training epoch: 3717, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 3718, train loss: 0.01581, val loss: 0.01616\n",
      "Training epoch: 3719, train loss: 0.01537, val loss: 0.01576\n",
      "Training epoch: 3720, train loss: 0.01546, val loss: 0.01585\n",
      "Training epoch: 3721, train loss: 0.01561, val loss: 0.01597\n",
      "Training epoch: 3722, train loss: 0.01566, val loss: 0.01604\n",
      "Training epoch: 3723, train loss: 0.01544, val loss: 0.01582\n",
      "Training epoch: 3724, train loss: 0.01551, val loss: 0.01589\n",
      "Training epoch: 3725, train loss: 0.01543, val loss: 0.01581\n",
      "Training epoch: 3726, train loss: 0.01535, val loss: 0.01572\n",
      "Training epoch: 3727, train loss: 0.01544, val loss: 0.01579\n",
      "Training epoch: 3728, train loss: 0.01539, val loss: 0.01576\n",
      "Training epoch: 3729, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3730, train loss: 0.01538, val loss: 0.01574\n",
      "Training epoch: 3731, train loss: 0.01529, val loss: 0.01566\n",
      "Training epoch: 3732, train loss: 0.01535, val loss: 0.01573\n",
      "Training epoch: 3733, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 3734, train loss: 0.01538, val loss: 0.01573\n",
      "Training epoch: 3735, train loss: 0.01536, val loss: 0.01575\n",
      "Training epoch: 3736, train loss: 0.01554, val loss: 0.01592\n",
      "Training epoch: 3737, train loss: 0.01550, val loss: 0.01582\n",
      "Training epoch: 3738, train loss: 0.01529, val loss: 0.01565\n",
      "Training epoch: 3739, train loss: 0.01535, val loss: 0.01572\n",
      "Training epoch: 3740, train loss: 0.01532, val loss: 0.01570\n",
      "Training epoch: 3741, train loss: 0.01558, val loss: 0.01597\n",
      "Training epoch: 3742, train loss: 0.01578, val loss: 0.01617\n",
      "Training epoch: 3743, train loss: 0.01529, val loss: 0.01568\n",
      "Training epoch: 3744, train loss: 0.01548, val loss: 0.01582\n",
      "Training epoch: 3745, train loss: 0.01546, val loss: 0.01584\n",
      "Training epoch: 3746, train loss: 0.01520, val loss: 0.01558\n",
      "Training epoch: 3747, train loss: 0.01539, val loss: 0.01578\n",
      "Training epoch: 3748, train loss: 0.01588, val loss: 0.01628\n",
      "Training epoch: 3749, train loss: 0.01531, val loss: 0.01565\n",
      "Training epoch: 3750, train loss: 0.01527, val loss: 0.01561\n",
      "Training epoch: 3751, train loss: 0.01544, val loss: 0.01580\n",
      "Training epoch: 3752, train loss: 0.01530, val loss: 0.01568\n",
      "Training epoch: 3753, train loss: 0.01539, val loss: 0.01577\n",
      "Training epoch: 3754, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 3755, train loss: 0.01580, val loss: 0.01618\n",
      "Training epoch: 3756, train loss: 0.01542, val loss: 0.01580\n",
      "Training epoch: 3757, train loss: 0.01540, val loss: 0.01577\n",
      "Training epoch: 3758, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3759, train loss: 0.01533, val loss: 0.01574\n",
      "Training epoch: 3760, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 3761, train loss: 0.01549, val loss: 0.01585\n",
      "Training epoch: 3762, train loss: 0.01538, val loss: 0.01577\n",
      "Training epoch: 3763, train loss: 0.01542, val loss: 0.01581\n",
      "Training epoch: 3764, train loss: 0.01564, val loss: 0.01598\n",
      "Training epoch: 3765, train loss: 0.01555, val loss: 0.01594\n",
      "Training epoch: 3766, train loss: 0.01539, val loss: 0.01577\n",
      "Training epoch: 3767, train loss: 0.01585, val loss: 0.01625\n",
      "Training epoch: 3768, train loss: 0.01529, val loss: 0.01567\n",
      "Training epoch: 3769, train loss: 0.01532, val loss: 0.01567\n",
      "Training epoch: 3770, train loss: 0.01544, val loss: 0.01577\n",
      "Training epoch: 3771, train loss: 0.01546, val loss: 0.01582\n",
      "Training epoch: 3772, train loss: 0.01540, val loss: 0.01577\n",
      "Training epoch: 3773, train loss: 0.01551, val loss: 0.01588\n",
      "Training epoch: 3774, train loss: 0.01566, val loss: 0.01606\n",
      "Training epoch: 3775, train loss: 0.01543, val loss: 0.01582\n",
      "Training epoch: 3776, train loss: 0.01577, val loss: 0.01611\n",
      "Training epoch: 3777, train loss: 0.01539, val loss: 0.01578\n",
      "Training epoch: 3778, train loss: 0.01556, val loss: 0.01595\n",
      "Training epoch: 3779, train loss: 0.01562, val loss: 0.01595\n",
      "Training epoch: 3780, train loss: 0.01570, val loss: 0.01605\n",
      "Training epoch: 3781, train loss: 0.01554, val loss: 0.01593\n",
      "Training epoch: 3782, train loss: 0.01536, val loss: 0.01571\n",
      "Training epoch: 3783, train loss: 0.01542, val loss: 0.01579\n",
      "Training epoch: 3784, train loss: 0.01569, val loss: 0.01607\n",
      "Training epoch: 3785, train loss: 0.01617, val loss: 0.01657\n",
      "Training epoch: 3786, train loss: 0.01545, val loss: 0.01583\n",
      "Training epoch: 3787, train loss: 0.01563, val loss: 0.01599\n",
      "Training epoch: 3788, train loss: 0.01531, val loss: 0.01566\n",
      "Training epoch: 3789, train loss: 0.01539, val loss: 0.01578\n",
      "Training epoch: 3790, train loss: 0.01522, val loss: 0.01561\n",
      "Training epoch: 3791, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3792, train loss: 0.01527, val loss: 0.01563\n",
      "Training epoch: 3793, train loss: 0.01584, val loss: 0.01622\n",
      "Training epoch: 3794, train loss: 0.01531, val loss: 0.01569\n",
      "Training epoch: 3795, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 3796, train loss: 0.01529, val loss: 0.01567\n",
      "Training epoch: 3797, train loss: 0.01556, val loss: 0.01591\n",
      "Training epoch: 3798, train loss: 0.01593, val loss: 0.01632\n",
      "Training epoch: 3799, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3800, train loss: 0.01565, val loss: 0.01600\n",
      "Training epoch: 3801, train loss: 0.01534, val loss: 0.01573\n",
      "Training epoch: 3802, train loss: 0.01526, val loss: 0.01565\n",
      "Training epoch: 3803, train loss: 0.01535, val loss: 0.01570\n",
      "Training epoch: 3804, train loss: 0.01552, val loss: 0.01590\n",
      "Training epoch: 3805, train loss: 0.01530, val loss: 0.01566\n",
      "Training epoch: 3806, train loss: 0.01543, val loss: 0.01580\n",
      "Training epoch: 3807, train loss: 0.01530, val loss: 0.01569\n",
      "Training epoch: 3808, train loss: 0.01538, val loss: 0.01575\n",
      "Training epoch: 3809, train loss: 0.01528, val loss: 0.01568\n",
      "Training epoch: 3810, train loss: 0.01540, val loss: 0.01578\n",
      "Training epoch: 3811, train loss: 0.01616, val loss: 0.01657\n",
      "Training epoch: 3812, train loss: 0.01525, val loss: 0.01561\n",
      "Training epoch: 3813, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 3814, train loss: 0.01552, val loss: 0.01585\n",
      "Training epoch: 3815, train loss: 0.01534, val loss: 0.01571\n",
      "Training epoch: 3816, train loss: 0.01540, val loss: 0.01577\n",
      "Training epoch: 3817, train loss: 0.01547, val loss: 0.01584\n",
      "Training epoch: 3818, train loss: 0.01570, val loss: 0.01599\n",
      "Training epoch: 3819, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 3820, train loss: 0.01567, val loss: 0.01605\n",
      "Training epoch: 3821, train loss: 0.01543, val loss: 0.01578\n",
      "Training epoch: 3822, train loss: 0.01536, val loss: 0.01572\n",
      "Training epoch: 3823, train loss: 0.01539, val loss: 0.01577\n",
      "Training epoch: 3824, train loss: 0.01545, val loss: 0.01584\n",
      "Training epoch: 3825, train loss: 0.01541, val loss: 0.01579\n",
      "Training epoch: 3826, train loss: 0.01536, val loss: 0.01574\n",
      "Training epoch: 3827, train loss: 0.01568, val loss: 0.01607\n",
      "Training epoch: 3828, train loss: 0.01528, val loss: 0.01569\n",
      "Training epoch: 3829, train loss: 0.01557, val loss: 0.01593\n",
      "Training epoch: 3830, train loss: 0.01585, val loss: 0.01625\n",
      "Training epoch: 3831, train loss: 0.01549, val loss: 0.01588\n",
      "Training epoch: 3832, train loss: 0.01540, val loss: 0.01578\n",
      "Training epoch: 3833, train loss: 0.01544, val loss: 0.01584\n",
      "Training epoch: 3834, train loss: 0.01553, val loss: 0.01592\n",
      "Training epoch: 3835, train loss: 0.01561, val loss: 0.01599\n",
      "Training epoch: 3836, train loss: 0.01543, val loss: 0.01579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 3837, train loss: 0.01549, val loss: 0.01585\n",
      "Training epoch: 3838, train loss: 0.01529, val loss: 0.01565\n",
      "Training epoch: 3839, train loss: 0.01578, val loss: 0.01609\n",
      "Training epoch: 3840, train loss: 0.01542, val loss: 0.01581\n",
      "Training epoch: 3841, train loss: 0.01536, val loss: 0.01573\n",
      "Training epoch: 3842, train loss: 0.01585, val loss: 0.01623\n",
      "Training epoch: 3843, train loss: 0.01534, val loss: 0.01570\n",
      "Training epoch: 3844, train loss: 0.01555, val loss: 0.01596\n",
      "Training epoch: 3845, train loss: 0.01533, val loss: 0.01566\n",
      "Training epoch: 3846, train loss: 0.01550, val loss: 0.01587\n",
      "Training epoch: 3847, train loss: 0.01528, val loss: 0.01567\n",
      "Training epoch: 3848, train loss: 0.01538, val loss: 0.01572\n",
      "Training epoch: 3849, train loss: 0.01527, val loss: 0.01566\n",
      "Training epoch: 3850, train loss: 0.01523, val loss: 0.01562\n",
      "Training epoch: 3851, train loss: 0.01547, val loss: 0.01586\n",
      "Training epoch: 3852, train loss: 0.01638, val loss: 0.01679\n",
      "Training epoch: 3853, train loss: 0.01587, val loss: 0.01624\n",
      "Training epoch: 3854, train loss: 0.01529, val loss: 0.01569\n",
      "Training epoch: 3855, train loss: 0.01537, val loss: 0.01573\n",
      "Training epoch: 3856, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3857, train loss: 0.01575, val loss: 0.01617\n",
      "Training epoch: 3858, train loss: 0.01541, val loss: 0.01575\n",
      "Training epoch: 3859, train loss: 0.01545, val loss: 0.01583\n",
      "Training epoch: 3860, train loss: 0.01539, val loss: 0.01573\n",
      "Training epoch: 3861, train loss: 0.01554, val loss: 0.01593\n",
      "Training epoch: 3862, train loss: 0.01545, val loss: 0.01578\n",
      "Training epoch: 3863, train loss: 0.01531, val loss: 0.01565\n",
      "Training epoch: 3864, train loss: 0.01538, val loss: 0.01571\n",
      "Training epoch: 3865, train loss: 0.01541, val loss: 0.01578\n",
      "Training epoch: 3866, train loss: 0.01568, val loss: 0.01607\n",
      "Training epoch: 3867, train loss: 0.01523, val loss: 0.01562\n",
      "Training epoch: 3868, train loss: 0.01539, val loss: 0.01577\n",
      "Training epoch: 3869, train loss: 0.01534, val loss: 0.01570\n",
      "Training epoch: 3870, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 3871, train loss: 0.01567, val loss: 0.01609\n",
      "Training epoch: 3872, train loss: 0.01538, val loss: 0.01577\n",
      "Training epoch: 3873, train loss: 0.01532, val loss: 0.01571\n",
      "Training epoch: 3874, train loss: 0.01541, val loss: 0.01578\n",
      "Training epoch: 3875, train loss: 0.01543, val loss: 0.01587\n",
      "Training epoch: 3876, train loss: 0.01548, val loss: 0.01589\n",
      "Training epoch: 3877, train loss: 0.01561, val loss: 0.01599\n",
      "Training epoch: 3878, train loss: 0.01537, val loss: 0.01576\n",
      "Training epoch: 3879, train loss: 0.01536, val loss: 0.01575\n",
      "Training epoch: 3880, train loss: 0.01525, val loss: 0.01560\n",
      "Training epoch: 3881, train loss: 0.01543, val loss: 0.01581\n",
      "Training epoch: 3882, train loss: 0.01521, val loss: 0.01556\n",
      "Training epoch: 3883, train loss: 0.01548, val loss: 0.01586\n",
      "Training epoch: 3884, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 3885, train loss: 0.01556, val loss: 0.01593\n",
      "Training epoch: 3886, train loss: 0.01571, val loss: 0.01606\n",
      "Training epoch: 3887, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3888, train loss: 0.01550, val loss: 0.01595\n",
      "Training epoch: 3889, train loss: 0.01528, val loss: 0.01564\n",
      "Training epoch: 3890, train loss: 0.01570, val loss: 0.01605\n",
      "Training epoch: 3891, train loss: 0.01545, val loss: 0.01586\n",
      "Training epoch: 3892, train loss: 0.01561, val loss: 0.01600\n",
      "Training epoch: 3893, train loss: 0.01533, val loss: 0.01570\n",
      "Training epoch: 3894, train loss: 0.01527, val loss: 0.01566\n",
      "Training epoch: 3895, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3896, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 3897, train loss: 0.01533, val loss: 0.01568\n",
      "Training epoch: 3898, train loss: 0.01535, val loss: 0.01575\n",
      "Training epoch: 3899, train loss: 0.01533, val loss: 0.01570\n",
      "Training epoch: 3900, train loss: 0.01527, val loss: 0.01562\n",
      "Training epoch: 3901, train loss: 0.01550, val loss: 0.01588\n",
      "Training epoch: 3902, train loss: 0.01532, val loss: 0.01573\n",
      "Training epoch: 3903, train loss: 0.01628, val loss: 0.01667\n",
      "Training epoch: 3904, train loss: 0.01541, val loss: 0.01578\n",
      "Training epoch: 3905, train loss: 0.01559, val loss: 0.01593\n",
      "Training epoch: 3906, train loss: 0.01534, val loss: 0.01571\n",
      "Training epoch: 3907, train loss: 0.01558, val loss: 0.01591\n",
      "Training epoch: 3908, train loss: 0.01537, val loss: 0.01578\n",
      "Training epoch: 3909, train loss: 0.01532, val loss: 0.01573\n",
      "Training epoch: 3910, train loss: 0.01529, val loss: 0.01565\n",
      "Training epoch: 3911, train loss: 0.01542, val loss: 0.01580\n",
      "Training epoch: 3912, train loss: 0.01540, val loss: 0.01581\n",
      "Training epoch: 3913, train loss: 0.01561, val loss: 0.01602\n",
      "Training epoch: 3914, train loss: 0.01525, val loss: 0.01566\n",
      "Training epoch: 3915, train loss: 0.01540, val loss: 0.01576\n",
      "Training epoch: 3916, train loss: 0.01525, val loss: 0.01561\n",
      "Training epoch: 3917, train loss: 0.01533, val loss: 0.01571\n",
      "Training epoch: 3918, train loss: 0.01538, val loss: 0.01573\n",
      "Training epoch: 3919, train loss: 0.01527, val loss: 0.01566\n",
      "Training epoch: 3920, train loss: 0.01545, val loss: 0.01579\n",
      "Training epoch: 3921, train loss: 0.01545, val loss: 0.01584\n",
      "Training epoch: 3922, train loss: 0.01552, val loss: 0.01584\n",
      "Training epoch: 3923, train loss: 0.01529, val loss: 0.01560\n",
      "Training epoch: 3924, train loss: 0.01544, val loss: 0.01579\n",
      "Training epoch: 3925, train loss: 0.01544, val loss: 0.01581\n",
      "Training epoch: 3926, train loss: 0.01576, val loss: 0.01614\n",
      "Training epoch: 3927, train loss: 0.01571, val loss: 0.01613\n",
      "Training epoch: 3928, train loss: 0.01527, val loss: 0.01566\n",
      "Training epoch: 3929, train loss: 0.01542, val loss: 0.01580\n",
      "Training epoch: 3930, train loss: 0.01593, val loss: 0.01635\n",
      "Training epoch: 3931, train loss: 0.01598, val loss: 0.01626\n",
      "Training epoch: 3932, train loss: 0.01608, val loss: 0.01644\n",
      "Training epoch: 3933, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 3934, train loss: 0.01531, val loss: 0.01569\n",
      "Training epoch: 3935, train loss: 0.01544, val loss: 0.01583\n",
      "Training epoch: 3936, train loss: 0.01624, val loss: 0.01674\n",
      "Training epoch: 3937, train loss: 0.01528, val loss: 0.01565\n",
      "Training epoch: 3938, train loss: 0.01529, val loss: 0.01567\n",
      "Training epoch: 3939, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 3940, train loss: 0.01553, val loss: 0.01590\n",
      "Training epoch: 3941, train loss: 0.01552, val loss: 0.01590\n",
      "Training epoch: 3942, train loss: 0.01536, val loss: 0.01577\n",
      "Training epoch: 3943, train loss: 0.01543, val loss: 0.01583\n",
      "Training epoch: 3944, train loss: 0.01546, val loss: 0.01587\n",
      "Training epoch: 3945, train loss: 0.01526, val loss: 0.01560\n",
      "Training epoch: 3946, train loss: 0.01526, val loss: 0.01562\n",
      "Training epoch: 3947, train loss: 0.01532, val loss: 0.01571\n",
      "Training epoch: 3948, train loss: 0.01546, val loss: 0.01583\n",
      "Training epoch: 3949, train loss: 0.01543, val loss: 0.01580\n",
      "Training epoch: 3950, train loss: 0.01541, val loss: 0.01579\n",
      "Training epoch: 3951, train loss: 0.01534, val loss: 0.01575\n",
      "Training epoch: 3952, train loss: 0.01530, val loss: 0.01570\n",
      "Training epoch: 3953, train loss: 0.01546, val loss: 0.01587\n",
      "Training epoch: 3954, train loss: 0.01549, val loss: 0.01589\n",
      "Training epoch: 3955, train loss: 0.01533, val loss: 0.01572\n",
      "Training epoch: 3956, train loss: 0.01535, val loss: 0.01573\n",
      "Training epoch: 3957, train loss: 0.01593, val loss: 0.01631\n",
      "Training epoch: 3958, train loss: 0.01549, val loss: 0.01588\n",
      "Training epoch: 3959, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 3960, train loss: 0.01544, val loss: 0.01577\n",
      "Training epoch: 3961, train loss: 0.01528, val loss: 0.01565\n",
      "Training epoch: 3962, train loss: 0.01556, val loss: 0.01594\n",
      "Training epoch: 3963, train loss: 0.01534, val loss: 0.01573\n",
      "Training epoch: 3964, train loss: 0.01527, val loss: 0.01569\n",
      "Training epoch: 3965, train loss: 0.01533, val loss: 0.01571\n",
      "Training epoch: 3966, train loss: 0.01534, val loss: 0.01570\n",
      "Training epoch: 3967, train loss: 0.01559, val loss: 0.01595\n",
      "Training epoch: 3968, train loss: 0.01546, val loss: 0.01586\n",
      "Training epoch: 3969, train loss: 0.01537, val loss: 0.01573\n",
      "Training epoch: 3970, train loss: 0.01538, val loss: 0.01572\n",
      "Training epoch: 3971, train loss: 0.01539, val loss: 0.01576\n",
      "Training epoch: 3972, train loss: 0.01565, val loss: 0.01603\n",
      "Training epoch: 3973, train loss: 0.01533, val loss: 0.01568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 3974, train loss: 0.01533, val loss: 0.01567\n",
      "Training epoch: 3975, train loss: 0.01532, val loss: 0.01570\n",
      "Training epoch: 3976, train loss: 0.01567, val loss: 0.01605\n",
      "Training epoch: 3977, train loss: 0.01546, val loss: 0.01578\n",
      "Training epoch: 3978, train loss: 0.01546, val loss: 0.01579\n",
      "Training epoch: 3979, train loss: 0.01536, val loss: 0.01571\n",
      "Training epoch: 3980, train loss: 0.01562, val loss: 0.01597\n",
      "Training epoch: 3981, train loss: 0.01539, val loss: 0.01578\n",
      "Training epoch: 3982, train loss: 0.01547, val loss: 0.01581\n",
      "Training epoch: 3983, train loss: 0.01548, val loss: 0.01584\n",
      "Training epoch: 3984, train loss: 0.01591, val loss: 0.01627\n",
      "Training epoch: 3985, train loss: 0.01566, val loss: 0.01604\n",
      "Training epoch: 3986, train loss: 0.01537, val loss: 0.01572\n",
      "Training epoch: 3987, train loss: 0.01540, val loss: 0.01581\n",
      "Training epoch: 3988, train loss: 0.01539, val loss: 0.01574\n",
      "Training epoch: 3989, train loss: 0.01528, val loss: 0.01565\n",
      "Training epoch: 3990, train loss: 0.01569, val loss: 0.01609\n",
      "Training epoch: 3991, train loss: 0.01572, val loss: 0.01613\n",
      "Training epoch: 3992, train loss: 0.01522, val loss: 0.01561\n",
      "Training epoch: 3993, train loss: 0.01583, val loss: 0.01621\n",
      "Training epoch: 3994, train loss: 0.01593, val loss: 0.01634\n",
      "Training epoch: 3995, train loss: 0.01549, val loss: 0.01581\n",
      "Training epoch: 3996, train loss: 0.01531, val loss: 0.01568\n",
      "Training epoch: 3997, train loss: 0.01531, val loss: 0.01565\n",
      "Training epoch: 3998, train loss: 0.01527, val loss: 0.01560\n",
      "Training epoch: 3999, train loss: 0.01546, val loss: 0.01585\n",
      "Training epoch: 4000, train loss: 0.01554, val loss: 0.01597\n",
      "Training epoch: 4001, train loss: 0.01549, val loss: 0.01589\n",
      "Training epoch: 4002, train loss: 0.01556, val loss: 0.01596\n",
      "Training epoch: 4003, train loss: 0.01547, val loss: 0.01578\n",
      "Training epoch: 4004, train loss: 0.01576, val loss: 0.01614\n",
      "Training epoch: 4005, train loss: 0.01540, val loss: 0.01578\n",
      "Training epoch: 4006, train loss: 0.01528, val loss: 0.01566\n",
      "Training epoch: 4007, train loss: 0.01528, val loss: 0.01565\n",
      "Training epoch: 4008, train loss: 0.01540, val loss: 0.01580\n",
      "Training epoch: 4009, train loss: 0.01536, val loss: 0.01574\n",
      "Training epoch: 4010, train loss: 0.01630, val loss: 0.01669\n",
      "Training epoch: 4011, train loss: 0.01560, val loss: 0.01602\n",
      "Training epoch: 4012, train loss: 0.01559, val loss: 0.01598\n",
      "Training epoch: 4013, train loss: 0.01614, val loss: 0.01650\n",
      "Training epoch: 4014, train loss: 0.01534, val loss: 0.01571\n",
      "Training epoch: 4015, train loss: 0.01527, val loss: 0.01566\n",
      "Training epoch: 4016, train loss: 0.01554, val loss: 0.01590\n",
      "Training epoch: 4017, train loss: 0.01540, val loss: 0.01578\n",
      "Training epoch: 4018, train loss: 0.01544, val loss: 0.01585\n",
      "Training epoch: 4019, train loss: 0.01590, val loss: 0.01626\n",
      "Training epoch: 4020, train loss: 0.01531, val loss: 0.01573\n",
      "Training epoch: 4021, train loss: 0.01543, val loss: 0.01581\n",
      "Training epoch: 4022, train loss: 0.01541, val loss: 0.01578\n",
      "Training epoch: 4023, train loss: 0.01538, val loss: 0.01573\n",
      "Training epoch: 4024, train loss: 0.01529, val loss: 0.01566\n",
      "Training epoch: 4025, train loss: 0.01547, val loss: 0.01584\n",
      "Training epoch: 4026, train loss: 0.01555, val loss: 0.01592\n",
      "Training epoch: 4027, train loss: 0.01540, val loss: 0.01577\n",
      "Training epoch: 4028, train loss: 0.01537, val loss: 0.01575\n",
      "Training epoch: 4029, train loss: 0.01546, val loss: 0.01577\n",
      "Training epoch: 4030, train loss: 0.01537, val loss: 0.01577\n",
      "Training epoch: 4031, train loss: 0.01533, val loss: 0.01569\n",
      "Training epoch: 4032, train loss: 0.01528, val loss: 0.01562\n",
      "Training epoch: 4033, train loss: 0.01534, val loss: 0.01574\n",
      "Training epoch: 4034, train loss: 0.01530, val loss: 0.01567\n",
      "Training epoch: 4035, train loss: 0.01537, val loss: 0.01575\n",
      "Training epoch: 4036, train loss: 0.01534, val loss: 0.01572\n",
      "Training epoch: 4037, train loss: 0.01535, val loss: 0.01577\n",
      "Training epoch: 4038, train loss: 0.01530, val loss: 0.01567\n",
      "Training epoch: 4039, train loss: 0.01549, val loss: 0.01591\n",
      "Training epoch: 4040, train loss: 0.01543, val loss: 0.01584\n",
      "Training epoch: 4041, train loss: 0.01565, val loss: 0.01605\n",
      "Training epoch: 4042, train loss: 0.01538, val loss: 0.01574\n",
      "Training epoch: 4043, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 4044, train loss: 0.01530, val loss: 0.01570\n",
      "Training epoch: 4045, train loss: 0.01542, val loss: 0.01579\n",
      "Training epoch: 4046, train loss: 0.01537, val loss: 0.01576\n",
      "Training epoch: 4047, train loss: 0.01564, val loss: 0.01602\n",
      "Training epoch: 4048, train loss: 0.01536, val loss: 0.01574\n",
      "Training epoch: 4049, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 4050, train loss: 0.01556, val loss: 0.01599\n",
      "Training epoch: 4051, train loss: 0.01538, val loss: 0.01577\n",
      "Training epoch: 4052, train loss: 0.01563, val loss: 0.01602\n",
      "Training epoch: 4053, train loss: 0.01543, val loss: 0.01579\n",
      "Training epoch: 4054, train loss: 0.01530, val loss: 0.01565\n",
      "Training epoch: 4055, train loss: 0.01527, val loss: 0.01564\n",
      "Training epoch: 4056, train loss: 0.01535, val loss: 0.01572\n",
      "Training epoch: 4057, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 4058, train loss: 0.01561, val loss: 0.01598\n",
      "Training epoch: 4059, train loss: 0.01527, val loss: 0.01566\n",
      "Training epoch: 4060, train loss: 0.01544, val loss: 0.01576\n",
      "Training epoch: 4061, train loss: 0.01533, val loss: 0.01570\n",
      "Training epoch: 4062, train loss: 0.01529, val loss: 0.01564\n",
      "Training epoch: 4063, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 4064, train loss: 0.01526, val loss: 0.01565\n",
      "Training epoch: 4065, train loss: 0.01535, val loss: 0.01569\n",
      "Training epoch: 4066, train loss: 0.01530, val loss: 0.01567\n",
      "Training epoch: 4067, train loss: 0.01532, val loss: 0.01570\n",
      "Training epoch: 4068, train loss: 0.01528, val loss: 0.01568\n",
      "Training epoch: 4069, train loss: 0.01553, val loss: 0.01592\n",
      "Training epoch: 4070, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 4071, train loss: 0.01543, val loss: 0.01576\n",
      "Training epoch: 4072, train loss: 0.01541, val loss: 0.01574\n",
      "Training epoch: 4073, train loss: 0.01559, val loss: 0.01598\n",
      "Training epoch: 4074, train loss: 0.01548, val loss: 0.01579\n",
      "Training epoch: 4075, train loss: 0.01535, val loss: 0.01575\n",
      "Training epoch: 4076, train loss: 0.01531, val loss: 0.01568\n",
      "Training epoch: 4077, train loss: 0.01538, val loss: 0.01574\n",
      "Training epoch: 4078, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 4079, train loss: 0.01544, val loss: 0.01580\n",
      "Training epoch: 4080, train loss: 0.01531, val loss: 0.01570\n",
      "Training epoch: 4081, train loss: 0.01531, val loss: 0.01567\n",
      "Training epoch: 4082, train loss: 0.01538, val loss: 0.01575\n",
      "Training epoch: 4083, train loss: 0.01542, val loss: 0.01581\n",
      "Training epoch: 4084, train loss: 0.01567, val loss: 0.01607\n",
      "Training epoch: 4085, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4086, train loss: 0.01540, val loss: 0.01577\n",
      "Training epoch: 4087, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 4088, train loss: 0.01570, val loss: 0.01610\n",
      "Training epoch: 4089, train loss: 0.01572, val loss: 0.01609\n",
      "Training epoch: 4090, train loss: 0.01577, val loss: 0.01609\n",
      "Training epoch: 4091, train loss: 0.01556, val loss: 0.01594\n",
      "Training epoch: 4092, train loss: 0.01583, val loss: 0.01626\n",
      "Training epoch: 4093, train loss: 0.01544, val loss: 0.01583\n",
      "Training epoch: 4094, train loss: 0.01537, val loss: 0.01575\n",
      "Training epoch: 4095, train loss: 0.01613, val loss: 0.01646\n",
      "Training epoch: 4096, train loss: 0.01552, val loss: 0.01589\n",
      "Training epoch: 4097, train loss: 0.01533, val loss: 0.01571\n",
      "Training epoch: 4098, train loss: 0.01542, val loss: 0.01582\n",
      "Training epoch: 4099, train loss: 0.01532, val loss: 0.01570\n",
      "Training epoch: 4100, train loss: 0.01526, val loss: 0.01561\n",
      "Training epoch: 4101, train loss: 0.01532, val loss: 0.01568\n",
      "Training epoch: 4102, train loss: 0.01523, val loss: 0.01563\n",
      "Training epoch: 4103, train loss: 0.01532, val loss: 0.01573\n",
      "Training epoch: 4104, train loss: 0.01537, val loss: 0.01575\n",
      "Training epoch: 4105, train loss: 0.01573, val loss: 0.01614\n",
      "Training epoch: 4106, train loss: 0.01557, val loss: 0.01595\n",
      "Training epoch: 4107, train loss: 0.01538, val loss: 0.01574\n",
      "Training epoch: 4108, train loss: 0.01587, val loss: 0.01619\n",
      "Training epoch: 4109, train loss: 0.01540, val loss: 0.01577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 4110, train loss: 0.01533, val loss: 0.01574\n",
      "Training epoch: 4111, train loss: 0.01550, val loss: 0.01590\n",
      "Training epoch: 4112, train loss: 0.01560, val loss: 0.01601\n",
      "Training epoch: 4113, train loss: 0.01538, val loss: 0.01576\n",
      "Training epoch: 4114, train loss: 0.01541, val loss: 0.01577\n",
      "Training epoch: 4115, train loss: 0.01523, val loss: 0.01563\n",
      "Training epoch: 4116, train loss: 0.01596, val loss: 0.01636\n",
      "Training epoch: 4117, train loss: 0.01536, val loss: 0.01574\n",
      "Training epoch: 4118, train loss: 0.01526, val loss: 0.01562\n",
      "Training epoch: 4119, train loss: 0.01536, val loss: 0.01573\n",
      "Training epoch: 4120, train loss: 0.01526, val loss: 0.01561\n",
      "Training epoch: 4121, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 4122, train loss: 0.01533, val loss: 0.01571\n",
      "Training epoch: 4123, train loss: 0.01540, val loss: 0.01580\n",
      "Training epoch: 4124, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 4125, train loss: 0.01547, val loss: 0.01580\n",
      "Training epoch: 4126, train loss: 0.01541, val loss: 0.01580\n",
      "Training epoch: 4127, train loss: 0.01555, val loss: 0.01594\n",
      "Training epoch: 4128, train loss: 0.01587, val loss: 0.01627\n",
      "Training epoch: 4129, train loss: 0.01520, val loss: 0.01555\n",
      "Training epoch: 4130, train loss: 0.01539, val loss: 0.01572\n",
      "Training epoch: 4131, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 4132, train loss: 0.01525, val loss: 0.01565\n",
      "Training epoch: 4133, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 4134, train loss: 0.01527, val loss: 0.01566\n",
      "Training epoch: 4135, train loss: 0.01542, val loss: 0.01582\n",
      "Training epoch: 4136, train loss: 0.01537, val loss: 0.01578\n",
      "Training epoch: 4137, train loss: 0.01530, val loss: 0.01567\n",
      "Training epoch: 4138, train loss: 0.01541, val loss: 0.01576\n",
      "Training epoch: 4139, train loss: 0.01542, val loss: 0.01577\n",
      "Training epoch: 4140, train loss: 0.01543, val loss: 0.01580\n",
      "Training epoch: 4141, train loss: 0.01521, val loss: 0.01559\n",
      "Training epoch: 4142, train loss: 0.01529, val loss: 0.01562\n",
      "Training epoch: 4143, train loss: 0.01540, val loss: 0.01575\n",
      "Training epoch: 4144, train loss: 0.01525, val loss: 0.01565\n",
      "Training epoch: 4145, train loss: 0.01528, val loss: 0.01565\n",
      "Training epoch: 4146, train loss: 0.01532, val loss: 0.01570\n",
      "Training epoch: 4147, train loss: 0.01577, val loss: 0.01614\n",
      "Training epoch: 4148, train loss: 0.01534, val loss: 0.01570\n",
      "Training epoch: 4149, train loss: 0.01550, val loss: 0.01585\n",
      "Training epoch: 4150, train loss: 0.01528, val loss: 0.01567\n",
      "Training epoch: 4151, train loss: 0.01525, val loss: 0.01560\n",
      "Training epoch: 4152, train loss: 0.01527, val loss: 0.01563\n",
      "Training epoch: 4153, train loss: 0.01557, val loss: 0.01592\n",
      "Training epoch: 4154, train loss: 0.01549, val loss: 0.01587\n",
      "Training epoch: 4155, train loss: 0.01576, val loss: 0.01610\n",
      "Training epoch: 4156, train loss: 0.01551, val loss: 0.01594\n",
      "Training epoch: 4157, train loss: 0.01532, val loss: 0.01570\n",
      "Training epoch: 4158, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4159, train loss: 0.01530, val loss: 0.01566\n",
      "Training epoch: 4160, train loss: 0.01537, val loss: 0.01577\n",
      "Training epoch: 4161, train loss: 0.01550, val loss: 0.01587\n",
      "Training epoch: 4162, train loss: 0.01530, val loss: 0.01570\n",
      "Training epoch: 4163, train loss: 0.01552, val loss: 0.01592\n",
      "Training epoch: 4164, train loss: 0.01553, val loss: 0.01592\n",
      "Training epoch: 4165, train loss: 0.01578, val loss: 0.01617\n",
      "Training epoch: 4166, train loss: 0.01529, val loss: 0.01565\n",
      "Training epoch: 4167, train loss: 0.01526, val loss: 0.01566\n",
      "Training epoch: 4168, train loss: 0.01539, val loss: 0.01576\n",
      "Training epoch: 4169, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 4170, train loss: 0.01536, val loss: 0.01574\n",
      "Training epoch: 4171, train loss: 0.01600, val loss: 0.01639\n",
      "Training epoch: 4172, train loss: 0.01543, val loss: 0.01577\n",
      "Training epoch: 4173, train loss: 0.01580, val loss: 0.01617\n",
      "Training epoch: 4174, train loss: 0.01572, val loss: 0.01611\n",
      "Training epoch: 4175, train loss: 0.01531, val loss: 0.01568\n",
      "Training epoch: 4176, train loss: 0.01542, val loss: 0.01579\n",
      "Training epoch: 4177, train loss: 0.01540, val loss: 0.01579\n",
      "Training epoch: 4178, train loss: 0.01520, val loss: 0.01559\n",
      "Training epoch: 4179, train loss: 0.01524, val loss: 0.01563\n",
      "Training epoch: 4180, train loss: 0.01563, val loss: 0.01597\n",
      "Training epoch: 4181, train loss: 0.01546, val loss: 0.01578\n",
      "Training epoch: 4182, train loss: 0.01563, val loss: 0.01597\n",
      "Training epoch: 4183, train loss: 0.01527, val loss: 0.01564\n",
      "Training epoch: 4184, train loss: 0.01564, val loss: 0.01600\n",
      "Training epoch: 4185, train loss: 0.01531, val loss: 0.01570\n",
      "Training epoch: 4186, train loss: 0.01532, val loss: 0.01565\n",
      "Training epoch: 4187, train loss: 0.01541, val loss: 0.01577\n",
      "Training epoch: 4188, train loss: 0.01528, val loss: 0.01565\n",
      "Training epoch: 4189, train loss: 0.01555, val loss: 0.01590\n",
      "Training epoch: 4190, train loss: 0.01558, val loss: 0.01597\n",
      "Training epoch: 4191, train loss: 0.01542, val loss: 0.01579\n",
      "Training epoch: 4192, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 4193, train loss: 0.01535, val loss: 0.01573\n",
      "Training epoch: 4194, train loss: 0.01530, val loss: 0.01565\n",
      "Training epoch: 4195, train loss: 0.01537, val loss: 0.01577\n",
      "Training epoch: 4196, train loss: 0.01539, val loss: 0.01576\n",
      "Training epoch: 4197, train loss: 0.01533, val loss: 0.01569\n",
      "Training epoch: 4198, train loss: 0.01529, val loss: 0.01565\n",
      "Training epoch: 4199, train loss: 0.01543, val loss: 0.01576\n",
      "Training epoch: 4200, train loss: 0.01549, val loss: 0.01582\n",
      "Training epoch: 4201, train loss: 0.01554, val loss: 0.01593\n",
      "Training epoch: 4202, train loss: 0.01553, val loss: 0.01590\n",
      "Training epoch: 4203, train loss: 0.01572, val loss: 0.01612\n",
      "Training epoch: 4204, train loss: 0.01541, val loss: 0.01578\n",
      "Training epoch: 4205, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 4206, train loss: 0.01550, val loss: 0.01584\n",
      "Training epoch: 4207, train loss: 0.01588, val loss: 0.01626\n",
      "Training epoch: 4208, train loss: 0.01550, val loss: 0.01589\n",
      "Training epoch: 4209, train loss: 0.01537, val loss: 0.01574\n",
      "Training epoch: 4210, train loss: 0.01563, val loss: 0.01598\n",
      "Training epoch: 4211, train loss: 0.01544, val loss: 0.01582\n",
      "Training epoch: 4212, train loss: 0.01534, val loss: 0.01573\n",
      "Training epoch: 4213, train loss: 0.01537, val loss: 0.01577\n",
      "Training epoch: 4214, train loss: 0.01526, val loss: 0.01561\n",
      "Training epoch: 4215, train loss: 0.01528, val loss: 0.01567\n",
      "Training epoch: 4216, train loss: 0.01532, val loss: 0.01567\n",
      "Training epoch: 4217, train loss: 0.01547, val loss: 0.01583\n",
      "Training epoch: 4218, train loss: 0.01538, val loss: 0.01572\n",
      "Training epoch: 4219, train loss: 0.01526, val loss: 0.01560\n",
      "Training epoch: 4220, train loss: 0.01544, val loss: 0.01581\n",
      "Training epoch: 4221, train loss: 0.01554, val loss: 0.01591\n",
      "Training epoch: 4222, train loss: 0.01535, val loss: 0.01572\n",
      "Training epoch: 4223, train loss: 0.01528, val loss: 0.01562\n",
      "Training epoch: 4224, train loss: 0.01521, val loss: 0.01560\n",
      "Training epoch: 4225, train loss: 0.01537, val loss: 0.01574\n",
      "Training epoch: 4226, train loss: 0.01528, val loss: 0.01567\n",
      "Training epoch: 4227, train loss: 0.01539, val loss: 0.01577\n",
      "Training epoch: 4228, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 4229, train loss: 0.01533, val loss: 0.01571\n",
      "Training epoch: 4230, train loss: 0.01539, val loss: 0.01577\n",
      "Training epoch: 4231, train loss: 0.01527, val loss: 0.01562\n",
      "Training epoch: 4232, train loss: 0.01561, val loss: 0.01602\n",
      "Training epoch: 4233, train loss: 0.01546, val loss: 0.01582\n",
      "Training epoch: 4234, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 4235, train loss: 0.01544, val loss: 0.01585\n",
      "Training epoch: 4236, train loss: 0.01531, val loss: 0.01567\n",
      "Training epoch: 4237, train loss: 0.01528, val loss: 0.01561\n",
      "Training epoch: 4238, train loss: 0.01536, val loss: 0.01573\n",
      "Training epoch: 4239, train loss: 0.01533, val loss: 0.01566\n",
      "Training epoch: 4240, train loss: 0.01548, val loss: 0.01587\n",
      "Training epoch: 4241, train loss: 0.01537, val loss: 0.01575\n",
      "Training epoch: 4242, train loss: 0.01535, val loss: 0.01570\n",
      "Training epoch: 4243, train loss: 0.01545, val loss: 0.01581\n",
      "Training epoch: 4244, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 4245, train loss: 0.01532, val loss: 0.01572\n",
      "Training epoch: 4246, train loss: 0.01526, val loss: 0.01567\n",
      "Training epoch: 4247, train loss: 0.01532, val loss: 0.01566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 4248, train loss: 0.01608, val loss: 0.01649\n",
      "Training epoch: 4249, train loss: 0.01530, val loss: 0.01571\n",
      "Training epoch: 4250, train loss: 0.01535, val loss: 0.01569\n",
      "Training epoch: 4251, train loss: 0.01542, val loss: 0.01579\n",
      "Training epoch: 4252, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 4253, train loss: 0.01527, val loss: 0.01563\n",
      "Training epoch: 4254, train loss: 0.01537, val loss: 0.01572\n",
      "Training epoch: 4255, train loss: 0.01549, val loss: 0.01591\n",
      "Training epoch: 4256, train loss: 0.01600, val loss: 0.01638\n",
      "Training epoch: 4257, train loss: 0.01553, val loss: 0.01593\n",
      "Training epoch: 4258, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 4259, train loss: 0.01549, val loss: 0.01584\n",
      "Training epoch: 4260, train loss: 0.01525, val loss: 0.01560\n",
      "Training epoch: 4261, train loss: 0.01527, val loss: 0.01567\n",
      "Training epoch: 4262, train loss: 0.01538, val loss: 0.01574\n",
      "Training epoch: 4263, train loss: 0.01526, val loss: 0.01560\n",
      "Training epoch: 4264, train loss: 0.01534, val loss: 0.01569\n",
      "Training epoch: 4265, train loss: 0.01549, val loss: 0.01589\n",
      "Training epoch: 4266, train loss: 0.01554, val loss: 0.01592\n",
      "Training epoch: 4267, train loss: 0.01543, val loss: 0.01582\n",
      "Training epoch: 4268, train loss: 0.01548, val loss: 0.01587\n",
      "Training epoch: 4269, train loss: 0.01522, val loss: 0.01555\n",
      "Training epoch: 4270, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 4271, train loss: 0.01558, val loss: 0.01595\n",
      "Training epoch: 4272, train loss: 0.01545, val loss: 0.01582\n",
      "Training epoch: 4273, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4274, train loss: 0.01537, val loss: 0.01575\n",
      "Training epoch: 4275, train loss: 0.01562, val loss: 0.01597\n",
      "Training epoch: 4276, train loss: 0.01577, val loss: 0.01616\n",
      "Training epoch: 4277, train loss: 0.01535, val loss: 0.01572\n",
      "Training epoch: 4278, train loss: 0.01538, val loss: 0.01577\n",
      "Training epoch: 4279, train loss: 0.01569, val loss: 0.01600\n",
      "Training epoch: 4280, train loss: 0.01553, val loss: 0.01592\n",
      "Training epoch: 4281, train loss: 0.01555, val loss: 0.01594\n",
      "Training epoch: 4282, train loss: 0.01554, val loss: 0.01593\n",
      "Training epoch: 4283, train loss: 0.01530, val loss: 0.01568\n",
      "Training epoch: 4284, train loss: 0.01550, val loss: 0.01590\n",
      "Training epoch: 4285, train loss: 0.01542, val loss: 0.01580\n",
      "Training epoch: 4286, train loss: 0.01521, val loss: 0.01559\n",
      "Training epoch: 4287, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 4288, train loss: 0.01531, val loss: 0.01566\n",
      "Training epoch: 4289, train loss: 0.01529, val loss: 0.01565\n",
      "Training epoch: 4290, train loss: 0.01528, val loss: 0.01566\n",
      "Training epoch: 4291, train loss: 0.01527, val loss: 0.01564\n",
      "Training epoch: 4292, train loss: 0.01543, val loss: 0.01577\n",
      "Training epoch: 4293, train loss: 0.01539, val loss: 0.01578\n",
      "Training epoch: 4294, train loss: 0.01530, val loss: 0.01567\n",
      "Training epoch: 4295, train loss: 0.01536, val loss: 0.01574\n",
      "Training epoch: 4296, train loss: 0.01521, val loss: 0.01555\n",
      "Training epoch: 4297, train loss: 0.01525, val loss: 0.01561\n",
      "Training epoch: 4298, train loss: 0.01524, val loss: 0.01558\n",
      "Training epoch: 4299, train loss: 0.01545, val loss: 0.01583\n",
      "Training epoch: 4300, train loss: 0.01528, val loss: 0.01567\n",
      "Training epoch: 4301, train loss: 0.01541, val loss: 0.01579\n",
      "Training epoch: 4302, train loss: 0.01527, val loss: 0.01567\n",
      "Training epoch: 4303, train loss: 0.01539, val loss: 0.01574\n",
      "Training epoch: 4304, train loss: 0.01536, val loss: 0.01573\n",
      "Training epoch: 4305, train loss: 0.01535, val loss: 0.01571\n",
      "Training epoch: 4306, train loss: 0.01563, val loss: 0.01596\n",
      "Training epoch: 4307, train loss: 0.01528, val loss: 0.01566\n",
      "Training epoch: 4308, train loss: 0.01538, val loss: 0.01575\n",
      "Training epoch: 4309, train loss: 0.01599, val loss: 0.01634\n",
      "Training epoch: 4310, train loss: 0.01541, val loss: 0.01572\n",
      "Training epoch: 4311, train loss: 0.01534, val loss: 0.01570\n",
      "Training epoch: 4312, train loss: 0.01547, val loss: 0.01583\n",
      "Training epoch: 4313, train loss: 0.01558, val loss: 0.01597\n",
      "Training epoch: 4314, train loss: 0.01532, val loss: 0.01572\n",
      "Training epoch: 4315, train loss: 0.01532, val loss: 0.01566\n",
      "Training epoch: 4316, train loss: 0.01533, val loss: 0.01570\n",
      "Training epoch: 4317, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 4318, train loss: 0.01533, val loss: 0.01570\n",
      "Training epoch: 4319, train loss: 0.01527, val loss: 0.01569\n",
      "Training epoch: 4320, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4321, train loss: 0.01539, val loss: 0.01573\n",
      "Training epoch: 4322, train loss: 0.01566, val loss: 0.01606\n",
      "Training epoch: 4323, train loss: 0.01543, val loss: 0.01583\n",
      "Training epoch: 4324, train loss: 0.01543, val loss: 0.01582\n",
      "Training epoch: 4325, train loss: 0.01534, val loss: 0.01572\n",
      "Training epoch: 4326, train loss: 0.01546, val loss: 0.01580\n",
      "Training epoch: 4327, train loss: 0.01535, val loss: 0.01568\n",
      "Training epoch: 4328, train loss: 0.01526, val loss: 0.01562\n",
      "Training epoch: 4329, train loss: 0.01528, val loss: 0.01567\n",
      "Training epoch: 4330, train loss: 0.01527, val loss: 0.01562\n",
      "Training epoch: 4331, train loss: 0.01550, val loss: 0.01586\n",
      "Training epoch: 4332, train loss: 0.01588, val loss: 0.01628\n",
      "Training epoch: 4333, train loss: 0.01548, val loss: 0.01588\n",
      "Training epoch: 4334, train loss: 0.01531, val loss: 0.01568\n",
      "Training epoch: 4335, train loss: 0.01540, val loss: 0.01575\n",
      "Training epoch: 4336, train loss: 0.01554, val loss: 0.01589\n",
      "Training epoch: 4337, train loss: 0.01528, val loss: 0.01564\n",
      "Training epoch: 4338, train loss: 0.01526, val loss: 0.01562\n",
      "Training epoch: 4339, train loss: 0.01545, val loss: 0.01579\n",
      "Training epoch: 4340, train loss: 0.01538, val loss: 0.01577\n",
      "Training epoch: 4341, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 4342, train loss: 0.01556, val loss: 0.01594\n",
      "Training epoch: 4343, train loss: 0.01549, val loss: 0.01585\n",
      "Training epoch: 4344, train loss: 0.01542, val loss: 0.01580\n",
      "Training epoch: 4345, train loss: 0.01547, val loss: 0.01581\n",
      "Training epoch: 4346, train loss: 0.01641, val loss: 0.01680\n",
      "Training epoch: 4347, train loss: 0.01543, val loss: 0.01588\n",
      "Training epoch: 4348, train loss: 0.01528, val loss: 0.01564\n",
      "Training epoch: 4349, train loss: 0.01543, val loss: 0.01582\n",
      "Training epoch: 4350, train loss: 0.01532, val loss: 0.01572\n",
      "Training epoch: 4351, train loss: 0.01571, val loss: 0.01605\n",
      "Training epoch: 4352, train loss: 0.01588, val loss: 0.01626\n",
      "Training epoch: 4353, train loss: 0.01536, val loss: 0.01573\n",
      "Training epoch: 4354, train loss: 0.01537, val loss: 0.01575\n",
      "Training epoch: 4355, train loss: 0.01528, val loss: 0.01567\n",
      "Training epoch: 4356, train loss: 0.01542, val loss: 0.01581\n",
      "Training epoch: 4357, train loss: 0.01558, val loss: 0.01598\n",
      "Training epoch: 4358, train loss: 0.01526, val loss: 0.01562\n",
      "Training epoch: 4359, train loss: 0.01587, val loss: 0.01627\n",
      "Training epoch: 4360, train loss: 0.01551, val loss: 0.01589\n",
      "Training epoch: 4361, train loss: 0.01535, val loss: 0.01575\n",
      "Training epoch: 4362, train loss: 0.01547, val loss: 0.01583\n",
      "Training epoch: 4363, train loss: 0.01524, val loss: 0.01563\n",
      "Training epoch: 4364, train loss: 0.01549, val loss: 0.01585\n",
      "Training epoch: 4365, train loss: 0.01533, val loss: 0.01570\n",
      "Training epoch: 4366, train loss: 0.01525, val loss: 0.01561\n",
      "Training epoch: 4367, train loss: 0.01553, val loss: 0.01590\n",
      "Training epoch: 4368, train loss: 0.01544, val loss: 0.01577\n",
      "Training epoch: 4369, train loss: 0.01527, val loss: 0.01566\n",
      "Training epoch: 4370, train loss: 0.01576, val loss: 0.01607\n",
      "Training epoch: 4371, train loss: 0.01566, val loss: 0.01603\n",
      "Training epoch: 4372, train loss: 0.01527, val loss: 0.01561\n",
      "Training epoch: 4373, train loss: 0.01544, val loss: 0.01579\n",
      "Training epoch: 4374, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 4375, train loss: 0.01546, val loss: 0.01581\n",
      "Training epoch: 4376, train loss: 0.01535, val loss: 0.01572\n",
      "Training epoch: 4377, train loss: 0.01543, val loss: 0.01576\n",
      "Training epoch: 4378, train loss: 0.01535, val loss: 0.01573\n",
      "Training epoch: 4379, train loss: 0.01545, val loss: 0.01582\n",
      "Training epoch: 4380, train loss: 0.01531, val loss: 0.01571\n",
      "Training epoch: 4381, train loss: 0.01559, val loss: 0.01596\n",
      "Training epoch: 4382, train loss: 0.01582, val loss: 0.01620\n",
      "Training epoch: 4383, train loss: 0.01548, val loss: 0.01588\n",
      "Training epoch: 4384, train loss: 0.01564, val loss: 0.01601\n",
      "Training epoch: 4385, train loss: 0.01528, val loss: 0.01562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 4386, train loss: 0.01530, val loss: 0.01566\n",
      "Training epoch: 4387, train loss: 0.01539, val loss: 0.01577\n",
      "Training epoch: 4388, train loss: 0.01538, val loss: 0.01576\n",
      "Training epoch: 4389, train loss: 0.01521, val loss: 0.01561\n",
      "Training epoch: 4390, train loss: 0.01527, val loss: 0.01566\n",
      "Training epoch: 4391, train loss: 0.01561, val loss: 0.01601\n",
      "Training epoch: 4392, train loss: 0.01555, val loss: 0.01586\n",
      "Training epoch: 4393, train loss: 0.01544, val loss: 0.01582\n",
      "Training epoch: 4394, train loss: 0.01526, val loss: 0.01561\n",
      "Training epoch: 4395, train loss: 0.01548, val loss: 0.01586\n",
      "Training epoch: 4396, train loss: 0.01550, val loss: 0.01586\n",
      "Training epoch: 4397, train loss: 0.01529, val loss: 0.01564\n",
      "Training epoch: 4398, train loss: 0.01560, val loss: 0.01597\n",
      "Training epoch: 4399, train loss: 0.01540, val loss: 0.01577\n",
      "Training epoch: 4400, train loss: 0.01598, val loss: 0.01633\n",
      "Training epoch: 4401, train loss: 0.01530, val loss: 0.01568\n",
      "Training epoch: 4402, train loss: 0.01534, val loss: 0.01573\n",
      "Training epoch: 4403, train loss: 0.01573, val loss: 0.01608\n",
      "Training epoch: 4404, train loss: 0.01545, val loss: 0.01579\n",
      "Training epoch: 4405, train loss: 0.01540, val loss: 0.01578\n",
      "Training epoch: 4406, train loss: 0.01545, val loss: 0.01584\n",
      "Training epoch: 4407, train loss: 0.01539, val loss: 0.01570\n",
      "Training epoch: 4408, train loss: 0.01531, val loss: 0.01570\n",
      "Training epoch: 4409, train loss: 0.01549, val loss: 0.01584\n",
      "Training epoch: 4410, train loss: 0.01591, val loss: 0.01630\n",
      "Training epoch: 4411, train loss: 0.01537, val loss: 0.01575\n",
      "Training epoch: 4412, train loss: 0.01520, val loss: 0.01559\n",
      "Training epoch: 4413, train loss: 0.01521, val loss: 0.01559\n",
      "Training epoch: 4414, train loss: 0.01536, val loss: 0.01572\n",
      "Training epoch: 4415, train loss: 0.01533, val loss: 0.01565\n",
      "Training epoch: 4416, train loss: 0.01537, val loss: 0.01574\n",
      "Training epoch: 4417, train loss: 0.01521, val loss: 0.01560\n",
      "Training epoch: 4418, train loss: 0.01521, val loss: 0.01559\n",
      "Training epoch: 4419, train loss: 0.01525, val loss: 0.01565\n",
      "Training epoch: 4420, train loss: 0.01550, val loss: 0.01588\n",
      "Training epoch: 4421, train loss: 0.01546, val loss: 0.01579\n",
      "Training epoch: 4422, train loss: 0.01529, val loss: 0.01569\n",
      "Training epoch: 4423, train loss: 0.01571, val loss: 0.01605\n",
      "Training epoch: 4424, train loss: 0.01552, val loss: 0.01583\n",
      "Training epoch: 4425, train loss: 0.01547, val loss: 0.01585\n",
      "Training epoch: 4426, train loss: 0.01540, val loss: 0.01571\n",
      "Training epoch: 4427, train loss: 0.01539, val loss: 0.01577\n",
      "Training epoch: 4428, train loss: 0.01558, val loss: 0.01598\n",
      "Training epoch: 4429, train loss: 0.01531, val loss: 0.01568\n",
      "Training epoch: 4430, train loss: 0.01535, val loss: 0.01574\n",
      "Training epoch: 4431, train loss: 0.01530, val loss: 0.01571\n",
      "Training epoch: 4432, train loss: 0.01539, val loss: 0.01576\n",
      "Training epoch: 4433, train loss: 0.01541, val loss: 0.01582\n",
      "Training epoch: 4434, train loss: 0.01588, val loss: 0.01626\n",
      "Training epoch: 4435, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4436, train loss: 0.01536, val loss: 0.01575\n",
      "Training epoch: 4437, train loss: 0.01546, val loss: 0.01583\n",
      "Training epoch: 4438, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 4439, train loss: 0.01550, val loss: 0.01580\n",
      "Training epoch: 4440, train loss: 0.01536, val loss: 0.01572\n",
      "Training epoch: 4441, train loss: 0.01578, val loss: 0.01609\n",
      "Training epoch: 4442, train loss: 0.01528, val loss: 0.01561\n",
      "Training epoch: 4443, train loss: 0.01608, val loss: 0.01646\n",
      "Training epoch: 4444, train loss: 0.01530, val loss: 0.01567\n",
      "Training epoch: 4445, train loss: 0.01520, val loss: 0.01558\n",
      "Training epoch: 4446, train loss: 0.01533, val loss: 0.01570\n",
      "Training epoch: 4447, train loss: 0.01536, val loss: 0.01573\n",
      "Training epoch: 4448, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4449, train loss: 0.01547, val loss: 0.01581\n",
      "Training epoch: 4450, train loss: 0.01521, val loss: 0.01556\n",
      "Training epoch: 4451, train loss: 0.01530, val loss: 0.01567\n",
      "Training epoch: 4452, train loss: 0.01520, val loss: 0.01555\n",
      "Training epoch: 4453, train loss: 0.01551, val loss: 0.01583\n",
      "Training epoch: 4454, train loss: 0.01538, val loss: 0.01570\n",
      "Training epoch: 4455, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4456, train loss: 0.01531, val loss: 0.01563\n",
      "Training epoch: 4457, train loss: 0.01522, val loss: 0.01552\n",
      "Training epoch: 4458, train loss: 0.01560, val loss: 0.01598\n",
      "Training epoch: 4459, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 4460, train loss: 0.01537, val loss: 0.01576\n",
      "Training epoch: 4461, train loss: 0.01536, val loss: 0.01576\n",
      "Training epoch: 4462, train loss: 0.01539, val loss: 0.01576\n",
      "Training epoch: 4463, train loss: 0.01549, val loss: 0.01591\n",
      "Training epoch: 4464, train loss: 0.01529, val loss: 0.01568\n",
      "Training epoch: 4465, train loss: 0.01547, val loss: 0.01579\n",
      "Training epoch: 4466, train loss: 0.01542, val loss: 0.01578\n",
      "Training epoch: 4467, train loss: 0.01549, val loss: 0.01585\n",
      "Training epoch: 4468, train loss: 0.01555, val loss: 0.01589\n",
      "Training epoch: 4469, train loss: 0.01548, val loss: 0.01589\n",
      "Training epoch: 4470, train loss: 0.01537, val loss: 0.01575\n",
      "Training epoch: 4471, train loss: 0.01565, val loss: 0.01604\n",
      "Training epoch: 4472, train loss: 0.01562, val loss: 0.01598\n",
      "Training epoch: 4473, train loss: 0.01526, val loss: 0.01565\n",
      "Training epoch: 4474, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 4475, train loss: 0.01527, val loss: 0.01566\n",
      "Training epoch: 4476, train loss: 0.01530, val loss: 0.01568\n",
      "Training epoch: 4477, train loss: 0.01536, val loss: 0.01574\n",
      "Training epoch: 4478, train loss: 0.01540, val loss: 0.01578\n",
      "Training epoch: 4479, train loss: 0.01537, val loss: 0.01574\n",
      "Training epoch: 4480, train loss: 0.01521, val loss: 0.01559\n",
      "Training epoch: 4481, train loss: 0.01558, val loss: 0.01588\n",
      "Training epoch: 4482, train loss: 0.01526, val loss: 0.01565\n",
      "Training epoch: 4483, train loss: 0.01522, val loss: 0.01556\n",
      "Training epoch: 4484, train loss: 0.01569, val loss: 0.01604\n",
      "Training epoch: 4485, train loss: 0.01526, val loss: 0.01562\n",
      "Training epoch: 4486, train loss: 0.01539, val loss: 0.01574\n",
      "Training epoch: 4487, train loss: 0.01528, val loss: 0.01565\n",
      "Training epoch: 4488, train loss: 0.01543, val loss: 0.01577\n",
      "Training epoch: 4489, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 4490, train loss: 0.01532, val loss: 0.01570\n",
      "Training epoch: 4491, train loss: 0.01576, val loss: 0.01613\n",
      "Training epoch: 4492, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 4493, train loss: 0.01530, val loss: 0.01570\n",
      "Training epoch: 4494, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 4495, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4496, train loss: 0.01548, val loss: 0.01584\n",
      "Training epoch: 4497, train loss: 0.01523, val loss: 0.01563\n",
      "Training epoch: 4498, train loss: 0.01533, val loss: 0.01568\n",
      "Training epoch: 4499, train loss: 0.01537, val loss: 0.01573\n",
      "Training epoch: 4500, train loss: 0.01542, val loss: 0.01581\n",
      "Training epoch: 4501, train loss: 0.01530, val loss: 0.01568\n",
      "Training epoch: 4502, train loss: 0.01527, val loss: 0.01563\n",
      "Training epoch: 4503, train loss: 0.01532, val loss: 0.01570\n",
      "Training epoch: 4504, train loss: 0.01577, val loss: 0.01616\n",
      "Training epoch: 4505, train loss: 0.01561, val loss: 0.01596\n",
      "Training epoch: 4506, train loss: 0.01526, val loss: 0.01561\n",
      "Training epoch: 4507, train loss: 0.01564, val loss: 0.01600\n",
      "Training epoch: 4508, train loss: 0.01554, val loss: 0.01596\n",
      "Training epoch: 4509, train loss: 0.01531, val loss: 0.01569\n",
      "Training epoch: 4510, train loss: 0.01563, val loss: 0.01598\n",
      "Training epoch: 4511, train loss: 0.01521, val loss: 0.01556\n",
      "Training epoch: 4512, train loss: 0.01555, val loss: 0.01595\n",
      "Training epoch: 4513, train loss: 0.01526, val loss: 0.01567\n",
      "Training epoch: 4514, train loss: 0.01527, val loss: 0.01567\n",
      "Training epoch: 4515, train loss: 0.01616, val loss: 0.01654\n",
      "Training epoch: 4516, train loss: 0.01555, val loss: 0.01594\n",
      "Training epoch: 4517, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 4518, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 4519, train loss: 0.01548, val loss: 0.01585\n",
      "Training epoch: 4520, train loss: 0.01594, val loss: 0.01634\n",
      "Training epoch: 4521, train loss: 0.01614, val loss: 0.01652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 4522, train loss: 0.01541, val loss: 0.01578\n",
      "Training epoch: 4523, train loss: 0.01545, val loss: 0.01580\n",
      "Training epoch: 4524, train loss: 0.01548, val loss: 0.01583\n",
      "Training epoch: 4525, train loss: 0.01568, val loss: 0.01602\n",
      "Training epoch: 4526, train loss: 0.01541, val loss: 0.01578\n",
      "Training epoch: 4527, train loss: 0.01521, val loss: 0.01559\n",
      "Training epoch: 4528, train loss: 0.01557, val loss: 0.01593\n",
      "Training epoch: 4529, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 4530, train loss: 0.01531, val loss: 0.01565\n",
      "Training epoch: 4531, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 4532, train loss: 0.01552, val loss: 0.01590\n",
      "Training epoch: 4533, train loss: 0.01554, val loss: 0.01591\n",
      "Training epoch: 4534, train loss: 0.01555, val loss: 0.01592\n",
      "Training epoch: 4535, train loss: 0.01540, val loss: 0.01575\n",
      "Training epoch: 4536, train loss: 0.01538, val loss: 0.01577\n",
      "Training epoch: 4537, train loss: 0.01528, val loss: 0.01567\n",
      "Training epoch: 4538, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4539, train loss: 0.01542, val loss: 0.01575\n",
      "Training epoch: 4540, train loss: 0.01547, val loss: 0.01583\n",
      "Training epoch: 4541, train loss: 0.01537, val loss: 0.01572\n",
      "Training epoch: 4542, train loss: 0.01543, val loss: 0.01577\n",
      "Training epoch: 4543, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 4544, train loss: 0.01532, val loss: 0.01569\n",
      "Training epoch: 4545, train loss: 0.01552, val loss: 0.01590\n",
      "Training epoch: 4546, train loss: 0.01541, val loss: 0.01582\n",
      "Training epoch: 4547, train loss: 0.01530, val loss: 0.01567\n",
      "Training epoch: 4548, train loss: 0.01591, val loss: 0.01627\n",
      "Training epoch: 4549, train loss: 0.01532, val loss: 0.01572\n",
      "Training epoch: 4550, train loss: 0.01544, val loss: 0.01584\n",
      "Training epoch: 4551, train loss: 0.01580, val loss: 0.01616\n",
      "Training epoch: 4552, train loss: 0.01538, val loss: 0.01572\n",
      "Training epoch: 4553, train loss: 0.01544, val loss: 0.01586\n",
      "Training epoch: 4554, train loss: 0.01530, val loss: 0.01566\n",
      "Training epoch: 4555, train loss: 0.01529, val loss: 0.01564\n",
      "Training epoch: 4556, train loss: 0.01547, val loss: 0.01581\n",
      "Training epoch: 4557, train loss: 0.01583, val loss: 0.01620\n",
      "Training epoch: 4558, train loss: 0.01545, val loss: 0.01583\n",
      "Training epoch: 4559, train loss: 0.01549, val loss: 0.01586\n",
      "Training epoch: 4560, train loss: 0.01529, val loss: 0.01563\n",
      "Training epoch: 4561, train loss: 0.01551, val loss: 0.01588\n",
      "Training epoch: 4562, train loss: 0.01564, val loss: 0.01604\n",
      "Training epoch: 4563, train loss: 0.01535, val loss: 0.01571\n",
      "Training epoch: 4564, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 4565, train loss: 0.01535, val loss: 0.01569\n",
      "Training epoch: 4566, train loss: 0.01529, val loss: 0.01565\n",
      "Training epoch: 4567, train loss: 0.01544, val loss: 0.01579\n",
      "Training epoch: 4568, train loss: 0.01542, val loss: 0.01580\n",
      "Training epoch: 4569, train loss: 0.01540, val loss: 0.01578\n",
      "Training epoch: 4570, train loss: 0.01534, val loss: 0.01574\n",
      "Training epoch: 4571, train loss: 0.01534, val loss: 0.01573\n",
      "Training epoch: 4572, train loss: 0.01536, val loss: 0.01572\n",
      "Training epoch: 4573, train loss: 0.01541, val loss: 0.01579\n",
      "Training epoch: 4574, train loss: 0.01549, val loss: 0.01581\n",
      "Training epoch: 4575, train loss: 0.01541, val loss: 0.01576\n",
      "Training epoch: 4576, train loss: 0.01528, val loss: 0.01567\n",
      "Training epoch: 4577, train loss: 0.01544, val loss: 0.01579\n",
      "Training epoch: 4578, train loss: 0.01530, val loss: 0.01571\n",
      "Training epoch: 4579, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 4580, train loss: 0.01538, val loss: 0.01573\n",
      "Training epoch: 4581, train loss: 0.01581, val loss: 0.01615\n",
      "Training epoch: 4582, train loss: 0.01563, val loss: 0.01601\n",
      "Training epoch: 4583, train loss: 0.01547, val loss: 0.01586\n",
      "Training epoch: 4584, train loss: 0.01541, val loss: 0.01579\n",
      "Training epoch: 4585, train loss: 0.01575, val loss: 0.01617\n",
      "Training epoch: 4586, train loss: 0.01542, val loss: 0.01574\n",
      "Training epoch: 4587, train loss: 0.01522, val loss: 0.01562\n",
      "Training epoch: 4588, train loss: 0.01536, val loss: 0.01574\n",
      "Training epoch: 4589, train loss: 0.01576, val loss: 0.01611\n",
      "Training epoch: 4590, train loss: 0.01565, val loss: 0.01604\n",
      "Training epoch: 4591, train loss: 0.01536, val loss: 0.01573\n",
      "Training epoch: 4592, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 4593, train loss: 0.01539, val loss: 0.01574\n",
      "Training epoch: 4594, train loss: 0.01527, val loss: 0.01566\n",
      "Training epoch: 4595, train loss: 0.01530, val loss: 0.01569\n",
      "Training epoch: 4596, train loss: 0.01570, val loss: 0.01607\n",
      "Training epoch: 4597, train loss: 0.01532, val loss: 0.01566\n",
      "Training epoch: 4598, train loss: 0.01544, val loss: 0.01579\n",
      "Training epoch: 4599, train loss: 0.01578, val loss: 0.01615\n",
      "Training epoch: 4600, train loss: 0.01576, val loss: 0.01609\n",
      "Training epoch: 4601, train loss: 0.01529, val loss: 0.01563\n",
      "Training epoch: 4602, train loss: 0.01538, val loss: 0.01576\n",
      "Training epoch: 4603, train loss: 0.01567, val loss: 0.01606\n",
      "Training epoch: 4604, train loss: 0.01528, val loss: 0.01566\n",
      "Training epoch: 4605, train loss: 0.01548, val loss: 0.01587\n",
      "Training epoch: 4606, train loss: 0.01555, val loss: 0.01596\n",
      "Training epoch: 4607, train loss: 0.01543, val loss: 0.01579\n",
      "Training epoch: 4608, train loss: 0.01531, val loss: 0.01570\n",
      "Training epoch: 4609, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 4610, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4611, train loss: 0.01532, val loss: 0.01565\n",
      "Training epoch: 4612, train loss: 0.01534, val loss: 0.01569\n",
      "Training epoch: 4613, train loss: 0.01571, val loss: 0.01609\n",
      "Training epoch: 4614, train loss: 0.01592, val loss: 0.01631\n",
      "Training epoch: 4615, train loss: 0.01543, val loss: 0.01583\n",
      "Training epoch: 4616, train loss: 0.01568, val loss: 0.01602\n",
      "Training epoch: 4617, train loss: 0.01529, val loss: 0.01566\n",
      "Training epoch: 4618, train loss: 0.01577, val loss: 0.01616\n",
      "Training epoch: 4619, train loss: 0.01522, val loss: 0.01562\n",
      "Training epoch: 4620, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 4621, train loss: 0.01573, val loss: 0.01612\n",
      "Training epoch: 4622, train loss: 0.01551, val loss: 0.01590\n",
      "Training epoch: 4623, train loss: 0.01538, val loss: 0.01575\n",
      "Training epoch: 4624, train loss: 0.01557, val loss: 0.01588\n",
      "Training epoch: 4625, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 4626, train loss: 0.01551, val loss: 0.01591\n",
      "Training epoch: 4627, train loss: 0.01540, val loss: 0.01580\n",
      "Training epoch: 4628, train loss: 0.01594, val loss: 0.01629\n",
      "Training epoch: 4629, train loss: 0.01552, val loss: 0.01591\n",
      "Training epoch: 4630, train loss: 0.01535, val loss: 0.01572\n",
      "Training epoch: 4631, train loss: 0.01527, val loss: 0.01564\n",
      "Training epoch: 4632, train loss: 0.01537, val loss: 0.01578\n",
      "Training epoch: 4633, train loss: 0.01542, val loss: 0.01580\n",
      "Training epoch: 4634, train loss: 0.01581, val loss: 0.01620\n",
      "Training epoch: 4635, train loss: 0.01667, val loss: 0.01708\n",
      "Training epoch: 4636, train loss: 0.01575, val loss: 0.01615\n",
      "Training epoch: 4637, train loss: 0.01564, val loss: 0.01603\n",
      "Training epoch: 4638, train loss: 0.01583, val loss: 0.01617\n",
      "Training epoch: 4639, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4640, train loss: 0.01538, val loss: 0.01578\n",
      "Training epoch: 4641, train loss: 0.01524, val loss: 0.01559\n",
      "Training epoch: 4642, train loss: 0.01534, val loss: 0.01574\n",
      "Training epoch: 4643, train loss: 0.01553, val loss: 0.01592\n",
      "Training epoch: 4644, train loss: 0.01522, val loss: 0.01561\n",
      "Training epoch: 4645, train loss: 0.01541, val loss: 0.01577\n",
      "Training epoch: 4646, train loss: 0.01546, val loss: 0.01584\n",
      "Training epoch: 4647, train loss: 0.01541, val loss: 0.01580\n",
      "Training epoch: 4648, train loss: 0.01527, val loss: 0.01564\n",
      "Training epoch: 4649, train loss: 0.01542, val loss: 0.01581\n",
      "Training epoch: 4650, train loss: 0.01537, val loss: 0.01576\n",
      "Training epoch: 4651, train loss: 0.01545, val loss: 0.01583\n",
      "Training epoch: 4652, train loss: 0.01531, val loss: 0.01565\n",
      "Training epoch: 4653, train loss: 0.01537, val loss: 0.01574\n",
      "Training epoch: 4654, train loss: 0.01535, val loss: 0.01570\n",
      "Training epoch: 4655, train loss: 0.01569, val loss: 0.01603\n",
      "Training epoch: 4656, train loss: 0.01553, val loss: 0.01591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 4657, train loss: 0.01538, val loss: 0.01574\n",
      "Training epoch: 4658, train loss: 0.01555, val loss: 0.01586\n",
      "Training epoch: 4659, train loss: 0.01533, val loss: 0.01571\n",
      "Training epoch: 4660, train loss: 0.01556, val loss: 0.01594\n",
      "Training epoch: 4661, train loss: 0.01532, val loss: 0.01567\n",
      "Training epoch: 4662, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 4663, train loss: 0.01528, val loss: 0.01563\n",
      "Training epoch: 4664, train loss: 0.01544, val loss: 0.01579\n",
      "Training epoch: 4665, train loss: 0.01528, val loss: 0.01565\n",
      "Training epoch: 4666, train loss: 0.01534, val loss: 0.01567\n",
      "Training epoch: 4667, train loss: 0.01530, val loss: 0.01570\n",
      "Training epoch: 4668, train loss: 0.01563, val loss: 0.01595\n",
      "Training epoch: 4669, train loss: 0.01606, val loss: 0.01646\n",
      "Training epoch: 4670, train loss: 0.01528, val loss: 0.01565\n",
      "Training epoch: 4671, train loss: 0.01545, val loss: 0.01582\n",
      "Training epoch: 4672, train loss: 0.01531, val loss: 0.01568\n",
      "Training epoch: 4673, train loss: 0.01539, val loss: 0.01578\n",
      "Training epoch: 4674, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 4675, train loss: 0.01565, val loss: 0.01599\n",
      "Training epoch: 4676, train loss: 0.01529, val loss: 0.01562\n",
      "Training epoch: 4677, train loss: 0.01547, val loss: 0.01582\n",
      "Training epoch: 4678, train loss: 0.01558, val loss: 0.01594\n",
      "Training epoch: 4679, train loss: 0.01523, val loss: 0.01562\n",
      "Training epoch: 4680, train loss: 0.01535, val loss: 0.01571\n",
      "Training epoch: 4681, train loss: 0.01547, val loss: 0.01580\n",
      "Training epoch: 4682, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 4683, train loss: 0.01534, val loss: 0.01567\n",
      "Training epoch: 4684, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4685, train loss: 0.01546, val loss: 0.01586\n",
      "Training epoch: 4686, train loss: 0.01527, val loss: 0.01563\n",
      "Training epoch: 4687, train loss: 0.01526, val loss: 0.01562\n",
      "Training epoch: 4688, train loss: 0.01523, val loss: 0.01562\n",
      "Training epoch: 4689, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4690, train loss: 0.01601, val loss: 0.01630\n",
      "Training epoch: 4691, train loss: 0.01529, val loss: 0.01564\n",
      "Training epoch: 4692, train loss: 0.01537, val loss: 0.01578\n",
      "Training epoch: 4693, train loss: 0.01533, val loss: 0.01572\n",
      "Training epoch: 4694, train loss: 0.01547, val loss: 0.01579\n",
      "Training epoch: 4695, train loss: 0.01529, val loss: 0.01564\n",
      "Training epoch: 4696, train loss: 0.01531, val loss: 0.01570\n",
      "Training epoch: 4697, train loss: 0.01547, val loss: 0.01588\n",
      "Training epoch: 4698, train loss: 0.01520, val loss: 0.01558\n",
      "Training epoch: 4699, train loss: 0.01591, val loss: 0.01629\n",
      "Training epoch: 4700, train loss: 0.01523, val loss: 0.01563\n",
      "Training epoch: 4701, train loss: 0.01528, val loss: 0.01568\n",
      "Training epoch: 4702, train loss: 0.01529, val loss: 0.01563\n",
      "Training epoch: 4703, train loss: 0.01540, val loss: 0.01574\n",
      "Training epoch: 4704, train loss: 0.01556, val loss: 0.01594\n",
      "Training epoch: 4705, train loss: 0.01539, val loss: 0.01575\n",
      "Training epoch: 4706, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 4707, train loss: 0.01537, val loss: 0.01575\n",
      "Training epoch: 4708, train loss: 0.01531, val loss: 0.01568\n",
      "Training epoch: 4709, train loss: 0.01527, val loss: 0.01568\n",
      "Training epoch: 4710, train loss: 0.01531, val loss: 0.01568\n",
      "Training epoch: 4711, train loss: 0.01544, val loss: 0.01582\n",
      "Training epoch: 4712, train loss: 0.01527, val loss: 0.01564\n",
      "Training epoch: 4713, train loss: 0.01532, val loss: 0.01573\n",
      "Training epoch: 4714, train loss: 0.01519, val loss: 0.01558\n",
      "Training epoch: 4715, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 4716, train loss: 0.01564, val loss: 0.01599\n",
      "Training epoch: 4717, train loss: 0.01544, val loss: 0.01580\n",
      "Training epoch: 4718, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4719, train loss: 0.01556, val loss: 0.01589\n",
      "Training epoch: 4720, train loss: 0.01549, val loss: 0.01584\n",
      "Training epoch: 4721, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 4722, train loss: 0.01535, val loss: 0.01572\n",
      "Training epoch: 4723, train loss: 0.01528, val loss: 0.01564\n",
      "Training epoch: 4724, train loss: 0.01536, val loss: 0.01575\n",
      "Training epoch: 4725, train loss: 0.01560, val loss: 0.01599\n",
      "Training epoch: 4726, train loss: 0.01528, val loss: 0.01567\n",
      "Training epoch: 4727, train loss: 0.01527, val loss: 0.01562\n",
      "Training epoch: 4728, train loss: 0.01549, val loss: 0.01583\n",
      "Training epoch: 4729, train loss: 0.01539, val loss: 0.01572\n",
      "Training epoch: 4730, train loss: 0.01553, val loss: 0.01583\n",
      "Training epoch: 4731, train loss: 0.01560, val loss: 0.01599\n",
      "Training epoch: 4732, train loss: 0.01532, val loss: 0.01572\n",
      "Training epoch: 4733, train loss: 0.01541, val loss: 0.01579\n",
      "Training epoch: 4734, train loss: 0.01537, val loss: 0.01575\n",
      "Training epoch: 4735, train loss: 0.01549, val loss: 0.01588\n",
      "Training epoch: 4736, train loss: 0.01528, val loss: 0.01563\n",
      "Training epoch: 4737, train loss: 0.01556, val loss: 0.01594\n",
      "Training epoch: 4738, train loss: 0.01521, val loss: 0.01560\n",
      "Training epoch: 4739, train loss: 0.01532, val loss: 0.01566\n",
      "Training epoch: 4740, train loss: 0.01558, val loss: 0.01599\n",
      "Training epoch: 4741, train loss: 0.01544, val loss: 0.01583\n",
      "Training epoch: 4742, train loss: 0.01528, val loss: 0.01563\n",
      "Training epoch: 4743, train loss: 0.01565, val loss: 0.01594\n",
      "Training epoch: 4744, train loss: 0.01546, val loss: 0.01581\n",
      "Training epoch: 4745, train loss: 0.01540, val loss: 0.01576\n",
      "Training epoch: 4746, train loss: 0.01556, val loss: 0.01596\n",
      "Training epoch: 4747, train loss: 0.01583, val loss: 0.01624\n",
      "Training epoch: 4748, train loss: 0.01534, val loss: 0.01566\n",
      "Training epoch: 4749, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 4750, train loss: 0.01528, val loss: 0.01566\n",
      "Training epoch: 4751, train loss: 0.01528, val loss: 0.01565\n",
      "Training epoch: 4752, train loss: 0.01543, val loss: 0.01586\n",
      "Training epoch: 4753, train loss: 0.01549, val loss: 0.01588\n",
      "Training epoch: 4754, train loss: 0.01580, val loss: 0.01618\n",
      "Training epoch: 4755, train loss: 0.01569, val loss: 0.01605\n",
      "Training epoch: 4756, train loss: 0.01528, val loss: 0.01567\n",
      "Training epoch: 4757, train loss: 0.01537, val loss: 0.01575\n",
      "Training epoch: 4758, train loss: 0.01535, val loss: 0.01570\n",
      "Training epoch: 4759, train loss: 0.01547, val loss: 0.01579\n",
      "Training epoch: 4760, train loss: 0.01536, val loss: 0.01574\n",
      "Training epoch: 4761, train loss: 0.01551, val loss: 0.01589\n",
      "Training epoch: 4762, train loss: 0.01580, val loss: 0.01617\n",
      "Training epoch: 4763, train loss: 0.01534, val loss: 0.01570\n",
      "Training epoch: 4764, train loss: 0.01530, val loss: 0.01573\n",
      "Training epoch: 4765, train loss: 0.01539, val loss: 0.01578\n",
      "Training epoch: 4766, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 4767, train loss: 0.01572, val loss: 0.01609\n",
      "Training epoch: 4768, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 4769, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4770, train loss: 0.01533, val loss: 0.01571\n",
      "Training epoch: 4771, train loss: 0.01552, val loss: 0.01592\n",
      "Training epoch: 4772, train loss: 0.01550, val loss: 0.01588\n",
      "Training epoch: 4773, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4774, train loss: 0.01560, val loss: 0.01597\n",
      "Training epoch: 4775, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 4776, train loss: 0.01535, val loss: 0.01570\n",
      "Training epoch: 4777, train loss: 0.01532, val loss: 0.01568\n",
      "Training epoch: 4778, train loss: 0.01529, val loss: 0.01568\n",
      "Training epoch: 4779, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 4780, train loss: 0.01535, val loss: 0.01573\n",
      "Training epoch: 4781, train loss: 0.01535, val loss: 0.01573\n",
      "Training epoch: 4782, train loss: 0.01542, val loss: 0.01582\n",
      "Training epoch: 4783, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4784, train loss: 0.01552, val loss: 0.01597\n",
      "Training epoch: 4785, train loss: 0.01534, val loss: 0.01570\n",
      "Training epoch: 4786, train loss: 0.01540, val loss: 0.01574\n",
      "Training epoch: 4787, train loss: 0.01565, val loss: 0.01603\n",
      "Training epoch: 4788, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 4789, train loss: 0.01537, val loss: 0.01575\n",
      "Training epoch: 4790, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 4791, train loss: 0.01527, val loss: 0.01566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 4792, train loss: 0.01542, val loss: 0.01579\n",
      "Training epoch: 4793, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 4794, train loss: 0.01537, val loss: 0.01574\n",
      "Training epoch: 4795, train loss: 0.01549, val loss: 0.01588\n",
      "Training epoch: 4796, train loss: 0.01563, val loss: 0.01601\n",
      "Training epoch: 4797, train loss: 0.01532, val loss: 0.01568\n",
      "Training epoch: 4798, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 4799, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 4800, train loss: 0.01533, val loss: 0.01569\n",
      "Training epoch: 4801, train loss: 0.01554, val loss: 0.01593\n",
      "Training epoch: 4802, train loss: 0.01545, val loss: 0.01583\n",
      "Training epoch: 4803, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 4804, train loss: 0.01520, val loss: 0.01558\n",
      "Training epoch: 4805, train loss: 0.01561, val loss: 0.01598\n",
      "Training epoch: 4806, train loss: 0.01531, val loss: 0.01571\n",
      "Training epoch: 4807, train loss: 0.01541, val loss: 0.01577\n",
      "Training epoch: 4808, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 4809, train loss: 0.01544, val loss: 0.01581\n",
      "Training epoch: 4810, train loss: 0.01532, val loss: 0.01570\n",
      "Training epoch: 4811, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 4812, train loss: 0.01544, val loss: 0.01584\n",
      "Training epoch: 4813, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 4814, train loss: 0.01518, val loss: 0.01555\n",
      "Training epoch: 4815, train loss: 0.01536, val loss: 0.01570\n",
      "Training epoch: 4816, train loss: 0.01524, val loss: 0.01558\n",
      "Training epoch: 4817, train loss: 0.01530, val loss: 0.01566\n",
      "Training epoch: 4818, train loss: 0.01534, val loss: 0.01571\n",
      "Training epoch: 4819, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4820, train loss: 0.01526, val loss: 0.01558\n",
      "Training epoch: 4821, train loss: 0.01531, val loss: 0.01567\n",
      "Training epoch: 4822, train loss: 0.01538, val loss: 0.01580\n",
      "Training epoch: 4823, train loss: 0.01527, val loss: 0.01567\n",
      "Training epoch: 4824, train loss: 0.01579, val loss: 0.01615\n",
      "Training epoch: 4825, train loss: 0.01537, val loss: 0.01570\n",
      "Training epoch: 4826, train loss: 0.01533, val loss: 0.01571\n",
      "Training epoch: 4827, train loss: 0.01540, val loss: 0.01571\n",
      "Training epoch: 4828, train loss: 0.01535, val loss: 0.01568\n",
      "Training epoch: 4829, train loss: 0.01528, val loss: 0.01563\n",
      "Training epoch: 4830, train loss: 0.01530, val loss: 0.01569\n",
      "Training epoch: 4831, train loss: 0.01529, val loss: 0.01565\n",
      "Training epoch: 4832, train loss: 0.01549, val loss: 0.01588\n",
      "Training epoch: 4833, train loss: 0.01540, val loss: 0.01578\n",
      "Training epoch: 4834, train loss: 0.01534, val loss: 0.01569\n",
      "Training epoch: 4835, train loss: 0.01546, val loss: 0.01585\n",
      "Training epoch: 4836, train loss: 0.01545, val loss: 0.01584\n",
      "Training epoch: 4837, train loss: 0.01523, val loss: 0.01557\n",
      "Training epoch: 4838, train loss: 0.01548, val loss: 0.01585\n",
      "Training epoch: 4839, train loss: 0.01540, val loss: 0.01577\n",
      "Training epoch: 4840, train loss: 0.01526, val loss: 0.01566\n",
      "Training epoch: 4841, train loss: 0.01530, val loss: 0.01564\n",
      "Training epoch: 4842, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4843, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4844, train loss: 0.01549, val loss: 0.01583\n",
      "Training epoch: 4845, train loss: 0.01553, val loss: 0.01591\n",
      "Training epoch: 4846, train loss: 0.01547, val loss: 0.01582\n",
      "Training epoch: 4847, train loss: 0.01535, val loss: 0.01568\n",
      "Training epoch: 4848, train loss: 0.01529, val loss: 0.01566\n",
      "Training epoch: 4849, train loss: 0.01539, val loss: 0.01578\n",
      "Training epoch: 4850, train loss: 0.01553, val loss: 0.01591\n",
      "Training epoch: 4851, train loss: 0.01525, val loss: 0.01561\n",
      "Training epoch: 4852, train loss: 0.01534, val loss: 0.01569\n",
      "Training epoch: 4853, train loss: 0.01546, val loss: 0.01584\n",
      "Training epoch: 4854, train loss: 0.01526, val loss: 0.01565\n",
      "Training epoch: 4855, train loss: 0.01562, val loss: 0.01601\n",
      "Training epoch: 4856, train loss: 0.01539, val loss: 0.01574\n",
      "Training epoch: 4857, train loss: 0.01546, val loss: 0.01580\n",
      "Training epoch: 4858, train loss: 0.01521, val loss: 0.01560\n",
      "Training epoch: 4859, train loss: 0.01547, val loss: 0.01584\n",
      "Training epoch: 4860, train loss: 0.01537, val loss: 0.01575\n",
      "Training epoch: 4861, train loss: 0.01528, val loss: 0.01566\n",
      "Training epoch: 4862, train loss: 0.01535, val loss: 0.01575\n",
      "Training epoch: 4863, train loss: 0.01534, val loss: 0.01574\n",
      "Training epoch: 4864, train loss: 0.01531, val loss: 0.01568\n",
      "Training epoch: 4865, train loss: 0.01552, val loss: 0.01592\n",
      "Training epoch: 4866, train loss: 0.01542, val loss: 0.01576\n",
      "Training epoch: 4867, train loss: 0.01528, val loss: 0.01566\n",
      "Training epoch: 4868, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4869, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 4870, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 4871, train loss: 0.01537, val loss: 0.01573\n",
      "Training epoch: 4872, train loss: 0.01528, val loss: 0.01566\n",
      "Training epoch: 4873, train loss: 0.01538, val loss: 0.01579\n",
      "Training epoch: 4874, train loss: 0.01540, val loss: 0.01577\n",
      "Training epoch: 4875, train loss: 0.01547, val loss: 0.01586\n",
      "Training epoch: 4876, train loss: 0.01574, val loss: 0.01613\n",
      "Training epoch: 4877, train loss: 0.01564, val loss: 0.01598\n",
      "Training epoch: 4878, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 4879, train loss: 0.01530, val loss: 0.01567\n",
      "Training epoch: 4880, train loss: 0.01530, val loss: 0.01565\n",
      "Training epoch: 4881, train loss: 0.01519, val loss: 0.01554\n",
      "Training epoch: 4882, train loss: 0.01589, val loss: 0.01622\n",
      "Training epoch: 4883, train loss: 0.01537, val loss: 0.01574\n",
      "Training epoch: 4884, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4885, train loss: 0.01530, val loss: 0.01567\n",
      "Training epoch: 4886, train loss: 0.01530, val loss: 0.01566\n",
      "Training epoch: 4887, train loss: 0.01529, val loss: 0.01571\n",
      "Training epoch: 4888, train loss: 0.01554, val loss: 0.01594\n",
      "Training epoch: 4889, train loss: 0.01568, val loss: 0.01596\n",
      "Training epoch: 4890, train loss: 0.01531, val loss: 0.01570\n",
      "Training epoch: 4891, train loss: 0.01550, val loss: 0.01586\n",
      "Training epoch: 4892, train loss: 0.01519, val loss: 0.01557\n",
      "Training epoch: 4893, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4894, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 4895, train loss: 0.01534, val loss: 0.01571\n",
      "Training epoch: 4896, train loss: 0.01519, val loss: 0.01557\n",
      "Training epoch: 4897, train loss: 0.01549, val loss: 0.01586\n",
      "Training epoch: 4898, train loss: 0.01526, val loss: 0.01560\n",
      "Training epoch: 4899, train loss: 0.01530, val loss: 0.01566\n",
      "Training epoch: 4900, train loss: 0.01537, val loss: 0.01574\n",
      "Training epoch: 4901, train loss: 0.01566, val loss: 0.01596\n",
      "Training epoch: 4902, train loss: 0.01537, val loss: 0.01573\n",
      "Training epoch: 4903, train loss: 0.01537, val loss: 0.01571\n",
      "Training epoch: 4904, train loss: 0.01537, val loss: 0.01572\n",
      "Training epoch: 4905, train loss: 0.01532, val loss: 0.01566\n",
      "Training epoch: 4906, train loss: 0.01560, val loss: 0.01598\n",
      "Training epoch: 4907, train loss: 0.01532, val loss: 0.01565\n",
      "Training epoch: 4908, train loss: 0.01535, val loss: 0.01572\n",
      "Training epoch: 4909, train loss: 0.01545, val loss: 0.01581\n",
      "Training epoch: 4910, train loss: 0.01543, val loss: 0.01584\n",
      "Training epoch: 4911, train loss: 0.01549, val loss: 0.01586\n",
      "Training epoch: 4912, train loss: 0.01542, val loss: 0.01578\n",
      "Training epoch: 4913, train loss: 0.01535, val loss: 0.01574\n",
      "Training epoch: 4914, train loss: 0.01532, val loss: 0.01568\n",
      "Training epoch: 4915, train loss: 0.01638, val loss: 0.01674\n",
      "Training epoch: 4916, train loss: 0.01524, val loss: 0.01563\n",
      "Training epoch: 4917, train loss: 0.01581, val loss: 0.01619\n",
      "Training epoch: 4918, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 4919, train loss: 0.01528, val loss: 0.01569\n",
      "Training epoch: 4920, train loss: 0.01538, val loss: 0.01576\n",
      "Training epoch: 4921, train loss: 0.01527, val loss: 0.01563\n",
      "Training epoch: 4922, train loss: 0.01576, val loss: 0.01612\n",
      "Training epoch: 4923, train loss: 0.01528, val loss: 0.01566\n",
      "Training epoch: 4924, train loss: 0.01537, val loss: 0.01576\n",
      "Training epoch: 4925, train loss: 0.01525, val loss: 0.01560\n",
      "Training epoch: 4926, train loss: 0.01537, val loss: 0.01574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 4927, train loss: 0.01532, val loss: 0.01568\n",
      "Training epoch: 4928, train loss: 0.01574, val loss: 0.01615\n",
      "Training epoch: 4929, train loss: 0.01540, val loss: 0.01576\n",
      "Training epoch: 4930, train loss: 0.01532, val loss: 0.01570\n",
      "Training epoch: 4931, train loss: 0.01582, val loss: 0.01621\n",
      "Training epoch: 4932, train loss: 0.01527, val loss: 0.01561\n",
      "Training epoch: 4933, train loss: 0.01528, val loss: 0.01567\n",
      "Training epoch: 4934, train loss: 0.01552, val loss: 0.01591\n",
      "Training epoch: 4935, train loss: 0.01529, val loss: 0.01570\n",
      "Training epoch: 4936, train loss: 0.01546, val loss: 0.01580\n",
      "Training epoch: 4937, train loss: 0.01557, val loss: 0.01588\n",
      "Training epoch: 4938, train loss: 0.01549, val loss: 0.01588\n",
      "Training epoch: 4939, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4940, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 4941, train loss: 0.01558, val loss: 0.01595\n",
      "Training epoch: 4942, train loss: 0.01535, val loss: 0.01573\n",
      "Training epoch: 4943, train loss: 0.01545, val loss: 0.01581\n",
      "Training epoch: 4944, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 4945, train loss: 0.01529, val loss: 0.01565\n",
      "Training epoch: 4946, train loss: 0.01569, val loss: 0.01604\n",
      "Training epoch: 4947, train loss: 0.01547, val loss: 0.01581\n",
      "Training epoch: 4948, train loss: 0.01532, val loss: 0.01570\n",
      "Training epoch: 4949, train loss: 0.01521, val loss: 0.01560\n",
      "Training epoch: 4950, train loss: 0.01558, val loss: 0.01595\n",
      "Training epoch: 4951, train loss: 0.01566, val loss: 0.01595\n",
      "Training epoch: 4952, train loss: 0.01548, val loss: 0.01582\n",
      "Training epoch: 4953, train loss: 0.01553, val loss: 0.01586\n",
      "Training epoch: 4954, train loss: 0.01598, val loss: 0.01640\n",
      "Training epoch: 4955, train loss: 0.01535, val loss: 0.01574\n",
      "Training epoch: 4956, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4957, train loss: 0.01534, val loss: 0.01572\n",
      "Training epoch: 4958, train loss: 0.01583, val loss: 0.01623\n",
      "Early stop at epoch 4958, With Testing Error: 0.01623\n",
      "Subnetwork pruning.\n",
      "Fine tuning.\n",
      "Tuning epoch: 1, train loss: 0.01523, val loss: 0.01560\n",
      "Tuning epoch: 2, train loss: 0.01534, val loss: 0.01566\n",
      "Tuning epoch: 3, train loss: 0.01586, val loss: 0.01619\n",
      "Tuning epoch: 4, train loss: 0.01528, val loss: 0.01567\n",
      "Tuning epoch: 5, train loss: 0.01529, val loss: 0.01564\n",
      "Tuning epoch: 6, train loss: 0.01530, val loss: 0.01564\n",
      "Tuning epoch: 7, train loss: 0.01582, val loss: 0.01619\n",
      "Tuning epoch: 8, train loss: 0.01549, val loss: 0.01580\n",
      "Tuning epoch: 9, train loss: 0.01524, val loss: 0.01561\n",
      "Tuning epoch: 10, train loss: 0.01542, val loss: 0.01573\n",
      "Tuning epoch: 11, train loss: 0.01563, val loss: 0.01596\n",
      "Tuning epoch: 12, train loss: 0.01523, val loss: 0.01560\n",
      "Tuning epoch: 13, train loss: 0.01530, val loss: 0.01566\n",
      "Tuning epoch: 14, train loss: 0.01525, val loss: 0.01562\n",
      "Tuning epoch: 15, train loss: 0.01532, val loss: 0.01568\n",
      "Tuning epoch: 16, train loss: 0.01563, val loss: 0.01592\n",
      "Tuning epoch: 17, train loss: 0.01532, val loss: 0.01567\n",
      "Tuning epoch: 18, train loss: 0.01599, val loss: 0.01636\n",
      "Tuning epoch: 19, train loss: 0.01542, val loss: 0.01574\n",
      "Tuning epoch: 20, train loss: 0.01524, val loss: 0.01559\n",
      "Tuning epoch: 21, train loss: 0.01538, val loss: 0.01571\n",
      "Tuning epoch: 22, train loss: 0.01529, val loss: 0.01566\n",
      "Tuning epoch: 23, train loss: 0.01521, val loss: 0.01556\n",
      "Tuning epoch: 24, train loss: 0.01603, val loss: 0.01642\n",
      "Tuning epoch: 25, train loss: 0.01520, val loss: 0.01556\n",
      "Tuning epoch: 26, train loss: 0.01527, val loss: 0.01559\n",
      "Tuning epoch: 27, train loss: 0.01519, val loss: 0.01551\n",
      "Tuning epoch: 28, train loss: 0.01533, val loss: 0.01567\n",
      "Tuning epoch: 29, train loss: 0.01558, val loss: 0.01591\n",
      "Tuning epoch: 30, train loss: 0.01534, val loss: 0.01567\n",
      "Tuning epoch: 31, train loss: 0.01583, val loss: 0.01622\n",
      "Tuning epoch: 32, train loss: 0.01521, val loss: 0.01561\n",
      "Tuning epoch: 33, train loss: 0.01542, val loss: 0.01574\n",
      "Tuning epoch: 34, train loss: 0.01553, val loss: 0.01587\n",
      "Tuning epoch: 35, train loss: 0.01553, val loss: 0.01586\n",
      "Tuning epoch: 36, train loss: 0.01552, val loss: 0.01590\n",
      "Tuning epoch: 37, train loss: 0.01587, val loss: 0.01625\n",
      "Tuning epoch: 38, train loss: 0.01538, val loss: 0.01573\n",
      "Tuning epoch: 39, train loss: 0.01614, val loss: 0.01650\n",
      "Tuning epoch: 40, train loss: 0.01550, val loss: 0.01588\n",
      "Tuning epoch: 41, train loss: 0.01526, val loss: 0.01563\n",
      "Tuning epoch: 42, train loss: 0.01594, val loss: 0.01633\n",
      "Tuning epoch: 43, train loss: 0.01526, val loss: 0.01558\n",
      "Tuning epoch: 44, train loss: 0.01549, val loss: 0.01586\n",
      "Tuning epoch: 45, train loss: 0.01529, val loss: 0.01566\n",
      "Tuning epoch: 46, train loss: 0.01554, val loss: 0.01587\n",
      "Tuning epoch: 47, train loss: 0.01548, val loss: 0.01586\n",
      "Tuning epoch: 48, train loss: 0.01544, val loss: 0.01579\n",
      "Tuning epoch: 49, train loss: 0.01528, val loss: 0.01562\n",
      "Tuning epoch: 50, train loss: 0.01541, val loss: 0.01576\n",
      "Tuning epoch: 51, train loss: 0.01564, val loss: 0.01595\n",
      "Tuning epoch: 52, train loss: 0.01525, val loss: 0.01562\n",
      "Tuning epoch: 53, train loss: 0.01527, val loss: 0.01560\n",
      "Tuning epoch: 54, train loss: 0.01524, val loss: 0.01558\n",
      "Tuning epoch: 55, train loss: 0.01538, val loss: 0.01576\n",
      "Tuning epoch: 56, train loss: 0.01552, val loss: 0.01587\n",
      "Tuning epoch: 57, train loss: 0.01562, val loss: 0.01599\n",
      "Tuning epoch: 58, train loss: 0.01526, val loss: 0.01560\n",
      "Tuning epoch: 59, train loss: 0.01532, val loss: 0.01567\n",
      "Tuning epoch: 60, train loss: 0.01551, val loss: 0.01580\n",
      "Tuning epoch: 61, train loss: 0.01552, val loss: 0.01584\n",
      "Tuning epoch: 62, train loss: 0.01529, val loss: 0.01566\n",
      "Tuning epoch: 63, train loss: 0.01528, val loss: 0.01559\n",
      "Tuning epoch: 64, train loss: 0.01522, val loss: 0.01556\n",
      "Tuning epoch: 65, train loss: 0.01531, val loss: 0.01565\n",
      "Tuning epoch: 66, train loss: 0.01521, val loss: 0.01554\n",
      "Tuning epoch: 67, train loss: 0.01544, val loss: 0.01575\n",
      "Tuning epoch: 68, train loss: 0.01526, val loss: 0.01562\n",
      "Tuning epoch: 69, train loss: 0.01531, val loss: 0.01562\n",
      "Tuning epoch: 70, train loss: 0.01545, val loss: 0.01583\n",
      "Tuning epoch: 71, train loss: 0.01533, val loss: 0.01562\n",
      "Tuning epoch: 72, train loss: 0.01536, val loss: 0.01575\n",
      "Tuning epoch: 73, train loss: 0.01527, val loss: 0.01562\n",
      "Tuning epoch: 74, train loss: 0.01557, val loss: 0.01592\n",
      "Tuning epoch: 75, train loss: 0.01530, val loss: 0.01564\n",
      "Tuning epoch: 76, train loss: 0.01535, val loss: 0.01571\n",
      "Tuning epoch: 77, train loss: 0.01559, val loss: 0.01599\n",
      "Tuning epoch: 78, train loss: 0.01595, val loss: 0.01622\n",
      "Tuning epoch: 79, train loss: 0.01531, val loss: 0.01562\n",
      "Tuning epoch: 80, train loss: 0.01547, val loss: 0.01582\n",
      "Tuning epoch: 81, train loss: 0.01526, val loss: 0.01562\n",
      "Tuning epoch: 82, train loss: 0.01529, val loss: 0.01562\n",
      "Tuning epoch: 83, train loss: 0.01533, val loss: 0.01566\n",
      "Tuning epoch: 84, train loss: 0.01557, val loss: 0.01586\n",
      "Tuning epoch: 85, train loss: 0.01524, val loss: 0.01557\n",
      "Tuning epoch: 86, train loss: 0.01539, val loss: 0.01575\n",
      "Tuning epoch: 87, train loss: 0.01550, val loss: 0.01585\n",
      "Tuning epoch: 88, train loss: 0.01531, val loss: 0.01566\n",
      "Tuning epoch: 89, train loss: 0.01527, val loss: 0.01565\n",
      "Tuning epoch: 90, train loss: 0.01532, val loss: 0.01570\n",
      "Tuning epoch: 91, train loss: 0.01524, val loss: 0.01557\n",
      "Tuning epoch: 92, train loss: 0.01519, val loss: 0.01554\n",
      "Tuning epoch: 93, train loss: 0.01531, val loss: 0.01566\n",
      "Tuning epoch: 94, train loss: 0.01526, val loss: 0.01563\n",
      "Tuning epoch: 95, train loss: 0.01527, val loss: 0.01561\n",
      "Tuning epoch: 96, train loss: 0.01535, val loss: 0.01572\n",
      "Tuning epoch: 97, train loss: 0.01525, val loss: 0.01560\n",
      "Tuning epoch: 98, train loss: 0.01526, val loss: 0.01560\n",
      "Tuning epoch: 99, train loss: 0.01565, val loss: 0.01601\n",
      "Tuning epoch: 100, train loss: 0.01525, val loss: 0.01563\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from exnn import ExNN\n",
    "\n",
    "def data_generator1(datanum, testnum=10000, noise_sigma=1, rand_seed=0):\n",
    "    \n",
    "    corr = 0.5\n",
    "    np.random.seed(rand_seed)\n",
    "    proj_matrix = np.zeros((10, 4))\n",
    "    proj_matrix[:7, 0] = np.array([1,0,0,0,0,0,0])\n",
    "    proj_matrix[:7, 1] = np.array([0,1,0,0,0,0,0])\n",
    "    proj_matrix[:7, 2] = np.array([0,0,0.5,0.5,0,0,0])\n",
    "    proj_matrix[:7, 3] = np.array([0,0,0,0,0.2,0.3,0.5])\n",
    "    u = np.random.uniform(-1, 1, [datanum + testnum, 1])\n",
    "    t = np.sqrt(corr / (1 - corr))\n",
    "    x = np.zeros((datanum + testnum, 10))\n",
    "    for i in range(10):\n",
    "        x[:, i:i + 1] = (np.random.uniform(-1, 1, [datanum + testnum, 1]) + t * u) / (1 + t)\n",
    "\n",
    "    y = np.reshape(2 * np.dot(x, proj_matrix[:, 0]) + 0.2 * np.exp(-4 * np.dot(x, proj_matrix[:, 1])) + \\\n",
    "                   3 * (np.dot(x, proj_matrix[:, 2]))**2 + 2.5 * np.sin(np.pi * np.dot(x, proj_matrix[:, 3])), [-1, 1]) + \\\n",
    "              noise_sigma * np.random.normal(0, 1, [datanum + testnum, 1])\n",
    "    \n",
    "    task_type = \"Regression\"\n",
    "    meta_info = {\"X1\":{\"type\":\"continuous\"},\n",
    "             \"X2\":{\"type\":\"continuous\"},\n",
    "             \"X3\":{\"type\":\"continuous\"},\n",
    "             \"X4\":{\"type\":\"continuous\"},\n",
    "             \"X5\":{\"type\":\"continuous\"},\n",
    "             \"X6\":{\"type\":\"continuous\"},\n",
    "             \"X7\":{\"type\":\"continuous\"},\n",
    "             \"X8\":{\"type\":\"continuous\"},\n",
    "             \"X9\":{\"type\":\"continuous\"},\n",
    "             \"X10\":{\"type\":\"continuous\"},\n",
    "             \"Y\":{\"type\":\"target\"}}\n",
    "    for i, (key, item) in enumerate(meta_info.items()):\n",
    "        if item['type'] == \"target\":\n",
    "            sy = MinMaxScaler((-1, 1))\n",
    "            y = sy.fit_transform(y)\n",
    "            meta_info[key][\"scaler\"] = sy\n",
    "        elif item['type'] == \"categorical\":\n",
    "            enc = OrdinalEncoder()\n",
    "            enc.fit(x[:,[i]])\n",
    "            ordinal_feature = enc.transform(x[:,[i]])\n",
    "            x[:,[i]] = ordinal_feature\n",
    "            meta_info[key][\"values\"] = enc.categories_[0].tolist()\n",
    "        else:\n",
    "            sx = MinMaxScaler((-1, 1))\n",
    "            x[:,[i]] = sx.fit_transform(x[:,[i]])\n",
    "            meta_info[key][\"scaler\"] = sx\n",
    "\n",
    "    train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=testnum, random_state=rand_seed)\n",
    "    return train_x, test_x, train_y, test_y, task_type, meta_info\n",
    "\n",
    "train_x, test_x, train_y, test_y, task_type, meta_info = data_generator1(datanum=10000, testnum=10000, noise_sigma=1, rand_seed=0)\n",
    "model = ExNN(meta_info=meta_info,\n",
    "               subnet_num=10,\n",
    "               subnet_arch=[10, 6],\n",
    "               task_type=task_type,\n",
    "               activation_func=tf.tanh,\n",
    "               batch_size=min(1000, int(train_x.shape[0] * 0.2)),\n",
    "               training_epochs=5000,\n",
    "               lr_bp=0.001,\n",
    "               lr_cl=0.1,\n",
    "               beta_threshold=0.05,\n",
    "               tuning_epochs=100,\n",
    "               l1_proj=0.0001,\n",
    "               l1_subnet=0.00316,\n",
    "               l2_smooth=10**(-6),\n",
    "               verbose=True,\n",
    "               val_ratio=0.2,\n",
    "               early_stop_thres=500)\n",
    "\n",
    "model.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T06:41:27.913904Z",
     "start_time": "2020-07-21T06:41:26.579588Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x1296 with 8 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.visualize(folder=\"./\", name=\"exnn_demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T06:41:29.525473Z",
     "start_time": "2020-07-21T06:41:27.916637Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1728x331.2 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.visualize_new(cols_per_row=3, subnet_num=3, dummy_subnet_num=0, folder=\"./\", name=\"exnn_demo\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf2)",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
